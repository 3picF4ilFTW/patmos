\documentclass[a4paper,fontsize=10pt,twoside,DIV15,BCOR12mm,headinclude=true,footinclude=false,pagesize,bibtotoc]{scrbook}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{pslatex} % -- times instead of computer modern
\usepackage[scaled=.84]{beramono} % a sane monospace font
\usepackage{microtype}

\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xspace}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{colortbl}
\usepackage{multicol}
\usepackage{rotating}
\usepackage{subfig}
\usepackage{ulem}
\usepackage{enumerate}

% avoid clubs and widows
\clubpenalty=10000
\widowpenalty=10000

% tweak float placement
%% \renewcommand{\textfraction}{.15}
\renewcommand{\topfraction}{.75}
%% \renewcommand{\bottomfraction}{.7}
\renewcommand{\floatpagefraction}{.75}
%% \renewcommand{\dbltopfraction}{.66}
%% \renewcommand{\dblfloatpagefraction}{.66}
\setcounter{topnumber}{4}
%% \setcounter{bottomnumber}{4}
%% \setcounter{totalnumber}{16}
%% \setcounter{dbltopnumber}{4}

\newcommand{\code}[1]{{\texttt{#1}}}
\newcommand{\codefoot}[1]{{\textsf{#1}}}
\def\figref#1{Figure~\ref{fig:#1}}

% ulem package, otherwise emphasized text becomes underlined
\normalem


\newcommand{\todo}[1]{{\emph{TODO: #1}}}
%\renewcommand{\todo}[1]{}

%
% generic command to comment something
%
\newcommand{\comment}[3]{

\textsf{\textbf{#1}} {\color{#3}#2}}

%
% commentators
%
\newcommand{\tommy}[1]{\comment{Tommy}{#1}{Red}}
\newcommand{\wolf}[1]{\comment{Wolfgang}{#1}{OliveGreen}}
\newcommand{\martin}[1]{\comment{Martin}{#1}{Blue}}
\newcommand{\stefan}[1]{\comment{Stefan}{#1}{RoyalPurple}}
\newcommand{\daniel}[1]{\comment{Daniel}{#1}{RoyalBlue}}
\newcommand{\cullmann}[1]{\comment{Christoph}{#1}{Maroon}}
\newcommand{\gebhard}[1]{\comment{Gernot}{#1}{RedOrange}}
\newcommand{\fb}[1]{\comment{Florian}{#1}{Emerald}}
\newcommand{\jack}[1]{\comment{Jack}{#1}{Magenta}}
\newcommand{\sahar}[1]{\comment{Sahar}{#1}{Green}}
\newcommand{\rasmus}[1]{\comment{Rasmus}{#1}{Mahogany}}

% uncomment to get rid of comments
\renewcommand{\tommy}[1]{}
\renewcommand{\wolf}[1]{}
\renewcommand{\martin}[1]{}
\renewcommand{\stefan}[1]{}
\renewcommand{\daniel}[1]{}
\renewcommand{\cullmann}[1]{}
\renewcommand{\gebhard}[1]{}
\renewcommand{\fb}[1]{}
\renewcommand{\jack}[1]{}
\renewcommand{\sahar}[1]{}
\renewcommand{\rasmus}[1]{}

%
% custom colors
%
\definecolor{lightgray}{gray}{0.8}
\definecolor{gray}{gray}{0.5}

\usepackage{listings}

% general style for listings
\lstset{basicstyle=\ttfamily,keywordstyle=\ttfamily,showstringspaces=false,language=C}

\usepackage[endianness=big]{bytefield}

% long immediate in second slot
\newcommand{\lconst}{\texttt{const}_{32}}
% short immediate in ALU instruction
\newcommand{\sconst}{\texttt{Constant}_{12}}
% constant in Rs2 field
\newcommand{\rconst}{\texttt{Constant}_{5}}

% SH: to be used in text mode .. maybe we should change this to math mode?
\newcommand{\XOR}{\textasciicircum\xspace}
\newcommand{\OR}{\textbar\xspace}
\newcommand{\AND}{\&\xspace}
\newcommand{\NOT}{\texttildelow}
\newcommand{\shl}{\textless$\!$\textless\xspace}
\newcommand{\shr}{\textgreater$\!$\textgreater$\!$\textgreater\xspace}
\newcommand{\ashr}{\textgreater$\!$\textgreater\xspace}

\newcommand{\bitsunused}{\rule{\width}{\height}}
\newcommand{\bitssubclass}{\color{lightgray}\rule{\width}{\height}}

%
% allow click-able links
%
\usepackage[open]{bookmark}
\usepackage[all]{hypcap}

%
% hyperref setup (depends on bookmark/hyperref}
%
\hypersetup{
    pdftitle = {Patmos Reference Handbook},
    pdfsubject = {Technical Report},
    colorlinks = {true},
    citecolor = {black},
    filecolor = {black},
    linkcolor = {black},
    urlcolor = {black},
    final
}

%
% document contents
%
\begin{document}

\title{Patmos Reference Handbook}

\author{Martin Schoeberl, Florian Brandner, Stefan Hepp,\\Wolfgang Puffitsch, Daniel Prokesch}

\lowertitleback{Copyright \copyright{} 2014 Technical University of Denmark
  \medskip\\
  \begin{tabular}{lp{.8\textwidth}}
    \raisebox{-12pt}{\includegraphics[height=18pt]{fig/cc_by_sa}} &
     This work is licensed under a Creative Commons Attribution-ShareAlike
     4.0 International License.
     \url{http://creativecommons.org/licenses/by-sa/4.0/}\\
  \end{tabular}
}

\frontmatter

\maketitle

\chapter{Preface}

This handbook shall evolve to the documentation of the Patmos processor and the
Patmos compiler. In the mean time it is intended to collect design notes and discussions.
Especially ISA design notes now.

The latest version of this handbook is contained as LaTeX source in the Patmos repository in directory
\code{patmos/doc/handbook} and can be built with \code{make}.

\section*{Acknowledgment}

We would like to thank Tommy Thorn for the always intense and enjoyable
discussions of the Patmos ISA and processor design in general.
Jack Whitham offered his experience with RISC
ISA design and trade-offs.
Gernot Gebhard and Christoph Cullmann gave valuable feedback on the ISA
relative to WCET analysis.
Sahar Abbaspourseyedi is working on the stack
cache and verifies the ideas and concepts presented here. We thank
Rasmus Bo S{\o}rensen for fixing some documentation errors.

This work was partially funded under the
European Union's 7th Framework Programme
under grant agreement no. 288008:
Time-predictable Multi-Core Architecture for Embedded
Systems (T-CREST).

\tableofcontents

\begingroup
\let\cleardoublepage\clearpage
\listoffigures
\listoftables
\lstlistoflistings
\endgroup

\mainmatter

\chapter{Introduction}

Real-time systems need a time-predictable execution platform so that the worst-case execution time (WCET) can be statically estimated. It has been argued that we have to rethink computer architecture for real-time systems instead of trying to catch up with new processors in the WCET analysis tools~\cite{tpca:jes, pret:dac2007}.

We present the time-predictable processor Patmos as one approach to attack the complexity issue of WCET analysis. Patmos is a static scheduled, dual-issue RISC processor that is optimized for real-time systems.

\martin{This report shall converge towards a real manual. At the moment it serves
discussion well, but we shall keep this in mind. Here a starting list of TODOs:

\begin{itemize}
\item Send an email to all and ask about cleanup of some discussion points
\item Convert some discussion text into readable sections and argue why we
did what we did
\item Get a nice introduction and a good architecture section written
\end{itemize}

}

%\chapter{Related Work}
%\label{sec:related}


\fb{Martin wishes that Patmos will not require more than 3000 LC ;-)}
\martin{And fmax shall not be below 80\% of NIOS or MicroBlaze.
And performance (also average case ;-) shall be better, compared to NIOS/MB.
And we could compare against Tommy's YARI.}

\martin{TODO: The instruction description with the individual pipeline stages shall be
rewritten to reflect the actual simulator and hardware implementation of Patmos.
E.g. predicate registers are *not* read in the decode stage, but directly in EX.
Reading in decode would mean a one cycle generate use delay.}


\chapter{The Architecture of Patmos}
\label{sec:arch}

\section{Pipeline}

Figure~\ref{fig:pipeline} shows an overview of Patmos' pipeline. The pipeline
consist of 5 stages: (1) instruction fetch (\texttt{FE}), (2) decode and
register read (\texttt{DEC}), (3) execute (\texttt{EX}), (4) memory access (\texttt{MEM}), and (5) register write  back (\texttt{WB}).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{fig/pipeline}
    \caption{Pipeline of Patmos with fetch, decode, execute, memory, and write back stages.}\label{fig:pipeline}
\end{figure}

Some instructions define additional pipeline stages. Multiplication instructions
are executed, starting from the \texttt{EX} stage, in a parallel pipeline with
fixed-length (see the instruction definition). The respective stages are
referred to by \texttt{EX$_1$}, \dots, \texttt{EX$_n$}.

\martin{\todo{A more detailed description of the pipeline stages, as the individual
4 stage description is gone.} Here a start:}

\subsection{Fetch}

Fetch one or two words of instruction from the ROM or method cache.
Calculate next PC depending on the length of the instruction bundle.

\subsection{Decode}

Decode the instruction and generate control signals for the following stages.
Read register operands. Sign or zero extend immediate operands.

\subsection{Execute}

Read predicate registers. Conditional execute (ALU) instructions.
Write predicate register. Calculate effective address for memory operations.

\subsection{Memory}

Read or write memory. This is the only pipeline stage that might
stall the pipeline.

\subsection{Write Back}

Write result into destination register.

\stefan{Note that in the \texttt{pasim} simulator the Memory and Write Back stages are merged into a single MW stage at the time of writing,
and the stages are simulated from back to front.
While this has no effect on the timing and hazards of the instructions, there is only a single bypass from MW to EX (per pipeline). Apart
from that, all implementations of all instructions in the simulator should adhere to the above description of the stages.}

\section{Local Memories}

Patmos contains several on-chip memories, as sketched in Figure~\ref{fig:pipeline}.
We apply the idea of split caches~\cite{jop:dcache:rts} to simplify and enhance
the cache analysis. Instructions are fetched from the method cache that caches
whole methods. Patmos also supports instruction and data scratchpad memories.
Stack allocated data is cached in a stack cache and we envision also a normal
data cache with LRU replacement. Accesses to data that are hard to analyze can
bypass the data cache.

% \cite{jop:ocwcet:ccpe}



\section{Register Files}

The register files available in Patmos are depicted by
Figure~\ref{fig:registers}. In short, Patmos offers:
\begin{itemize}
  \item 32, 32-bit general-purpose registers (\texttt{R}) : \texttt{r0}, \ldots, \texttt{r31} \\
    \texttt{r0} is read-only, set to zero (\texttt{0}).
  \item 8, single-bit predicate registers (\texttt{P}): \texttt{p0}, \ldots, \texttt{p7}, \\
    \texttt{p0} is read-only, set to \texttt{true} (\texttt{1}).
  \item 16, 32-bit special-purpose registers (\texttt{S}): \texttt{s0}, \ldots, \texttt{s15}
\end{itemize}

The general-purpose registers \texttt{R} are read in the \code{DEC} stage
and written in the \code{WB} stage. Full forwarding makes them available
in the \code{EX} stage before written into the register file.

The predicate registers are single bits that are set and read in the \code{EX}
stage.

The special registers \code{S} is just a collection of various `special'
processor registers. These registers might be used by different units/stages
in the pipeline and are not physically collected in a `register file'.
The pipeline stage where those registers are read and written by the
\code{mfs} and \code{mts} are dependent on the type of the special
register.

\martin{The three `register' files shall constitute the state of the processor.
Every non-obvious register, such as method base, shall be mapped to a
`special' register. Even the current PC on an interrupt shall end up in a
register that is mapped into the special register domain.}

\stefan{Umm .. it would be really good to define somewhere in which stages which special register is actually read or written by mfs/mts
;).}

So all-in-all the recoverable process state is: general-purpose registers
\code{R}, the predicates \code{P}, and a collection of various processor
registers mapped to the `special' register files \code{S}.

Concurrently writing and reading the same register in the same cycle
will, for the read, yield the old value of the register. Reads in
subsequent cycles return the result most recently written to the
register, i.e., the pipeline implements full forwarding.

When writing concurrently to the same register, the result is
undefined. If two instructions of the current bundle have the same
destination register, the result is only defined if the predicate of
at most one instruction in the bundle evaluates to \texttt{true} (1).

\wolf{In practice, the result is even defined if both instructions
  write to the same register, but I would not make this a feature of
  the ISA.}

The predicate registers are usually encoded as $4$-bit operands, where the most
significant bit indicates that the value read from the register file should be
inverted before it is used. For operands that are written, this additional bit
is omitted.

The special-purpose registers of \texttt{S} allow access to some dedicated
registers:
\begin{itemize}
  \item The lower $8$ bits of \texttt{s0} can be used to save/restore
    \emph{all} predicate registers at once. The other bits of that
    register are currently reserved, but not used. Setting the reserved
	bits has no effect.
  \item \texttt{s1} can also be accessed through the name \texttt{sm} and
    represents the result of a decoupled load operation. The value is
    already sign-/zero-extended according to the load instruction.
  \item \texttt{s2} and \texttt{s3} can also be accessed through the names
    \texttt{sl} and \texttt{sh} and represent the lower and upper
    32-bits a multiplication.
  \item \texttt{s5} can also be accessed through the name \texttt{ss} and
    represents the register pointing to the top of the saved stack
	content in the main memory (i.e., the current stack spill pointer).
	Updating \texttt{s5} does not change \texttt{s6} or spill the stack cache.
  \item \texttt{s6} can also be accessed through the name \texttt{st} and
    represents a pointer to the top-most element of the content of the
    stack cache.
	Updating \texttt{s6} does not change \texttt{s5} or spill the stack cache.
\end{itemize}

\stefan{None of the registers should be read-only, as they need to be restored when
we switch contexts.}

\begin{figure}[p]

  \begin{multicols}{2}
    \centering

    \subfloat[\label{fig:registers:r}General-Purpose Registers (\texttt{R})]{
      \begin{bytefield}[bitwidth=.59em]{32}
        \bitheader{0-31} \\
        \bitbox{32}{r0 (zero, read-only)} \\
        \bitbox{32}{r1 (result, scratch)} \\
        \bitbox{32}{r2 (result 64-bit, scratch)} \\
        \bitbox{32}{r3 (argument 1, scratch)} \\
        \bitbox{32}{r4 (argument 2, scratch)} \\
        \bitbox{32}{r5 (argument 3, scratch)} \\
        \bitbox{32}{r6 (argument 4, scratch)} \\
        \bitbox{32}{r7 (argument 5, scratch)} \\
        \bitbox{32}{r8 (argument 6, scratch)} \\ \bitbox{32}{r9 (scratch)} \\
        \bitbox{32}{r10 (scratch)} \\ \bitbox{32}{r11 (scratch)} \\
        \bitbox{32}{r12 (scratch)} \\ \bitbox{32}{r13 (scratch)} \\
        \bitbox{32}{r14 (scratch)} \\ \bitbox{32}{r15 (scratch)} \\
        \bitbox{32}{r16 (scratch)} \\ \bitbox{32}{r17 (scratch)} \\
        \bitbox{32}{r18 (scratch)} \\ \bitbox{32}{r19 (scratch)} \\
        \bitbox{32}{r20 (scratch)} \\ \bitbox{32}{r21 (saved)} \\
        \bitbox{32}{r22 (saved)} \\ \bitbox{32}{r23 (saved)} \\
        \bitbox{32}{r24 (saved)} \\ \bitbox{32}{r25 (saved)} \\
        \bitbox{32}{r26 (saved)} \\ \bitbox{32}{r27 (saved)} \\
        \bitbox{32}{r28 (saved)} \\
        \bitbox{32}{r29 (temp.\ register, saved)} \\
        \bitbox{32}{r30 (frame pointer, saved)} \\
        \bitbox{32}{r31 (stack pointer, saved)} \\
      \end{bytefield}
    }

    \newpage

    \vspace*{\stretch{1}}

    \subfloat[\label{fig:registers:pr}Predicate Registers (\texttt{P})]{
      \newcommand{\bitlabel}[1]{
        \bitbox[]{1}{\raisebox{0pt}[5ex][1pt]{\turnbox{45}{\fontsize{7}{7}\selectfont#1}}}
      }
      \begin{bytefield}[bitwidth=1.5em]{8}
        \bitlabel{p7} \bitlabel{p6} \bitlabel{p5} \bitlabel{p4}
        \bitlabel{p3} \bitlabel{p2} \bitlabel{p1} \bitlabel{p0 -- Read only, always \texttt{1}} \\
        \bitheader{0-7} \\
        \bitbox{1}{} \bitbox{1}{} \bitbox{1}{} \bitbox{1}{}
        \bitbox{1}{} \bitbox{1}{} \bitbox{1}{} \bitbox{1}{} \\
      \end{bytefield}
    }

    \vspace*{\stretch{2}}

    \subfloat[\label{fig:registers:s}Special-Purpose Registers (\texttt{S})]{
      \begin{bytefield}[bitwidth=.59em]{32}
        \bitheader{0-31} \\
        \bitbox{24}{reserved} \bitbox{8}{p7 \dots p0} \bitbox[]{4}{s0} \\
        \bitbox{32}{sm (load result)} \bitbox[]{4}{s1} \\
        \bitbox{32}{sl (mul low)} \bitbox[]{4}{s2} \\
        \bitbox{32}{sh (mul high)} \bitbox[]{4}{s3} \\
        \bitbox{32}{s4} \\
        \bitbox{32}{ss (spill pointer)} \bitbox[]{4}{s5} \\
        \bitbox{32}{st (stack pointer)} \bitbox[]{4}{s6} \\
        \bitbox{32}{srb (return base)} \bitbox[]{4}{s7} \\
        \bitbox{32}{sro (return offset)} \bitbox[]{4}{s8} \\
        \bitbox{32}{sxb (exception return base)} \bitbox[]{4}{s9} \\
        \bitbox{32}{sxo (exception return offset)} \bitbox[]{4}{s10} \\
        \bitbox{32}{s11} \\
        \bitbox{32}{s12} \\ \bitbox{32}{s13} \\ \bitbox{32}{s14} \\ \bitbox{32}{s15} \\
      \end{bytefield}
    }

    \vspace*{\stretch{4}}~

  \end{multicols}

  \caption{General-purpose register file, predicate registers, and
           special-purpose registers of Patmos.}
  \label{fig:registers}
\end{figure}

\section{Bundle Formats}

All Patmos instructions are 32 bits wide and are structured according to
one of the instruction formats defined in the following section. Up to two
instructions can be combined to form an instruction bundle; Patmos bundles are
thus either 32 or 64 bits wide. The bundles sizes are recognized by the value of
the most significant bit, where $0$ indicates a short, 32-bit bundle and $1$ a
long, 64-bit bundle.

The following figures illustrate these two bundle variants:
\begin{itemize}
 \item 32-bit bundle format\\[2ex]
   \begin{bytefield}{32}
     \bitheader{0-31} \\
     \bitbox{1}{0} & \bitbox{31}{\bitssubclass} \\
   \end{bytefield}

 \item 64-bit bundle format \\[2ex]
   \begin{bytefield}[lsb=32]{32} \bitheader{32-63} \\
     \bitbox{1}{1} & \bitbox{31}{\bitssubclass} \\
   \end{bytefield}
   \hspace{.5em}
   \begin{bytefield}{32} \bitheader{0-31} \\
     \bitbox{1}{x} & \bitbox{31}{\bitssubclass} \\
   \end{bytefield}
\end{itemize}

\section{Instruction Formats}

This section gives an overview of all instruction formats defined in the Patmos
ISA. Individual instructions of the various formats are defined in the next
section. Gray fields indicate bits whose function is determined by a sub-class
of the instruction format. Black fields are not used.

\begin{itemize}
  \item ALUi -- Arithmetic Immediate \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{2}{00} & \bitbox{3}{Func} &
      \bitbox{5}{Rd} & \bitbox{5}{Rs1} & \bitbox{12}{Immediate} \\
    \end{bytefield}
  \item ALUl -- Long Immediate \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{1} & \bitbox{4}{Pred} & \bitbox{5}{11111} &
      \bitbox{5}{Rd} & \bitbox{5}{Rs1} & \bitbox{5}{\bitsunused} &
      \bitbox{3}{000} & \bitbox{4}{Func} \\
      \bitheader{0-31} \\
      \bitbox{32}{Long Immediate} \\
    \end{bytefield}
  \item ALU -- Arithmetic \\[2ex]
        \begin{bytefield}{32}
          \bitheader{0-31} \\
          \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
          \bitbox{15}{\bitssubclass} &
          \bitbox{3}{Opc} & \bitbox{4}{Func} \\
        \end{bytefield}

        \begin{bytefield}[leftcurly=.]{32}
          \begin{leftwordgroup}{\parbox{8em}{ALUr -- Register}}
            \bitheader{0-31} \\
            \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
            \bitbox{5}{Rd} & \bitbox{5}{Rs1} & \bitbox{5}{Rs2} &
            \bitbox{3}{000} & \bitbox{4}{Func}
          \end{leftwordgroup} \\
          \begin{leftwordgroup}{\parbox{8em}{ALUm -- Multiply}}
            \bitheader{0-31} \\
            \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
            \bitbox{5}{\bitsunused} & \bitbox{5}{Rs1} & \bitbox{5}{Rs2} &
            \bitbox{3}{010} & \bitbox{4}{Func}
          \end{leftwordgroup} \\
          \begin{leftwordgroup}{\parbox{8em}{ALUc -- Compare}}
            \bitheader{0-31} \\
            \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
            \bitbox{2}{\bitsunused} & \bitbox{3}{Pd} & \bitbox{5}{Rs1} & \bitbox{5}{Rs2} &
            \bitbox{3}{011} & \bitbox{4}{Func}
          \end{leftwordgroup} \\
          \begin{leftwordgroup}{\parbox{8em}{ALUp -- Predicate}}
            \bitheader{0-31} \\
            \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
            \bitbox{2}{\bitsunused} & \bitbox{3}{Pd} & \bitbox{1}{\bitsunused} & \bitbox{4}{Ps1} & \bitbox{1}{\bitsunused} & \bitbox{4}{Ps2} &
            \bitbox{3}{100} & \bitbox{4}{Func}
          \end{leftwordgroup} \\
          \begin{leftwordgroup}{\parbox{8em}{ALUb -- Bitcopy}}
            \bitheader{0-31} \\
            \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
            \bitbox{5}{Rd} & \bitbox{5}{Rs1} & \bitbox{5}{Imm} &
            \bitbox{3}{101} & \bitbox{4}{Ps}
          \end{leftwordgroup} \\
          \begin{leftwordgroup}{\parbox{8em}{ALUci -- Compare immediate}}
            \bitheader{0-31} \\
            \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
            \bitbox{2}{\bitsunused} & \bitbox{3}{Pd} & \bitbox{5}{Rs1} & \bitbox{5}{Imm} &
            \bitbox{3}{110} & \bitbox{4}{Func}
          \end{leftwordgroup} \\
          %% \begin{leftwordgroup}{\parbox{8em}{Unused}}
          %%   \bitheader{0-31} \\
          %%   \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
          %%   \bitbox{15}{\bitssubclass} &
          %%   \bitbox{3}{111} & \bitbox{4}{Func}
          %% \end{leftwordgroup} \\
        \end{bytefield}

  \item SPC -- Special \\[2ex]
        \begin{bytefield}{32}
          \bitheader{0-31} \\
          \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} &
          \bitbox{15}{\bitssubclass} &
          \bitbox{3}{Opc} & \bitbox{4}{I/R/F} \\
        \end{bytefield}

        \begin{bytefield}[leftcurly=.]{32}
          \begin{leftwordgroup}{\parbox{11em}{SPCw -- Wait}}
            \bitheader{0-31} \\
            \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} &
            \bitbox{15}{\bitsunused} &
            \bitbox{3}{001} & \bitbox{4}{Func}
          \end{leftwordgroup} \\
          \begin{leftwordgroup}{\parbox{11em}{SPCt -- Move To Special}}
            \bitheader{0-31} \\
            \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} &
            \bitbox{5}{\bitsunused} & \bitbox{5}{Rs1} & \bitbox{5}{\bitsunused} &
            \bitbox{3}{010} & \bitbox{4}{Sd}
          \end{leftwordgroup} \\
          \begin{leftwordgroup}{\parbox{11em}{SPCf -- Move From Special}}
            \bitheader{0-31} \\
            \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} &
            \bitbox{5}{Rd} & \bitbox{10}{\bitsunused} &
            \bitbox{3}{011} & \bitbox{4}{Ss}
          \end{leftwordgroup} \\
          %% \begin{leftwordgroup}{\parbox{11em}{Unused}}
          %%   \bitheader{0-31} \\
          %%   \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} &
          %%   \bitbox{15}{\bitssubclass} &
          %%   \bitbox{3}{000} & \bitbox{4}{I/R/F}
          %% \end{leftwordgroup} \\
          %% \begin{leftwordgroup}{\parbox{11em}{Unused}}
          %%   \bitheader{0-31} \\
          %%   \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} &
          %%   \bitbox{15}{\bitssubclass} &
          %%   \bitbox{3}{100} & \bitbox{4}{I/R/F}
          %% \end{leftwordgroup} \\
          %% \begin{leftwordgroup}{\parbox{11em}{Unused}}
          %%   \bitheader{0-31} \\
          %%   \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} &
          %%   \bitbox{15}{\bitssubclass} &
          %%   \bitbox{3}{101} & \bitbox{4}{I/R/F}
          %% \end{leftwordgroup} \\
          %% \begin{leftwordgroup}{\parbox{11em}{Unused}}
          %%   \bitheader{0-31} \\
          %%   \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} &
          %%   \bitbox{15}{\bitssubclass} &
          %%   \bitbox{3}{110} & \bitbox{4}{I/R/F}
          %% \end{leftwordgroup} \\
          %% \begin{leftwordgroup}{\parbox{11em}{Unused}}
          %%   \bitheader{0-31} \\
          %%   \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} &
          %%   \bitbox{15}{\bitssubclass} &
          %%   \bitbox{3}{111} & \bitbox{4}{I/R/F}
          %% \end{leftwordgroup}
        \end{bytefield}

  \item LDT -- Load Typed \\[2ex]
        \begin{bytefield}{32}
          \bitheader{0-31} \\
          \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01010} &
          \bitbox{5}{Rd} & \bitbox{5}{Ra} & \bitbox{5}{Type} & \bitbox{7}{Offset} \\
        \end{bytefield}
  \item STT -- Store Typed \\[2ex]
        \begin{bytefield}{32}
          \bitheader{0-31} \\
          \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01011} &
          \bitbox{5}{Type} & \bitbox{5}{Ra} & \bitbox{5}{Rs} & \bitbox{7}{Offset} \\
        \end{bytefield}

  \item STC -- Stack Control \\[2ex]
        \begin{bytefield}{32}
          \bitheader{0-31} \\
          \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01100} &
          \bitbox{2}{Op} & \bitbox{2}{F} & \bitbox{18}{\bitssubclass} \\
        \end{bytefield}

        \begin{bytefield}[leftcurly=.]{32}
          \begin{leftwordgroup}{\parbox{13em}{STCi -- Stack Control Immediate}}
            \bitheader{0-31} \\
            \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01100} &
            \bitbox{2}{Op} & \bitbox{2}{00} &
            \bitbox{18}{Immediate}
          \end{leftwordgroup} \\
          \begin{leftwordgroup}{\parbox{13em}{STCr -- Stack Control Register}}
            \bitheader{0-31} \\
            \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01100} &
            \bitbox{2}{Op} & \bitbox{2}{01} &
            \bitbox{1}{\bitsunused} & \bitbox{5}{Rs} & \bitbox{12}{\bitsunused}
          \end{leftwordgroup}
        \end{bytefield}

  \item CFLi -- Control Flow with Immediate \nopagebreak\\[2ex]
        \begin{bytefield}{32}
          \bitheader{0-31} \\
          \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{2}{10} & \bitbox{2}{Op} & \bitbox{1}{d} &
          \bitbox{22}{Immediate} \\
        \end{bytefield}

  \item CFLr -- Control Flow with Registers \\[2ex]
        \begin{bytefield}{32}
          \bitheader{0-31} \\
          \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{4}{1100} & \bitbox{1}{d} &
          \bitbox{18}{\bitssubclass} & \bitbox{2}{F} & \bitbox{2}{Op} \\
        \end{bytefield}

        \begin{bytefield}[leftcurly=.]{32}
          \begin{leftwordgroup}{\parbox{18em}{CFLri -- Control Flow with implicit registers}}
          \bitheader{0-31} \\
          \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{4}{1100} & \bitbox{1}{d} &
          \bitbox{18}{\bitsunused} &
          \bitbox{2}{00} & \bitbox{2}{Op}
          \end{leftwordgroup}\\
          \begin{leftwordgroup}{\parbox{18em}{CFLrs -- Control Flow with single register}}
          \bitheader{0-31} \\
          \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{4}{1100} & \bitbox{1}{d} &
          \bitbox{5}{\bitsunused} & \bitbox{5}{Rs} & \bitbox{8}{\bitsunused} &
          \bitbox{2}{01} & \bitbox{2}{Op}
          \end{leftwordgroup}\\
          \begin{leftwordgroup}{\parbox{18em}{CFLrt -- Control Flow with two registers}}
          \bitheader{0-31} \\
          \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{4}{1100} & \bitbox{1}{d} &
          \bitbox{5}{\bitsunused} & \bitbox{5}{Rs1} & \bitbox{5}{Rs2} & \bitbox{3}{\bitsunused} &
          \bitbox{2}{10} & \bitbox{2}{Op}
          \end{leftwordgroup}
        \end{bytefield}
\end{itemize}

\clearpage
\section{Instruction Opcodes}
\label{sec:instruction_opcodes}

This section defines the instruction set architecture, the instruction opcodes,
and the behavior of the respective instructions of Patmos. This section should
be less used for discussions and should slowly converge to a final definition
of the instruction set.

\subsection{Binary Arithmetic}

Applies to the ALUr, ALUi, and ALUl formats.  Operand \texttt{Op2}
denotes either the \texttt{Rs2}, or the \texttt{Immediate} operand, or
the \texttt{Long Immediate}. The immediate operand is zero-extended.  For
shift and rotate operations, only the lower $5$ bits of the operand
are considered. Table~\ref{tab:alufunc} shows the encoding of the
\texttt{func} field; for ALUi instructions, only functions in the
upper half of that table are available.

\begin{itemize}
  \item ALUr -- Register \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
      \bitbox{5}{Rd} & \bitbox{5}{Rs1} & \bitbox{5}{Rs2} &
      \bitbox{3}{000} & \bitbox{4}{Func} \\
    \end{bytefield}
  \item ALUi -- Arithmetic Immediate \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{2}{00} & \bitbox{3}{Func} &
      \bitbox{5}{Rd} & \bitbox{5}{Rs1} & \bitbox{12}{Immediate} \\
    \end{bytefield}
  \item ALUl -- Long Immediate\\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{1} & \bitbox{4}{Pred} & \bitbox{5}{11111} &
      \bitbox{5}{Rd} & \bitbox{5}{Rs1} & \bitbox{5}{\bitsunused} &
      \bitbox{3}{000} & \bitbox{4}{Func} \\
      \bitheader{0-31} \\
      \bitbox{32}{Long Immediate} \\
   \end{bytefield}
\end{itemize}

\begin{table}[hb]
  \centering
  \begin{tabular}{lll}
    \toprule
    Func & Name   & Semantics \\
    \midrule
    0000 & add    & \texttt{Rd = Rs1 + Op2} \\
    0001 & sub    & \texttt{Rd = Rs1 - Op2} \\
    0010 & xor    & \texttt{Rd = Rs1 \XOR Op2} \\
    0011 & sl     & \texttt{Rd = Rs1 \shl Op2$_{[0:4]}$} \\
    0100 & sr     & \texttt{Rd = Rs1 \shr Op2$_{[0:4]}$} \\
    0101 & sra    & \texttt{Rd = Rs1 \ashr Op2$_{[0:4]}$} \\
    0110 & or     & \texttt{Rd = Rs1 \OR Op2} \\
    0111 & and    & \texttt{Rd = Rs1 \AND Op2} \\
    \cmidrule{1-3}
    1000 & ---	& unused \\
    1001 & ---    & unused \\
    1010 & ---    & unused \\
    1011 & nor    & \texttt{Rd = \NOT (Rs1 \OR Op2)} \\
    1100 & shadd  & \texttt{Rd = (Rs1 \shl 1) + Op2} \\
    1101 & shadd2 & \texttt{Rd = (Rs1 \shl 2) + Op2} \\
    1110 & ---    & unused \\
    1111 & ---    & unused \\
    \bottomrule
  \end{tabular}
  \caption{General ALU functions}
  \label{tab:alufunc}
\end{table}

\paragraph{Pseudo Instructions}
\begin{itemize}
  \item \texttt{mov Rd = Rs}~\dots~\texttt{add Rd = Rs + 0}
  \item \texttt{clr Rd}~\dots~\texttt{add Rd = r0 + 0}
  \item \texttt{neg Rd = -Rs}~\dots~\texttt{sub Rd = 0 - Rs}
  \item \texttt{not Rd = \NOT Rs}~\dots~\texttt{nor Rd = \NOT (Rs \OR R0)}
  \item \texttt{li Rd = Immediate}~\dots~\texttt{add Rd = r0 + Immediate}
  \item \texttt{li Rd = Immediate}~\dots~\texttt{sub Rd = r0 - Immediate}
  \item \texttt{nop}~\dots~\texttt{sub r0 = r0 - 0}
\end{itemize}


\paragraph{Note}
The use of \texttt{sub r0 = r0 - 0} to encode a \texttt{nop}
pseudo-instruction results in a value of \texttt{0x00400000} in the
binary instruction stream. This helps in distinguishing the execution
of compiler-generated \texttt{nops} from executing instructions from
memory that happens to be zero.

\clearpage
\subsection{Multiply}

Applies to the ALUm format only. Multiplications are executed in
parallel with the regular pipeline and finish within a fixed number of
cycles \todo{how many?}. Table~\ref{tab:mulfunc} shows the encoding of
the \texttt{func} field for the ALUm instruction format.

\begin{itemize}
  \item ALUm -- Multiply \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
      \bitbox{5}{\bitsunused} & \bitbox{5}{Rs1} & \bitbox{5}{Rs2} &
      \bitbox{3}{010} & \bitbox{4}{Func} \\
    \end{bytefield}
\end{itemize}

\begin{table}[hb]
  \centering
  \begin{tabular}{lll}
    \toprule
    Func & Name   & Semantics \\
    \midrule
    0000 & mul    & \texttt{sl = Rs1 * Rs2;}\\
         &        & \texttt{sh = (Rs1 * Rs2) \shr 32} \\
    0001 & mulu   & \texttt{sl = (uint32\_t)Rs1 * (uint32\_t)Rs2;} \\
         &        & \texttt{sh = ((uint32\_t)Rs1 * (uint32\_t)Rs2) \shr 32} \\
    0010 & ---    & unused \\
    \dots& \dots  & \dots \\
    1111 & ---    & unused \\
    \bottomrule
  \end{tabular}
  \caption{Multiplication functions}
  \label{tab:mulfunc}
\end{table}


\paragraph{Behavior}

Perform multiplication in multiple cycles and write the result into
destination registers \texttt{sl} and \texttt{sh}.

\paragraph{Note}

Multiplications are pipelined, it is thus possible to issue one multiplication on
every cycles. Multiplications can only be issued in the first slot.

\stefan{Not yet final. 64bit support and the special registers for multiply might
vanish. Maybe merge mul and mulu into the ALU Func field somehow to kill off the ALUm format?
We could keep the special register to access the high word and write the low
word into a GPR when using ALUl format, so we save one mfs per mul.
(remove this comment when multiply ISA is (somewhat) finalized)}

\clearpage
\subsection{Compare}

Applies to the ALUc and ALUci formats only. Operand \texttt{Op2}
denotes either the \texttt{Rs2}, or the \texttt{Imm}
operand. Tables~\ref{tab:cmpfunc} show the encoding of the
\texttt{func} field for the ALUc and ALUci formats.

\begin{itemize}
  \item ALUc -- Compare \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
      \bitbox{2}{\bitsunused} & \bitbox{3}{Pd} & \bitbox{5}{Rs1} & \bitbox{5}{Rs2} &
      \bitbox{3}{011} & \bitbox{4}{Func} \\
    \end{bytefield}
  \item ALUci -- Compare immediate \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
      \bitbox{2}{\bitsunused} & \bitbox{3}{Pd} & \bitbox{5}{Rs1} & \bitbox{5}{Imm} &
      \bitbox{3}{110} & \bitbox{4}{Func} \\
    \end{bytefield}
\end{itemize}

\begin{table}[hb]
  \centering
  \begin{tabular}{lll}
    \toprule
    Func & Name   & Semantics \\
    \midrule
    0000 & cmpeq  & \texttt{Pd = Rs1 ==  Op2} \\
    0001 & cmpneq & \texttt{Pd = Rs1 !=  Op2} \\
    0010 & cmplt  & \texttt{Pd = Rs1 \textless\ Op2} \\
    0011 & cmple  & \texttt{Pd = Rs1 \textless= Op2} \\
    0100 & cmpult & \texttt{Pd = Rs1 \textless\ Op2, unsigned} \\
    0101 & cmpule & \texttt{Pd = Rs1 \textless= Op2, unsigned} \\
    0110 & btest  & \texttt{Pd = (Rs1 \AND (1 \shl Op2)) != 0} \\
    0111 & ---    & unused \\
    \dots& \dots  & \dots \\
    1111 & ---    & unused \\
    \bottomrule
  \end{tabular}
  \caption{Compare functions}
  \label{tab:cmpfunc}
\end{table}

\paragraph{Pseudo Instructions}
\begin{itemize}
  \item \texttt{isodd Pd = Rs1}~\dots~\texttt{btest Pd = Rs1[r0]}
  \item \texttt{mov Pd = Rs}~\dots~\texttt{cmpneq Pd = Rs != r0}
\end{itemize}

\paragraph{Note}

The predicate register is read and written in the execute stage.

\stefan{We discussed about adding short immediate versions, but with the current predicate handling this is not
a trivial change for the compiler, so we skip that. Instead, we might introduce bez/bnez instructions, which is much more common anyway.}

\clearpage
\subsection{Predicate}

Applies to the ALUp format only, the opcodes correspond to those of
the ALU operations on general purpose
registers. Table~\ref{tab:predfunc} shows the encoding of the
\texttt{func} field for the ALUp format.

\martin{Why do we have this encoding of predicate operations with empty function
codes in-between? No need to be `compatible' with ALU operations, just makes decoding
a little bit more complex.}

\begin{itemize}
  \item ALUp -- Predicate \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
      \bitbox{2}{\bitsunused} & \bitbox{3}{Pd} & \bitbox{1}{\bitsunused} & \bitbox{4}{Ps1} & \bitbox{1}{\bitsunused} & \bitbox{4}{Ps2} &
      \bitbox{3}{100} & \bitbox{4}{Func} \\
    \end{bytefield}
\end{itemize}

\begin{table}[hb]
  \centering
  \begin{tabular}{lll}
    \toprule
    Func & Name   & Semantics \\
    \midrule
    0000 & ---    & unused \\
    \dots& \dots  & \dots \\
    0101 & ---    & unused \\
    0110 & por     & \texttt{Pd = Ps1 \OR Ps2} \\
    0111 & pand    & \texttt{Pd = Ps1 \AND Ps2} \\
    1000 & ---    & unused \\
    1001 & ---    & unused \\
    1010 & pxor    & \texttt{Pd = Ps1 \XOR Ps2} \\
    1011 & ---    & unused \\
    \dots& \dots  & \dots \\
    1111 & ---    & unused \\
    \bottomrule
  \end{tabular}
  \caption{Predicate functions}
  \label{tab:predfunc}
\end{table}

\paragraph{Pseudo Instructions}
\begin{itemize}
  \item \texttt{pmov Pd = Ps}~\dots~\texttt{por Pd = Ps \OR Ps}
  \item \texttt{pnot Pd = \NOT Ps}~\dots~\texttt{pxor Pd = (Ps \XOR p0)}
  \item \texttt{pset Pd = 1}~\dots~\texttt{por Pd = p0 \OR p0}
  \item \texttt{pclr Pd = 0}~\dots~\texttt{pxor Pd = p0 \XOR p0}
\end{itemize}

\paragraph{Note}
The predicate register is read and written in the execute stage.
All predicate combine instruction mnemonics (including pseudo instructions) are prefixed with \texttt{p},
all other instructions involving predicates are not prefixed (e.g., moving from register to predicate).

\stefan{We have a mov with two registers (\texttt{add}), mov with two predicates (\texttt{por}), a mov from register to
predicate (\texttt{cmpnez}), as well as mov from register bit to predicate (\texttt{btest}), but we are missing a
mov from predicate to register (is now a predicated set and predicated clr), and mov from predicate to register bit (is now done with
bitmasks).}

%Applies to the SPCn format only. Issue \texttt{Imm} NOP
%operations to the pipeline. The main idea is to save code size, however, the
%multi-cycle NOP might also be used to enforce certain timing constraints, e.g.,
%wait for a certain amount of time, ensure a minimal execution time, et cetera.
%
%\begin{itemize}
%  \item SPCn -- NOP \\[2ex]
%        \begin{bytefield}{32} \bitheader{0-31}\\ \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} & \bitbox{15}{\bitsunused} & \bitbox{3}{000} & \bitbox{4}{Imm} \end{bytefield}\\
%\end{itemize}
%
%\paragraph{Behavior}
%\begin{itemize}
%  \item[\texttt{IF}] --
%  \item[\texttt{DR}] Read register operands \texttt{Pred}.\\
%                     \texttt{tmp} = \texttt{Pred} ? \texttt{Imm} : 0. \\
%                     while (\texttt{tmp}-\,- != 0) \{ stall \texttt{DR}; next cycle; \}
%  \item[\texttt{EX}] --
%  \item[\texttt{MW}] --
%\end{itemize}
%
%\paragraph{Note}
%
%A NOP can only be issued at the first position within a bundle. This does in no
%way restrict the use of the NOP. It can always be scheduled in the next/previous
%bundle, while incrementing/decrementing the number of NOP cycles accordingly;
%the code size is in both cases not increased.


\clearpage
\subsection{Bitcopy}

Applies to the ALUb format only. The only instruction with this
encoding is \texttt{bcopy}, which is the ``inverse'' of the
\code{btest} instruction. It has the following semantics: \texttt{Rd =
  (Rs1 \AND \NOT (1 \shl Imm)) \OR (Ps \shl Imm)}

\begin{itemize}
  \item ALUb -- Bitcopy \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01000} &
      \bitbox{5}{Rd} & \bitbox{5}{Rs1} & \bitbox{5}{Imm} &
      \bitbox{3}{101} & \bitbox{4}{Ps} \\
    \end{bytefield}
\end{itemize}

\paragraph{Pseudo Instructions}
\begin{itemize}
  \item \texttt{mov Rd = Ps}~\dots~\texttt{bcopy Rd = r0, 0, Ps}
\end{itemize}

\paragraph{Note}
The predicate register is read in the execute stage.

\clearpage
\subsection{Wait}

Applies to the SPCw format only. Wait for a multiplication or memory
operation to complete by stalling the
pipeline. Table~\ref{tab:waitfunc} shows the encoding of the
\code{func} field.

\begin{itemize}
  \item SPCw -- Wait \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} &
      \bitbox{15}{\bitsunused} &
      \bitbox{3}{001} & \bitbox{4}{Func} \\
    \end{bytefield}
\end{itemize}

\begin{table}[hb]
  \centering
  \begin{tabular}{lll}
    \toprule
    Func & Name     & Semantics \\
    \midrule
    0000 & wait.mem & Wait for a memory access \\
    0001 & ---    & unused \\
    \dots& \dots  & \dots \\
    1111 & ---    & unused \\
    \bottomrule
  \end{tabular}
  \caption{Wait function encoding}
  \label{tab:waitfunc}
\end{table}

\paragraph{Note}

A Wait can only be issued at the first position within a bundle.
This instruction is not (yet) implemented in hardware as we do not
(yet) support split loads. \martin{And maybe we will never do as
the gain is not worth the effort.}

\clearpage
\subsection{Move To Special}

Applies to the SPCt format only. Copy the value of a general-purpose
register to a special-purpose register. The only instruction is
\texttt{mts}, which stores the content of general-purpose register
\texttt{Rs1} in special register \texttt{Sd}.

\begin{itemize}
  \item SPCt -- Move To Special \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} &
      \bitbox{5}{\bitsunused} & \bitbox{5}{Rs1} & \bitbox{5}{\bitsunused} &
      \bitbox{3}{010} & \bitbox{4}{Sd} \\
    \end{bytefield}
\end{itemize}



\paragraph{Note}



Special registers \texttt{sm}, \texttt{sl}, \texttt{sh} are read-only, writing
to those registers may result in undefined behavior.

\stefan{What should we do in case of a context switch? We would either need to restart the last load and multiply, or restore all special registers..}
\martin{Restart sounds problematic. I would say the special registers are part of the context and need to be saved and
than writable.}

\wolf{Saving \texttt{sm} can be faked by storing the contents of
  \texttt{sm} to a reserved memory location and loading from there to
  restore it. \texttt{sl} and \texttt{sh} should be writable though.}

\martin{\todo{We should document in which stage each special register
is read and written.}}

%\martin{But I don't think we need a 64-bit result in the
%multiplication. No direct support in C. If we do want to support more fancy
%DSP stuff I would actually go for a MAC operation. But not for now...}
%
%\wolf{The 64-bit result is already implemented in the hardware, so I guess we can keep it...}
%
%\martin{Yes. Thanks.} -- Text can be removed after reading

\clearpage
\subsection{Move From Special}

Applies to the SPCf format only. Copy the value of a special-purpose
register to a general-purpose register. The only instruction is
\texttt{mfs}, which loads the content of special register \texttt{Ss}
to general-purpose register \texttt{Rd}.

\begin{itemize}
  \item SPCf -- Move From Special \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01001} &
      \bitbox{5}{Rd} & \bitbox{10}{\bitsunused} &
      \bitbox{3}{011} & \bitbox{4}{Ss} \\
    \end{bytefield}
\end{itemize}

\fb{It would make sense to merge the \texttt{wait} instructions with this
    instruction?!? This could also simplify the definition of decoupled loads.}

\stefan{This would be a good thing.
On the other hand, if this also means that this instruction can only be issued as first operation, according to the definition of wait
above, then we cannot read the result of a split load and start the next load in the same cycle.

Are we able to issue two \texttt{mfs} ops in one cycle (e.g., read return base and return offset in the same cycle)?}

\clearpage
\subsection{Load Typed}
\label{subsec:load_typed}

Applies to the LDT format only. Load from a memory or
cache. In the table accesses to the stack cache are denoted by \texttt{sc}, to
the local scratchpad memory by \texttt{lm}, to the date cache by \texttt{dc},
and to the global shared memory by \texttt{gm}. By default all load variants are
considered with an implicit \texttt{wait} -- which causes the load to stall
until the memory access is completed.

In addition, \emph{decoupled} loads are provided that do \emph{not wait} for the
memory access to be completed. The result is then loaded into the special
register \texttt{sm}. A dedicated \texttt{wait.mem} instruction can be used to
stall the pipeline explicitly until the load is completed.

If a decoupled load is executed while another decoupled load is still in
progress, the pipeline will be stalled implicitly until the already running load
is completed before the next memory access is issued. The value of the previous
load can then be read from \texttt{sm} for (at least) one cycle.

Regular loads incur a load-to-use latency that has to be respected by
the compiler/programmer. The result of the load is not available in
the bundle immediately after the load, i.e., there must be one bundle
between the load instruction and the first use of the destination
register. The value of the destination register is undefined during
this load delay slot.

The displacement value (\code{Imm}) value is interpreted unsigned.

\begin{itemize}
  \item LDT -- Load Typed \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01010} &
      \bitbox{5}{Rd} & \bitbox{5}{Ra} & \bitbox{5}{Type} & \bitbox{7}{Immediate} \\
    \end{bytefield}
\end{itemize}

\begin{table}[htb!]
  \centering
\begin{tabular}{lll}
  \toprule
  Type            & Name   & Semantics \\
  \midrule
  000 \textbar~00 & lws    & \texttt{Rd=sc[Ra+(Imm \shl 2)]$_{32}$} \\
  000 \textbar~01 & lwl    & \texttt{Rd=lm[Ra+(Imm \shl 2)]$_{32}$ } \\
  000 \textbar~10 & lwc    & \texttt{Rd=dc[Ra+(Imm \shl 2)]$_{32}$} \\
  000 \textbar~11 & lwm    & \texttt{Rd=gm[Ra+(Imm \shl 2)]$_{32}$} \\
  001 \textbar~00 & lhs    & \texttt{Rd=(int32\_t)sc[Ra+(Imm \shl 1)]$_{16}$} \\
  001 \textbar~01 & lhl    & \texttt{Rd=(int32\_t)lm[Ra+(Imm \shl 1)]$_{16}$ } \\
  001 \textbar~10 & lhc    & \texttt{Rd=(int32\_t)dc[Ra+(Imm \shl 1)]$_{16}$} \\
  001 \textbar~11 & lhm    & \texttt{Rd=(int32\_t)gm[Ra+(Imm \shl 1)]$_{16}$} \\
  010 \textbar~00 & lbs    & \texttt{Rd=(int32\_t)sc[Ra+Imm]$_{8}$} \\
  010 \textbar~01 & lbl    & \texttt{Rd=(int32\_t)lm[Ra+Imm]$_{8}$ } \\
  010 \textbar~10 & lbc    & \texttt{Rd=(int32\_t)dc[Ra+Imm]$_{8}$} \\
  010 \textbar~11 & lbm    & \texttt{Rd=(int32\_t)gm[Ra+Imm]$_{8}$} \\
  011 \textbar~00 & lhus   & \texttt{Rd=(uint32\_t)sc[Ra+(Imm \shl 1)]$_{16}$} \\
  011 \textbar~01 & lhul   & \texttt{Rd=(uint32\_t)lm[Ra+(Imm \shl 1)]$_{16}$ } \\
  011 \textbar~10 & lhuc   & \texttt{Rd=(uint32\_t)dc[Ra+(Imm \shl 1)]$_{16}$} \\
  011 \textbar~11 & lhum   & \texttt{Rd=(uint32\_t)gm[Ra+(Imm \shl 1)]$_{16}$} \\
  100 \textbar~00 & lbus   & \texttt{Rd=(uint32\_t)sc[Ra+Imm]$_{8}$} \\
  100 \textbar~01 & lbul   & \texttt{Rd=(uint32\_t)lm[Ra+Imm]$_{8}$ } \\
  100 \textbar~10 & lbuc   & \texttt{Rd=(uint32\_t)dc[Ra+Imm]$_{8}$} \\
  100 \textbar~11 & lbum   & \texttt{Rd=(uint32\_t)gm[Ra+Imm]$_{8}$} \\
  \cmidrule{1-3}
  1010 \textbar~0 & dlwc   & \texttt{sm=dc[Ra+(Imm \shl 2)]$_{32}$} \\
  1010 \textbar~1 & dlwm   & \texttt{sm=gm[Ra+(Imm \shl 2)]$_{32}$} \\
  1011 \textbar~0 & dlhc   & \texttt{sm=(int32\_t)dc[Ra+(Imm \shl 1)]$_{16}$} \\
  1011 \textbar~1 & dlhm   & \texttt{sm=(int32\_t)gm[Ra+(Imm \shl 1)]$_{16}$} \\
  1100 \textbar~0 & dlbc   & \texttt{sm=(int32\_t)dc[Ra+Imm]$_{8}$} \\
  1100 \textbar~1 & dlbm   & \texttt{sm=(int32\_t)gm[Ra+Imm]$_{8}$} \\
  1101 \textbar~0 & dlhuc  & \texttt{sm=(uint32\_t)dc[Ra+(Imm \shl 1)]$_{16}$} \\
  1101 \textbar~1 & dlhum  & \texttt{sm=(uint32\_t)gm[Ra+(Imm \shl 1)]$_{16}$} \\
  1110 \textbar~0 & dlbuc  & \texttt{sm=(uint32\_t)dc[Ra+Imm]$_{8}$} \\
  1110 \textbar~1 & dlbum  & \texttt{sm=(uint32\_t)gm[Ra+Imm]$_{8}$} \\
  \cmidrule{1-3}
  11110 & ---    & unused \\
  11111 & ---    & unused \\
  \bottomrule
\end{tabular}
\caption{Typed loads}
\label{tab:loads}
\end{table}


\martin{I don't think that (uint32\_t) does the zero extension. But I might be wrong
as my C knowledge is rusty. We should use the H\&P MIPS green card notion.}



\martin{Do we do a memory read even when the predicate is false?
Some memory locations (I/O) might have side effects on a read.
Therefore, we shall not read when the predicate is false.}



\paragraph{Note}

All loads can only be issued on the first slot.

Two successive decoupled loads can be used in the following manner without the
use of an additional \texttt{wait} instruction:

\texttt{~~dlwc   [\$r1 + 5]}

\texttt{~~...}

\texttt{\{ dlwc   [\$r2 + 7]   ;   mfs  \$r2 = \$sm \}}

\clearpage
\subsection{Store Typed} Applies to the STT format only. Store to a memory or
cache. In the table accesses to the stack cache are denoted by \texttt{sc}, to
the local scratchpad memory by \texttt{lm}, to the date cache by \texttt{dc},
and to the global shared memory by \texttt{gm}.

\martin{TODO: make is clear at some point that \code{gm} is basically
data cache bypass. Would be natural for I/O, but mapping I/O to \code{lm},
as it is currently in passim, is also fine.}

The displacement value (\code{Imm}) value is interpreted unsigned.
Stores can only be issued on the first slot.

\begin{itemize}
  \item STT -- Store Typed \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01011} &
      \bitbox{5}{Type} & \bitbox{5}{Ra} & \bitbox{5}{Rs} & \bitbox{7}{Offset} \\
    \end{bytefield}
\end{itemize}

\begin{table}[hb]
  \centering
\begin{tabular}{lll}
  \toprule
  Type            & Name   & Semantics \\
  \midrule
  000 \textbar~00 & sws    & \texttt{sc[Ra+(Imm \shl 2)]$_{32}$ = Rs} \\
  000 \textbar~01 & swl    & \texttt{lm[Ra+(Imm \shl 2)]$_{32}$ = Rs} \\
  000 \textbar~10 & swc    & \texttt{dc[Ra+(Imm \shl 2)]$_{32}$ = Rs} \\
  000 \textbar~11 & swm    & \texttt{gm[Ra+(Imm \shl 2)]$_{32}$ = Rs} \\
  001 \textbar~00 & shs    & \texttt{sc[Ra+(Imm \shl 1)]$_{16}$ = Rs$_{[15:0]}$} \\
  001 \textbar~01 & shl    & \texttt{lm[Ra+(Imm \shl 1)]$_{16}$ = Rs$_{[15:0]}$} \\
  001 \textbar~10 & shc    & \texttt{dc[Ra+(Imm \shl 1)]$_{16}$ = Rs$_{[15:0]}$} \\
  001 \textbar~11 & shm    & \texttt{gm[Ra+(Imm \shl 1)]$_{16}$ = Rs$_{[15:0]}$} \\
  010 \textbar~00 & sbs    & \texttt{sc[Ra+Imm]$_{8}$ = Rs$_{[7:0]}$} \\
  010 \textbar~01 & sbl    & \texttt{lm[Ra+Imm]$_{8}$ = Rs$_{[7:0]}$} \\
  010 \textbar~10 & sbc    & \texttt{dc[Ra+Imm]$_{8}$ = Rs$_{[7:0]}$} \\
  010 \textbar~11 & sbm    & \texttt{gm[Ra+Imm]$_{8}$ = Rs$_{[7:0]}$} \\
  01100 & ---   & unused \\
  \dots& \dots  & \dots \\
  11111 & ---    & unused \\
  \bottomrule
\end{tabular}
\caption{Typed stores}
\label{tab:stores}
\end{table}


\paragraph{Note - Global Memory / Data Cache}

With regard the data cache, stores are performed using a \emph{write-through}
strategy without \emph{write-allocation}. Data that is not available in the
cache will not be loaded by stores; but will be updated if it is available in
the cache.

Consistency between loads and other stores is
assumed to be guaranteed by the memory interface, i.e., memory accesses are
handled in-order with respect to a specific processor. This has implications on
the bus, the network-on-chip, and the global memory.

% TODO:
% - Arbitration of global memory

\clearpage
\subsection{Stack Control} Applies to the STC format only. Manipulate the stack
frame in the stack cache. \texttt{sres} reserves space on the stack, potentially
spilling other stack frames to main memory. \texttt{sens} ensures that a stack
frame is entirely loaded to the stack cache, or otherwise refills the stack
cache as needed. \texttt{sfree} frees space on the stack frame (without any
other side effect, i.e., no spill/fill is executed).
\texttt{sspill} writes the tail of the stack cache to main memory and updates the
spill pointer.

All immediate stack control operations are carried out assuming word
size, i.e., the immediate operand is multiplied by four. All register
operands and stack pointer addresses in special registers are in units
of bytes.

A more detailed description of the stack cache is given in
Section~\ref{sec:stack-cache}. Table~\ref{tab:stciops} shows the
encoding of operations for STCi, while Table~\ref{tab:stcrops} shows
the encoding for STCr.

\martin{This or the stack cache section shall contain pseudo code for the
  stack cache.}

\martin{TODO: There is no mentioning in load/store section that stack ld/st
use an implicit stack pointer for addressing.}

\begin{itemize}
  \item STCi -- Stack Control Immediate \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01100} &
      \bitbox{2}{Op} & \bitbox{2}{00} & \bitbox{18}{Immediate} \\
    \end{bytefield}
\end{itemize}

% TODO: more formal semantics
\begin{table}[hb]
  \centering
  \begin{tabular}{lll}
    \toprule
    Op & Name   & Semantics \\
    \midrule
    00 & sres   & Reserve space on the stack (with spill) \\
    01 & sens   & Ensure stack space (with refill) \\
    10 & sfree  & Free stack space. \\
    11 & sspill & Spill tail of the stack cache to memory \\
    \bottomrule
  \end{tabular}
  \caption{Stack control operations with immediates}
  \label{tab:stciops}
\end{table}

\begin{itemize}
  \item STCr -- Stack Control Register \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{5}{01100} &
      \bitbox{2}{Op} & \bitbox{2}{01} & \bitbox{1}{\bitsunused} & \bitbox{5}{Rs} & \bitbox{12}{\bitsunused} \\
    \end{bytefield}
\end{itemize}

% TODO: more formal semantics
\begin{table}[hb]
  \centering
  \begin{tabular}{lll}
    \toprule
    Op & Name   & Semantics \\
    \midrule
    00 & ---    & unused \\
    01 & sens   & Ensure stack space (with refill) \\
    10 & ---    & unused \\
    11 & sspill & Spill tail of the stack cache to memory \\
    \bottomrule
  \end{tabular}
  \caption{Stack control operations for registers}
  \label{tab:stcrops}
\end{table}


\paragraph{Behavior}

\texttt{sres}: Check free space left in the stack cache. Update stack-cache registers.
If needed, spill to global memory using \texttt{ss}.

\texttt{sense}: Check reserved space available in the stack cache.
If needed, refill from global memory using \texttt{ss}.

\texttt{sfree}:  Account for $\mathtt{head} - \mathtt{tail} < 0$, update
                     \texttt{ss} and \texttt{st}.
                     Update stack-cache register \texttt{head}.

\texttt{sspill}: Update \texttt{ss} and \texttt{st}.
                     Update stack-cache register \texttt{tail}.
Spill to global memory using \texttt{ss}.

\paragraph{Note}

Stack control instructions can only be issued on the first position within a
bundle.

It is permissible to use several reserve, ensure, and free operations within the
same function.


\stefan{We only need sens and sspill with registers for context-switch. Should we allow a
register version for sres and sfree as well nevertheless (I would say no)?}

\stefan{At the moment, the way it is implemented in the simulator, we have a 1-cycle hazard between
any STC instruction and a mfs, and a 1-cycle hazard between any two STC instructions, since STC
modifies the special registers in the MW stage, while mfs reads in the EX stage.
Is this the same for the hardware?

This (and all hazards for all other instructions) should be documented here in the TR explicitly, since it is a pain to debug!}

\sahar{Special registers are written/read separately. Is this 1-cycle hazard necessary? What is the reason to add it?}

\stefan{The reason for the hazards is primarily the current implementation of the simulator, there the special registers
are updated separately from the internal stack pointers, which causes the hazards. We should define how the
hardware actually behaves, and then the simulator should be updated to reflect that behavior. However, the latter is not quite trivial, I will
leave that to Florian ;) }

\clearpage
\subsection{Control-Flow Instructions}

Applies to CFLi and CFLr format only.
Transfer control to another function or perform function-local branches.
\texttt{br} performs a function-local branch.
\texttt{call} performs a function call, storing the return information
(i.e., where to resume execution when returning) in
\texttt{srb}/\texttt{sro}.
\texttt{brcf} (``branch with cache fill'') performs a global
branch. With regard to addressing modes and caching, \texttt{brcf}
behaves like a call, but it does not store any return information.
\texttt{trap} performs a system call (see Section~\ref{sec:exc}).
\texttt{ret} returns from a function, using the return information in
\texttt{srb}/\texttt{sro}.
\texttt{xret} is similar to \code{ret} but uses the return information
in \texttt{sxb}/\texttt{sxo}, which are set by interrupts, exceptions,
and traps.

With a method cache, \texttt{call}, \texttt{brfc}, \texttt{trap},
\texttt{ret}, and \texttt{xret} may cause a cache miss and a
subsequent cache refill to load the target code; they expect the size
of the code block fetched to the cache in number of bytes at
\textit{<base>-4}. \texttt{br} is assumed to be a cache hit.

Immediate call and branch instructions interpret the operand as
\emph{unsigned} for \texttt{call} and \texttt{brcf}, and as
\emph{signed} for PC-relative branches (\texttt{br}). These immediate
values are interpreted in \emph{word size}. The target address of
PC-relative branches is computed relative to the address of the branch
instruction. The immediate value for \texttt{trap} is an index into an
exception vector (see Section~\ref{sec:exc}).

Indirect call and branch instructions interpret the operand as
\emph{unsigned} absolute addresses in \emph{byte size}. Indirect
\texttt{brcf} takes two operands: a base address and an offset. The
base address is the address of the code block to be fetched; the
effective branch target is \textit{<base>+<offset>}.

The return information provided by \texttt{call} in
\texttt{srb}/\texttt{sro} should only be passed to
\texttt{ret}. Likewise, the exception return information in
\texttt{sxb}/\texttt{sxo} should only be passed to \texttt{xret}. The
unit and addressing mode of these values is implementation
dependent.\wolf{Should we keep it implementation dependent, or should
  we enforce way it is in the current implementation?}

All control-flow instructions (except \texttt{trap}) have a delayed
and a non-delayed variant. For the delayed variant, $N$
bundles following the control-flow instruction in the code are always
executed. For the non-delayed variants, the control-flow change
appears to happen immediately. However, non-delayed control-flow
instructions may require more than one cycle to be executed. The
precise timing behavior is specified along with the detailed
description of the respective instructions.

The mnemonic of an instruction's non-delayed instruction variant is
suffixed with \texttt{nd}. For clarity, this document uses the
unsuffixed name to mean both variants when describing the general
properties of the respective instruction.

\texttt{br} instructions are executed in the \texttt{EX} stage, while
the other control-flow instructions are executed in the \texttt{MEM}
stage. This corresponds to a branch delay of 2 bundles for \texttt{br}
and 3 bundles for other control-flow instructions. In case there no
other instructions available, NOP instructions can be used to fill the
delay slots. There are no restrictions with regard to the size or type
of instructions in the delay slot. The only exception is that
executing control-flow instructions in a delay slot may lead to
unspecified behavior.

\begin{table}[hb]
  \centering
  \begin{tabular}{llllll}
    \toprule
    Instruction & Immediate   & Indirect & Cache fill & Link & Delay Slots \\
    \midrule
    call        & absolute, words    & absolute, bytes        & yes        & yes  & 3 \\
    br          & PC relative, words & absolute, bytes        & no         & no   & 2 \\
    brcf        & absolute, words    & absolute+offset, bytes & yes        & no   & 3 \\
    trap        & exception vector index & ---                & yes        & no   & -- \\
    \cmidrule{1-6}
    ret         & ---         & implementation dependent      & yes        & no   & 3 \\
    xret        & ---         & implementation dependent      & yes        & no   & 3 \\
    \bottomrule
  \end{tabular}  
  \caption{Addressing modes of control-flow instructions}
  \label{tab:cfladdr}
\end{table}

\clearpage

\begin{itemize}
  \item CFLi -- Control Flow with Immediate \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{2}{10} & \bitbox{2}{Op} & \bitbox{1}{d} &
      \bitbox{22}{Immediate} \\
    \end{bytefield}
\end{itemize}

\begin{table}[hb]
  \centering
  \begin{tabular}{llll}
    \toprule
    Op & d & Name & Semantics \\
    \midrule
    00 & 0 & callnd & function call (absolute, with cache fill, non-delayed) \\
    00 & 1 & call   & delayed function call (absolute, with cache fill, delayed) \\
    01 & 0 & brnd   & local branch (PC-relative, always hit, non-delayed) \\
    01 & 1 & br     & local branch (PC-relative, always hit, delayed) \\
    10 & 0 & brcfnd & branch (absolute, with cache fill, non-delayed) \\
    10 & 1 & brcf   & branch (absolute, with cache fill, delayed) \\
    11 & 0 & trap   & system call (via exception vector, with cache fill, non-delayed) \\
    \bottomrule
  \end{tabular}
  \caption{Control-flow operations with immediate}
  \label{tab:cfliops}
\end{table}

\begin{itemize}
  \item CFLri -- Control Flow with implicit registers \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{4}{1100} & \bitbox{1}{d} &
      \bitbox{18}{\bitsunused} &
      \bitbox{2}{00} & \bitbox{2}{Op} \\
    \end{bytefield}
\end{itemize}

\begin{table}[hb]
  \centering
  \begin{tabular}{llll}
    \toprule
    Op & d & Name & Semantics \\
    \midrule
    00 & 0 & retnd  & return (with cache fill, non-delayed) \\
    00 & 1 & ret    & return (with cache fill, delayed) \\
    01 & 0 & xretnd & return from exception (with cache fill, non-delayed) \\
    01 & 1 & xret   & return from exception (with cache fill, delayed) \\
    \bottomrule
  \end{tabular}
  \caption{Control-flow operations with implicit register operands}
  \label{tab:cflriops}
\end{table}

\begin{itemize}
  \item CFLrs -- Control Flow with single registers \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{4}{1100} & \bitbox{1}{d} &
      \bitbox{5}{\bitsunused} & \bitbox{5}{Rs} & \bitbox{8}{\bitsunused} &
      \bitbox{2}{01} & \bitbox{2}{Op} \\
    \end{bytefield}
\end{itemize}

\begin{table}[hb]
  \centering
  \begin{tabular}{llll}
    \toprule
    Op & d & Name & Semantics \\
    \midrule
    00 & 0 & callnd & function call (indirect, with cache fill, non-delayed) \\
    00 & 1 & call   & function call (indirect, with cache fill, delayed) \\
    01 & 0 & brnd   & local branch (indirect, always hit, non-delayed) \\
    01 & 1 & br     & local branch (indirect, always hit, delayed) \\
    \bottomrule
  \end{tabular}
  \caption{Control-flow operations with single register operand}
  \label{tab:cflrsops}
\end{table}

\begin{itemize}
  \item CFLrt -- Control Flow with two registers \\[2ex]
    \begin{bytefield}{32}
      \bitheader{0-31} \\
      \bitbox{1}{x} & \bitbox{4}{Pred} & \bitbox{4}{1100} & \bitbox{1}{d} &
      \bitbox{5}{\bitsunused} & \bitbox{5}{Rs1} & \bitbox{5}{Rs2} & \bitbox{3}{\bitsunused} &
      \bitbox{2}{10} & \bitbox{2}{Op} \\
    \end{bytefield}
\end{itemize}

\begin{table}[ht]
  \centering
  \begin{tabular}{llll}
    \toprule
    Op & d & Name & Semantics \\
    \midrule
    10 & 0 & brcfnd & branch (indirect with offset, with cache fill, non-delayed) \\
    10 & 1 & brcf   & branch (indirect with offset, with cache fill, delayed) \\
    \bottomrule
  \end{tabular}
  \caption{Control-flow operations with two register operands}
  \label{tab:cflrtops}
\end{table}

\paragraph{Behavior -- call}

Perform a function call, filling the method cache if needed.

Listing~\ref{lst:call} shows the pseudo-code for a call. The parameter
\texttt{addr} is either an immediate value (for calls in the CFLi
format), or comes from a general-purpose register (for indirect
calls). The variables \texttt{\$srb} and \texttt{\$sro} denote the
special registers \texttt{srb} and \texttt{sro}, respectively.

First, it stores the return information, and remembers
the new base address in an internal variable. Then, it retrieves the
offset into the cache for the function to be called and if necessary
copies the instructions into the cache. Finally, it updates the
internal program counter and continues execution from there.

\begin{lstlisting}[language=C,float=h,caption={Call\label{lst:call}}]
call(addr) {
  // Store return information
  $srb = base;
  $sro = PC;
  // Remember base address
  base = addr;
  // Cache look-up and load
  coff = offset(addr);
  if (!hit(addr)) memcpy(cache[coff], mem[base], mem[base-4]);
  // Update PC
  PC = coff;
}
\end{lstlisting}

The timing of a \texttt{callnd} instruction that is executed is
equivalent to the timing of an istruction sequence \texttt{call; nop;
  nop; nop}. The timing of a \texttt{callnd} instruction with a
predicate that evaluates to \texttt{false} is equivalent to a single
\texttt{nop} instruction.

\paragraph{Behavior -- brcf}

Perform a branch, filling the method cache if needed.

The pseudo-code for a PC-relative \texttt{brcf} is shown in
Listing~\ref{lst:brcf}. It is similar to the call, but does not store
any return information. Additionally, an offset \texttt{off} may be
specified for a \texttt{brcf} in the CFLrt format; this offset is 0
for \texttt{brcf} in the CFLi format.

\begin{lstlisting}[language=C,float=h,caption={Branch with cache fill\label{lst:brcf}}]
call(addr, off) {
  // Remember base address
  base = addr;
  // Cache look-up and load
  coff = offset(addr);
  if (!hit(addr)) memcpy(cache[coff], mem[base], mem[base-4]);
  // Update PC
  PC = coff+off;
}
\end{lstlisting}

The timing of a \texttt{brcfnd} instruction that is executed is
equivalent to the timing of an istruction sequence \texttt{brcf; nop;
  nop; nop}. The timing of a \texttt{brcfnd} instruction with a
predicate that evaluates to \texttt{false} is equivalent to a single
\texttt{nop} instruction.

\paragraph{Behavior -- trap}

Perform a system call. Details are described in Section~\ref{sec:exc}.
Like the \texttt{call}, \texttt{trap} stores return information, but
it uses special registers \texttt{sxb} and \texttt{sxo} for that
purpose.

\paragraph{Behavior -- ret, xret}

Return from function call. The \texttt{ret} instruction uses the
return information in the special registers \texttt{srb}/\texttt{sro}
to compute its target address. The \texttt{xret} instruction uses the
registers \texttt{sxb}/\texttt{sxo}.

Listing~\ref{lst:return} shows the pseudo-code for \texttt{ret}. It
first retrieves the return base and does the appropriate cache
handling. It then adds the cache offset and the return offset and
assigns the sum to the internal program counter, from which execution
continues.

\begin{lstlisting}[language=C,float=h,caption={Return\label{lst:return}}]
ret() {
  // Retrieve return base
  base = $srb;
  // Cache look-up and load
  coff = offset($srb);
  if (!hit($srb)) memcpy(cache[coff], mem[base], mem[base-4]);
  // Update PC
  PC = coff+$sro;
}
\end{lstlisting}

The timing of a \texttt{retnd}/\texttt{xretnd} instruction that is
executed is equivalent to the timing of an istruction sequence
\texttt{ret; nop; nop; nop}. The timing of a
\texttt{retnd}/\texttt{xretnd} instruction with a predicate that
evaluates to \texttt{false} is equivalent to a single \texttt{nop}
instruction.

\paragraph{Behavior -- br}

Local branch, compute new program counter value and update program
counter.

The timing of a \texttt{brnd} instruction may be implementation
defined. Implementations are allowed to use branch prediction for
\texttt{brnd} instructions, if the underlying mechanism is documented
in detail. Unless specified and documented otherwise, the timing of
\texttt{brnd} must be according to ``predict-not-taken''. The timing
of a \texttt{brnd} instruction that is executed is then equivalent to
the timing of the sequence \texttt{br; nop; nop}, while the timing
with a \texttt{false} predicate is equivalent to a single
\texttt{nop}.

\paragraph{Note}

All control-flow instructions can only be issued on the first position
within a bundle.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\clearpage
\section{Exceptions: Interrupts, Faults and Traps}
\label{sec:exc}

In the following, we use \emph{exception} to denote any kind of
``abnormal'' transfer of control. \emph{Interrupts} are generated
outside of the pipeline by I/O devices. \emph{Faults} are triggered by
the pipeline for instructions that cannot be executed as expected
(accesses to unmapped memory, undecodable instructions,
etc.). \emph{Traps} are willfully generated exceptions, and are used
to invoke operating system functions.

An \emph{exception unit} that is mapped to the I/O space (see
Chapter~\ref{chap:memsyst}) is responsible for managing exceptions. It
includes the device registers shown in Table~\ref{tab:excioregs}. The
general principle of operation is that the exception unit requests the
execution of an exception from the pipeline, and the pipeline returns
an acknowledges when it starts the execution of the respective
exception handler.

The \texttt{status} register is 32 bits wide; bit 0 of that register
determines whether interrupts are enabled. They are enabled if it is
1, and they are disabled when it is zero. The \texttt{status} register
is shifted left by one bit when an exception handler is triggered, and
shifted right by one bit when returning from an exception handler via
\texttt{xret}. Overflows of the status register (which might be caused
by nested exception handlers) are silently ignored.

Internally generated exceptions, i.e., faults and traps, always take
precedence over interrupts. If more than one internal exception or
interrupt is pending at the same time, the exception with the lower
number takes precedence.

\subsection{Exception Vector}

The exception unit supports 32 exception vector entries, shown in the
lower half of Table~\ref{tab:excioregs}. Exceptions 0
and 1 are reserved for the ``illegal operation'' and ``illegal memory
access'' faults. While exceptions 2 to 15 can be used freely (e.g., by
the operating system), exceptions 16 to 32 are attached to interrupts.

\begin{table}[b]
  \centering
  \begin{tabular}{llp{.6\textwidth}}
    \toprule
    Address             & Name             & Description \\
    \midrule
    \texttt{0xf0000100} & \texttt{status} & Interrupt-enable flag \\
    \texttt{0xf0000104} & \texttt{mask} & Mask of enabled interrupts \\
    \texttt{0xf0000108} & \texttt{pend} & Pending flags for interrupts \\
    \texttt{0xf000010c} & \texttt{source} & Number of exception that
    is about to be served \\
    \texttt{0xf0000110} & \texttt{sleep} & Sleep mode (optional) \\
    \cmidrule{1-3}
    \texttt{0xf0000180} & \texttt{vec<0>} & Address of exception handler 0, illegal operation \\
    \texttt{0xf0000184} & \texttt{vec<1>} & Address of exception handler 1, illegal memory access \\
    \ldots & \ldots & \ldots \\
    \cmidrule{1-3}
    \texttt{0xf00001c0} & \texttt{vec<16>} & Address of exception handler 16, interrupt 0 \\
    \ldots & \ldots & \ldots \\
    \texttt{0xf00001fc} & \texttt{vec<31>} & Address of exception handler 31, interrupt 15 \\
    \bottomrule
  \end{tabular}
  \caption{Exception unit device registers}
  \label{tab:excioregs}
\end{table}

\subsection{Traps}

The instruction \texttt{trap <n>} triggers exception number $n$. In
order to assimilate the handling of faults and traps in the pipeline,
\texttt{trap} instructions never have a delay slot. Traps can in
principle be called for any exception, e.g., to trigger an interrupt
handler from software.

\subsection{Return Information}

The return information for exceptions must be stored in registers that
are never used otherwise. Two special registers \texttt{sxb}
(\texttt{s9}) and \texttt{sxo} (\texttt{s10}) provide the return base
and return offset for exceptions. The instruction \texttt{xret}
implicitly uses these registers, as opposed to the \texttt{ret}
instruction, which uses special registers \texttt{srb} and
\texttt{sro}.

\subsection{Resuming Execution}

The return information for interrupts is set such that \texttt{xret}
returns to the bundle that was replaced by the interrupt
instruction in the pipeline.

For faults, the return information points to the bundle that triggered
the fault. After a fault, resuming execution must either have fixed
the cause of the fault and reexecute the whole bundle again, or
emulate the effects of the whole bundle (including the triggering of
further faults) and continue execution after the bundle. Due to the
complexity of the second option, we consider faults where the
respective instruction cannot be reexecuted \emph{fatal}, and advise
developers to terminate execution instead of trying to resume.

Resuming after a trap in principle has to take into account the same
considerations as faults. However, the content of the bundle is under
the control of the compiler. We require that a trap instruction is the
only instruction in a bundle. The return address for traps points to
the bundle after the one that contains the trap instruction.

\subsection{Delayed Triggering of Interrupts}

Instructions that stall the pipeline (loads, stores, calls, etc.)
delay the triggering of interrupts until the pipeline resumes
execution. Therefore, method cache fills or stack spills cannot be
interrupted. Control-flow instructions delay the triggering of
interrupts such that interrupts are never triggered inside a delay
slot or while executing instructions speculatively. Multiplications
delay the triggering of interrupts such that no multiplications are
``in flight'' when an interrupt handler is entered. Outstanding
delayed loads do not delay the triggering of interrupts. Interrupt
handlers must ensure that the contents of the special register
\texttt{sm} are saved and restored correctly.

\subsection{Sleep Mode}

The exception unit provides support for putting the processor to
sleep. Writes to the \texttt{sleep} register halt the pipeline until
an interrupt (or other exception) occurs. After executing the
respective exception handler, execution resumes after that
write. Therefore, writes to the \texttt{sleep} register should be
enclosed in a loop for continuous sleeping.

Support for sleeping is optional. On implementations that do not
support sleeping, writes to the \texttt{sleep} register are ignored
and do not have any effect. Therefore, continuing execution after a
write to the \texttt{sleep} register is not proof that an interrupt
has occurred.

\subsection{Examples}

The API for exception and interrupt handling for Patmos is provided by
the include file \texttt{machine/exceptions.h}.

Listing~\ref{lst:excreg} shows how to register exception handlers for
specific exceptions. First, \texttt{fault\_handler} is registered as
handler for all exceptions. Then, \texttt{trap\_handler} is registered
for exception number eight, and \texttt{intr\_handler} for exceptions
16 to 19, i.e., for interrupts 0 to 3.

\begin{lstlisting}[float, caption={Exception handler registration\label{lst:excreg}}]
  for (unsigned i = 0; i < 32; i++) {
	exc_register(i, &fault_handler);
  }
  exc_register(8, &trap_handler);
  exc_register(16, &intr_handler);
  exc_register(17, &intr_handler);
  exc_register(18, &intr_handler);
  exc_register(19, &intr_handler);
\end{lstlisting}

\begin{lstlisting}[float, caption={Interrupt enabling\label{lst:intrena}}]
  // unmask interrupts
  intr_unmask_all();
  // clear pending flags
  intr_clear_all_pending();
  // enable interrupts
  intr_enable();
\end{lstlisting}

Listing~\ref{lst:intrena} shows how to enable interrupts at the start
of an application. First, all interrupts are unmasked. Then, all
pending flags are cleared to avoid triggering any ``stale''
interrupts. Finally, interrupts are enabled; after that point, Patmos
will call the respective interrupt handler when an interrupt occurs.

To unmask only certain interrupts, \texttt{machine/exceptions.h}
provides a function \texttt{intr\_unmask}, which takes an interrupt
number as parameter. Similarly, \texttt{intr\_clear\_pending} can be
used to clear only a particular pending flag. Masking interrupts can
be done through the \texttt{intr\_mask\_all} and \texttt{intr\_mask}
functions. Disabling interrupt handling in general is done with the
\texttt{intr\_disable} function.

Listing~\ref{lst:faulthandler} shows a basic fault handler. As Patmos
cannot recover from faults, the handler does not use a special
prologue, and calls \texttt{abort} at the end instead of returning. In
the function body, the handler displays the exception source on the
leds and prints a message corresponding to the type of fault that
occurred.

\begin{lstlisting}[float, caption={Fault handler example\label{lst:faulthandler}}]
void fault_handler(void) {
  unsigned source = exc_get_source();
  LEDS = source;

  const char *msg = "FAULT";
  switch(source) {
  case 0: msg = "Illegal operation"; break;
  case 1: msg = "Illegal memory access"; break;
  }
  puts(msg);

  // cannot recover from a fault
  abort();
}
\end{lstlisting}

Listing~\ref{lst:intrhandler} shows a minimal interrupt handler. It
uses the macros \texttt{exc\_prologue} and \texttt{exc\_epilogue} to
save and restore the processor state. The actual functionality is that
the state of the LEDs is incremented according to the exception source.

\begin{lstlisting}[float, caption={Interrupt handler example\label{lst:intrhandler}}]
void intr_handler(void) {
  exc_prologue();

  LEDS += exc_get_source() & 0xf;

  exc_epilogue();
}
\end{lstlisting}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\clearpage
\section{Dual Issue Instructions}

Not all instructions can be executed in both pipelines. In general, the first
pipeline implements all instructions, the second pipeline only a subset.
All memory operations are only executed in the first pipeline.

What other instructions can be executed in both pipelines is still open for
discussion and evaluation with benchmarks. A minimal approach, as first
step for the hardware implementation, is to have only ALU instructions
available in the second pipelines (excluding predicate manipulation instructions).

\stefan{From what I see in the code, it might also help a lot to allow SWS in the
second pipeline, as they are quite common. Predicate instructions are not that common
now but this will change with the new single-path passes.

This section should also talk about hazards, i.e., can we predicate the second
slot with something that we write in the first slot, what if we use a GP register
in both slots and write to it, ... }

\martin{TODO: agree that we/I should write more on this. Currently
Wolfgang has implemented ALU and predicate operations in both
pipelines. More details shall be described.}

\stefan{Is MFS/MTS allowed in both pipelines (and at the same time)? Should be helpful for prologue/epiloge code,
especially when we move return infos back to special registers.}


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Assembly Format}

\martin{This is not pasim, right?}

A VLIW instruction consists of one or two operations that are issued in the first or both pipelines.
Each operation is predicated, the predicate register is specified before the operation in parentheses \texttt{()}.
If the predicate register is prefixed by a \texttt{!}, its negation is considered.
If omitted, it defaults to \texttt{(p0)}, i.e.\ always true.

A semi-colon \texttt{;} or a newline denotes the end of an instruction or operation. If an instruction contains two operations, the operations
in the bundle must be enclosed by curly brackets. Bundles do not need to be separated by newlines or semi-colons. For bundles consisting of
only one operation, the curly brackets are optional. Labels that are prefixed by \texttt{.L} are local labels.

All register names must be prefixed by \texttt{\$}.
We use destination before source in the instructions, between destination and source a \texttt{=} character must be used instead of a comma.
Immediate values are not prefixed for decimal notation, the usual 0 and 0x formats are accepted for octal and hexadecimal immediates.
Comments start with the hash symbol \texttt{\#} and are considered to the end of the line. For memory operations, the syntax is
\texttt{[\$register + offset]}. Register or offset can be omitted, in that case the zero register \texttt{r0} or an offset of $0$ is used.

Example:
\begin{verbatim}
       # add 42 to contents of r2
       # and store result in r1 (first slot)
       { add   $r1 = $r2, $42
       # if r3 equals 50, set p1 to true
       cmpeq $p1, $r3, 50 }
       # if p1 is true, jump to label_1
 ($p1) br label_1 ; nop; nop # then wait 2 cycles
       # Load the address of a symbol into r2
       li $r2 = .L.str2
       # perform a memory store and a pred op
       { swc [$r31 + 2] = $r3 ; or $p1 = !$p2, $p3 }
       ...
label_1:
       ...
\end{verbatim}

\stefan{TODO: some words about units of .align, .size, ..; describe .fstart;
I would like to move the assembly format description out into a public repo (patmos-misc) and merge
it with a compiler usage manual, ELF file format and backend description, though.}

\subsection{Instruction Mnemonics}

The LLVM assembler supports the instructions mnemonics as specified in this document, including all pseudo instructions.

The \texttt{paasm} assembler and the \texttt{pasim} simulator use the same basic instruction mnemonic, but a \texttt{i} or
\texttt{l} suffix is appended for \emph{immediate} and \emph{long immediate} variants, while no suffix in general refers to
the register indirect variant of the instructions. As exception, the control flow instructions use a \texttt{r} suffix for the register
indirect variants and no suffix for the immediate instructions.
\stefan{This should be cleaned up, always use \texttt{i} suffix for immediates in pasim/paasm, including control flow.}

\subsection{Inline Assembly}

Inline assembly syntax is similar to GCC inline assembly. It uses \texttt{\%0}, \texttt{\%1}, \ldots as placeholders
for operands. Accepted register constraints are: \texttt{r} or \texttt{R} for any general purpose register, or
\texttt{\{<registername>\}} to use a specific register.

Example:
\begin{lstlisting}
    int i, j, k;
    asm("mov  $r31 = %1  # copy i into r31\n\t"
        "add  %0 = $r5, %2"
        : "=r" (j)
        : "r" (i), "{r10}" (k));
\end{lstlisting}

\chapter{Memory and I/O Subsystem}
\label{chap:memsyst}

\section{Local and Global Address Space}

The typed loads of Patmos imply two address spaces: a local address
space that is accessed through local loads and stores, and a global
address space that is accessed when using other access types. All
caches use memory that is mapped to the global address space as
backing memory. For example, the data cache fetches data from global
memory on a cache miss, and the stack cache uses global memory for
spilling and filling. Consequently, there are two memory maps, one for
the local address space and one for the global address
space. Tables~\ref{tab:lmmap} and~\ref{tab:gmmap} show the respective
address mappings. To simplify address decoding, the top four bits
(A31--A28) are generally used to distinguish between different memory
and I/O areas. The address range for I/O devices is divided further to
distinguish the different devices, as discussed in
Section~\ref{sec:iodevs}.

As \code{call}, \code{ret}, and \code{brcf} do not include memory type
information, the distinction between memory areas for these
instructions is done solely through the address mapping. The boot instruction ROM
and the instruction scratchpad memory are mapped to the lowest 128K of
the global address space. Note that this applies only to these
instructions; i.e., a \code{call} to address \code{0x00010100}
executes code that is located in the instruction scratchpad, while
non-local loads or stores to the same address access the external
SRAM. Therefore, a binary that is loaded to external memory can use
the lowest 128K of memory for data segments, but not for code
segments.

The global address space also includes a ROM and a scratchpad for
booting. The boot data ROM enables boot programs to use initialized
data segments. By copying (parts of) the boot data ROM to the boot
scratchpad, these programs can also use initialized data segments that
require write access. Note that these memory areas can be accessed by
loads and stores only; it is not possible to execute code located in
them.

\begin{table}
\centering
\begin{tabular}{ll}
\toprule
Address & Memory area \\
\midrule
\code{0x00000000}--\code{0x0000ffff} & Data Scratchpad Memory \\
\code{0x00010000}--\code{0x0001ffff} & Instruction Scratchpad Memory (write only) \\
\code{0xe0000000}--\code{0xe7ffffff} & NoC interface configuration registers \\
\code{0xe8000000}--\code{0xefffffff} & NoC communication memory \\
\code{0xf0000000}--\code{0xffffffff} & I/O devices \\
\bottomrule
\end{tabular}
\caption{Address mapping for local address space}
\label{tab:lmmap}
\end{table}

\begin{table}
\centering
\begin{tabular}{ll}
\toprule
Address & Memory area \\
\midrule
\code{0x00000000}--\code{0x0000ffff} & Boot Instruction ROM (only for code) \\
\code{0x00010000}--\code{0x0001ffff} & Instruction Scratchpad Memory (only for code) \\
\code{0x00000000}--\code{0x7fffffff} & External SRAM \\
\code{0x80000000}--\code{0x8000ffff} & Boot Data ROM (only for data) \\
\code{0x80010000}--\code{0x8001ffff} & Boot Data Scratchpad Memory (only for data) \\
\bottomrule
\end{tabular}
\caption{Address mapping for global address space}
\label{tab:gmmap}
\end{table}

\subsection{Boot Memories}

By convention, the first four words of the boot data ROM contain
information about data to be copied to the boot data scratchpad before
starting actual execution. Table~\ref{tab:bootrommap} shows the
respective data fields. Upon start, the program should copy
\code{src\_size} bytes of data from \code{src\_start} to
\code{dst\_start}. If \code{dst\_size} is greater than
\code{src\_size}, the remaining bytes are filled with zeroes.

\begin{table}
\centering
\begin{tabular}{lll}
\toprule
Address & Name & Description \\
\midrule
\code{0x80000000} & \code{src\_start} & Start address of data to be copied \\
\code{0x80000004} & \code{src\_size} & Size of data to be copied \\
\code{0x80000008} & \code{dst\_start} & Destination for copying \\
\code{0x8000000c} & \code{dst\_size} & Size of initialized data \\
\bottomrule
\end{tabular}
\caption{Boot data initialization information}
\label{tab:bootrommap}
\end{table}


\stefan{We discussed about making the stack cache memory mapped, like the SPM. The disadvantage is that
we need to reserve an additional register for the stack pointer of the stack cache. s5 (the stack cache spill pointer) must point into a main
memory address range. s6 (the stack top pointer) must point to the top of the stack cache in the cache address range and must be modifiable
(for context switching). An additional s7 must either contain the size of the stack cache or the tail pointer of the stack cache.

The advantage is we can then pass typed pointers to functions, similar to SPM pointers, if the caller stack frame is not evicted, in a first
step. As a second step, we can drop the typed loads and use the compiler to emit address range information to the WCET analysis.
A pointer into the stack cache address range is always guaranteed to be a hit (accessing an address that has been evicted is an error). We
can therefore get rid of function duplication depending on their pointer types. We basically move the information about the accessed memory from the
memory instruction to the pointer value, which can be context-sensitive.}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{I/O Devices}
\label{sec:iodevs}

Each processor contains a minimum set of standard I/O devices, such as:
processor ID, cycle counter, timer, and interrupt controller. For a minimum
communication with the outside world a processor shall be attached to a
serial port (UART). The UART represents \code{stdout}.

Within the I/O device memory area bits 11--8 are used to distinguish between different devices.
I/O device registers are mapped and aligned to 32-bit words. If a register is shorter than a word,
the upper bits shall be filled with 0 on a read. With this mapping each I/O device can have up to
64 32-bit registers.

In the initial prototype of Patmos we have 3 I/O devices: a system device that contains cycle and microsecond counters, a UART for basic communication, and LEDs
on the FPGA board. One counter ticks with the clock frequency and the second
counter ticks with 1~MHz for clock frequency independent time measurements.
The counters are 64-bit values and readout of the lower 32 bits also latches the
upper 32 bits.
Table~\ref{tab:iomap} shows the I/O devices and the registers.

\begin{table}
\centering
\begin{tabular}{llll}
\toprule
Address & I/O Device & read & write \\
\midrule
\code{0xf0000000} & cpuinfo & processor ID & -- \\
\code{0xf0000004} & cpuinfo & clock frequency (Hz) & -- \\
\code{0xf0000200} & timer & clock cycles (high word) & cycle interrupt time (high word) \\
\code{0xf0000204} & timer & clock cycles (low word) & cycle interrupt time (low word) \\
\code{0xf0000208} & timer & time in $\mu s$ (high word) & $\mu s$ interrupt time (high word) \\
\code{0xf000020c} & timer & time in $\mu s$ (low word) &  $\mu s$ interrupt time (low word) \\
\code{0xf0000800} & UART & status & control \\
\code{0xf0000804} & UART & receive buffer & transmit buffer \\
\code{0xf0000900} & LED & -- & output register \\
\code{0xf0000a00} & Keys & input register & -- \\
\bottomrule
\end{tabular}
\caption{I/O devices and registers}
\label{tab:iomap}
\end{table}

\subsection{Timer}

The timer device provides a means to measure time as well as to
trigger an interrupt at a certain point in time. It provides two
64-bit counters. While the first counter is incremented every clock
cycle, the second counter is incremented every microsecond.

Interrupts can be triggered by storing a value in the ``cycle
interrupt time'' and ``$\mu s$ interrupt time'' registers. The timer
device will then trigger an interrupt when the respective counter
reaches the value provided in that register. The ``cycle'' interrupt
is tied to interrupt~0; the ``$\mu s$'' interrupt is tied to
interrupt~1.

To read out the 64-bit counter values consistently, the low word (at
the higher address) must be read first. This latches the high word of
the counter into an internal register, which is then returned when
reading the high word (at the lower address). Similarly, the low word
of the interrupt times must be written first. The write to the
internal 64-bit register takes effect when the high word is written.

\subsection{UART}

The UART is a minimal IO device for \texttt{stdout} and \texttt{stdin}.
It is also used for program download. Table~\ref{tab:uart} shows the
bits of the control register.

\begin{table}
\centering
\begin{tabular}{lllll}
\toprule
Bit & Status & & Control & \\
\midrule
0 & TRE & TX Transmit ready & -- & -- \\
1 & DAV & RX Data available & -- & -- \\
% 2 & PAE & RX Parity error (or EOF) & -- & -- \\
% 3 & --  & -- & TFL & TX Flush \\
\bottomrule
\end{tabular}
\caption{UART status bits} % {-- What does flush mean on a UART?}}
\label{tab:uart}
\end{table}

The UART address for \code{pasim} is defined in \code{patmos/simulator/include/uart.h}.
For the compiler/library the constant is in \code{llvm/tools/clang/lib/Driver/Tools.cpp}.

\paragraph{Proposal for CMP UART Sharing}
On a multicore system only one processor can be directly connected to the
UART. However, for debugging it would be convenient to attach several (or all)
cores to the UART. Sharing the UART can be achieved by an arbitration device
that has $n$ input ports and a simple protocol that precedes UART data by a
marker from which core the data is coming. The marker byte may be precede
each data byte or it may be distinguished by setting Bit 8. This mechanism can
also be used to represent several UARTs per core (e.g., stdout, stderr, user for SLIP,...).
\martin{The protocol shall be done in HW -- let's find a student for this.}

On the PC side a small program is needed to dispatch/demultiplex the different
data streams.

\stefan{Would be nice to be able to insert control bytes to flush the output as well, but that must be understood by the simulator as well (it
must flush its output stream).

With multiple cores writing to the UART, we need to prevent mixing up control bytes and data bytes from different cores, so I assume adding
the control bytes to is done by hardware? We could also designate one core to I/O and communicate over SPMs with the other cores, although it
adds another layer of potential bugs to the debugging process.}

\martin{Yes, the idea is that the UARTs look simple for all cores and the HW does the arbitration and additional control byte. However, no one is assigned this task :-(}

\martin{Other I/O (and using a dedicated core) is application dependent and not the topic of this report.}





\section{The Stack Cache}
\label{sec:stack-cache}

The stack cache is a processor-local, on-chip memory~\cite{patmos:stack:seus}. The stack
cache operates similar to a ring buffer. It can be seen as a stack-cache-sized
window into the main memory address range.
To manage the stack cache, we use three additional
instructions: \code{reserve}, \code{ensure},
and \code{free}. Two hardware registers define which
part of the stack area is currently in the stack cache.

\subsection{Stack Cache Manipulation}

We present the mechanics of the stack cache in C code for easier readability.
However, the hardware implementation is a synchronous design and
the algorithm is implemented by a state machine that handles
the memory spill and fill operations. In the C code following data structures are used:

\begin{description}
\item[\code{mem}] is an array representing the main memory,
\item[\code{sc}] is an array representing the stack cache,
\item[\code{m\_top}] is the register pointing to the top of the saved stack content in the main memory, and
\item[\code{sc\_top}] points to the top element in the stack cache.
\end{description}

The two pointers are full-length address registers. However,
when addressing the stack cache, only the lower $n$ bits
are used for a stack cache of a size of $2^n$ words.
The constant \code{SC\_SIZE} represents the stack
cache size and \code{SC\_MASK} is the bit mask for
the stack cache addressing. The stack cache is managed in 32-bit
words. % Therefore, the pointers count in 32-bit words.

At program start the stack cache is empty and both pointers,
\code{m\_top} and \code{sc\_top}, point to the same address,
the address that one higher as the stack area. \code{m\_top}
points to the last spilled word in main memory.
Similar, \code{sc\_top} points to the last slot in the stack
frame (top of stack). Therefore, the number of currently valid
elements in the stack cache is \code{m\_top} - \code{sc\_top}.

The compiler generates code to grow the stack downward,
as it is common for many architectures. Growing the stack downwards
has historical reasons. However, for multi-threaded
systems each thread needs a reserved, fixed memory area
for the stack and there is no benefit from growing the stack
downwards.

%We have some constants, two pointers, and main memory and the stack memory:
%\begin{figure} [!h]
%
%\begin{lstlisting}
%#define SC_SIZE 64
%#define SC_MASK (SC_SIZE-1)
%
%// The main memory
%static int mem[1024];
%// The stack cache
%static int sc[SC_SIZE];
%
%// Pointer the top memory saved stack content
%int m_top;
%
%// Pointer to the top element in the stack cache
%int sc_top;
%
%// #elements is m_top - sc_top and always has to 	be <= SC_SIZE
%\end{lstlisting}
%
% 	 \caption{Stack cache and main memory pointer definitions.}
% 	 \label{fig:sc_mem_def}
%\end{figure}

\begin{figure}
\begin{lstlisting}
void reserve(int n) {

    int nspill, i;

    sc_top -= n;
    nspill = m_top - sc_top - SC_SIZE;
    for (i=0; i<nspill; ++i) {
       --m_top;
       mem[m_top] = sc[m_top & SC_MASK];
    }
}
	\end{lstlisting}
 	 \caption{The \code{reserve} instruction provides \code{n}
	 free words in the stack cache. It may spill data into main memory.}
 	 \label{fig:res_iml}
\end{figure}


\paragraph{Reserve} The \code{reserve} instruction, as shown in Figure~\ref{fig:res_iml},
reserves space in the stack cache. Typed load and store instructions use
this reserved space. The reserve instruction may spill data to the
main memory. This spilling happens when there are not enough free words in the
stack cache to reserve the requested space.

The processor reads the number of words to be reserved
(the immediate operand of the instruction) in the decode stage.
The processor adjusts the \code{sc\_top} register in the execution
stage and also computes how many words need to be spilled in the
execution stage. The processor spills to the main memory
in the memory stage, as shown by the for loop in~Figure~\ref{fig:res_iml}.

\begin{figure}
\begin{lstlisting}
void free(int n) {

    sc_top += n;
    if (sc_top > m_top) {
        m_top = sc_top;
    }
}
\end{lstlisting}
	\caption{The \code{free} instruction drops \code{n}
	elements from the stack cache. It may change the top memory
	pointer \code{m\_top}.}
 	 \label{fig:free_iml}
\end{figure}

\paragraph{Free} The \code{free} instruction frees the reserved
space on the stack. It does not fill previously spilled data back into the stack cache.
It just changes the top of the stack pointer and may change the top of the memory
pointer, as shown in Figure~\ref{fig:free_iml}.


\begin{figure}
\begin{lstlisting}
void ensure(int n) {

    int nfill, i;

    nfill = n - (m_top - sc_top);
    for (i=0; i<nfill; ++i) {
        sc[m_top & SC_MASK] = mem[m_top];
        ++m_top;
    }
}
\end{lstlisting}
	\caption{The \code{ensure} instruction ensures that
	at least \code{n} elements are valid in the stack cache.
	It may need to fill data from main memory.}
 	\label{fig:ens_iml}
\end{figure}

\paragraph{Ensure} Returning into a function needs to ensure that the stack
frame of this function is available in the stack cache. The \code{ensure} instruction,
as shown in Figure~\ref{fig:ens_iml}, guarantees this condition.
This instruction may need to fill back the stack cache with previously spilled data.
This happens when the number of valid words in the stack cache is less than the
number of words that need to be in the stack cache.
Filling the stack cache is shown in the loop in Figure~\ref{fig:ens_iml}.

One processor register serves as stack pointer and points to the end of the stack frame.
Load and store instructions use displacement addressing relative to this stack pointer
to access the stack cache.

\begin{figure}
\begin{lstlisting}
// load one word from the stack cache
// addr is a main memory address (register value plus offset)

int load(int addr) {
    return sc[(sc_top + addr) & SC_MASK];
}

// store one word into the stack cache
// addr is a plain main memory address

void store(int addr, int val) {
    sc[(st_top + addr) & SC_MASK] = val;
}
\end{lstlisting}
	\caption{Pseudo code for the load and store instructions.}
 	\label{fig:ld_st_iml}
\end{figure}

\martin{We might update this section with more content from the S\$ paper}

As with regular ring buffers, when the size of the stack cache is not sufficient
in order to reserve additional space requested, it needs to spill some data
so far kept in the stack cache to the global memory, i.e., whenever
\code{m\_top} - \code{sc\_top} $>$ \emph{stack cache size}. A major difference,
however, is that freeing space does \emph{not} imply the reloading of data from
the global memory. When a free operation frees all stack space currently held in
the cache (or more), the special register \texttt{ss} is accordingly
incremented.

The stack cache is organized in blocks of fixed size, e.g. $32$ bytes. All
spill and fill operations are performed on the block level, while reserve, free
and ensure operations are in words.
\martin{We agreed that some size values are needed in the compiler to
generate correct code (soon). So we might bring in stack cache manipulation
in burst blocks as well.}

Addresses for load and store operations from/to the stack cache are relative to
the \code{sc\_top} pointer.

The base address for fill and spill operations of the stack cache is kept in
special register \texttt{ss}. \texttt{st} contains the address the top of the
stack cache would get if the stack cache would be fully spilled to memory.

The organization of the stack cache implies some limitations:
\begin{itemize}
  \item The maximum size of stack data accessible at any moment is limited to
        the size of the cache. The stack frame can be split, such that at any
        moment only a subset of the entire stack frame has to be resident in the
        stack cache, or a \emph{shadow} stack frame in global memory can be
        allocated.
  \item When passing pointers to data on the stack cache to other functions it
        has to be ensured that: (1) the data will be available in the cache, (2)
        the pointer is only used  with load and store operations of the stack
        cache, and (3) the relative displacement due to reserve and free
        operations on the stack is known. Alternatively, aliased stack data can
        be kept on a \emph{shadow} stack in the global memory without
        restrictions.
  \item The stack control operations only allow allocating constant-sized junks.
        Computed array sizes (C 90) and \texttt{alloca} with a computed
        allocation size have to be realized using a \emph{shadow} stack in
        global memory.
  \item The calling conventions for functions having a large number of arguments
        have to be adapted to account for the limitation of the stack cache
        size (see Section~\ref{sec:abi}).
\end{itemize}



% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Method Cache}
\label{sec:method-cache}

An overview of alternative design options with regard to the method cache can be
found in Section~\ref{sec:method_cache_options}. It is uncertain which of those
options is best, however, two candidates appear very promising and should
be evaluated: fetch on call with FIFO replacement and fetch on call with LRU
replacement. Compiler managed prefetching can still be added at a later stage.

\subsection{Common Features}

The cache is organized in blocks of a fixed size, e.g., 32 bytes.

Contiguous sequences of code are cached. These code sequences will often
correspond to entire functions. However, functions can be split into smaller
junks in order to reduce to overhead of loading the entire function at once.
Code transfers between the respective junks of the original function can be
performed using the \texttt{brcf} instruction.

A code sequence is either kept entirely in the method cache, or is entirely
purged from the cache. It is not possible to keep code sequences partially in
the cache.

Code intended for caching should be aligned in the global memory according to
the memory burst size. Call and branch instructions do not encode the size of
the target code sequence. The size is thus encoded in units of bytes right in front of the first
instruction of a code sequence that is intended for caching.
Figure~\ref{fig:cacheable_code} illustrates this convention.

\begin{figure}
  \centering
  \begin{bytefield}{25}
    \wordbox[tlr]{1}{length} \bitbox[]{1}{} \bitbox[t]{9}{burst aligned} \\
    \wordbox[tlr]{1}{first instruction}  \\
    \wordbox[tlr]{1}{second instruction} \\
    \wordbox{1}{\dots} \\
  \end{bytefield}
  \caption{Layout of code sequences intended to be cached in the method cache.}
  \label{fig:cacheable_code}
\end{figure}

The organization of the method cache implies some limitations:
\begin{itemize}
  \item The size of a code sequence intended for caching is limited to the size
        of the method cache. Splitting the function is possible.
  \item Compiler managed prefetching, if supported, has to ensure that the
        currently executed code is not purged.
\end{itemize}

% ------------------------------------------------------------------------------
\subsection{FIFO replacement}

The method cache with FIFO replacement allocates a single junk of contiguous
space for a cached code sequence. Every block in the cache is associated with a
tag, that corresponds to the base addresses of cached code sequences. However,
the tag is only set for the first block of a code sequence. The tags of all
other blocks are cleared. This simplifies the purging of cache content when
other code is fetched into the cache.

Code is fetched into the cache according to a \texttt{fifo-base} pointer, which
points to the first block of the method cache where the code will be placed.
After the fetching the code from global memory has completed this pointer is
advanced to point to the block immediately following the least recently fetched
block.

The design of the method cache with FIFO replacement is based on the
design for the Java processor JOP~\cite{jop:jtres_cache} .

% ------------------------------------------------------------------------------
\subsection{LRU replacement}

The LRU cache configuration is more complex. Code is \emph{not} kept in
contiguous blocks and might be scattered according to the LRU time stamps of the
blocks in the cache. Every block is thus tagged with the address of the block
in global memory and an LRU time stamp. The address part of the tag is need to
rediscover the block during instruction fetch. The time stamp is required to
implement the LRU policy.

In addition, the length of every code sequence currently in the cache has to be
stored.

\paragraph*{Time Stamps}
On every access to the method cache, i.e., for every call or non-cache-relative
branch, the LRU time stamps of the blocks (possibly all) in the cache has to be
done. It is important to note that \emph{all} blocks of a code sequence share
the \emph{same} LRU time stamp at all times.

\fb{The time stamps could also be stored per code sequence in the cache. This
    might simplify the implementation of the LRU policy.}

\paragraph*{Instruction Fetch}
In order to fetch an instruction from the method cache the address of the
instruction is compared with the address tag of every block in the cache
(excluding some of the least significant bits depending on the cache's block
size). If a matching tag is found, i.e., a cache hit, the respective word in the
block is fetched.

If no block with a matched tag exists, a cache miss occurs and the target block
has to be transferred from the global memory into the cache.
\martin{We have cache misses on fetch in a LRU based method cache? I don't think so.}

\paragraph*{Purging Blocks}
When a code sequence is to be loaded into the cache, it has to be ensured that
enough space is available to hold all the blocks of that code sequence by
purging some blocks currently in use. Note, again, only entire code sequences
are purged from the cache, i.e., all its blocks at once. Code sequences are
repeatedly purged until enough space becomes available in the cache.

\paragraph*{Cache Fill}
Once enough space is available, the code sequence is transferred block-wise from
global memory to the cache, the tag and LRU time stamp are set accordingly for
every fetched block.

\martin{You might all know my position on an LRU based method cache, right?
It will be a plain mess to implement and maybe an issue on the HW
side (fully associative lookup at each fetch!). However, if one would
like to go this direction, it might be an interesting experiment.
One optimization point could be instead of the full associative lookup
on each fetch to have a translation table of block addresses to cache
blocks. This might add `just' another pipeline stage into the fetch
part.}

\martin{We could also think about doing the load/replacement
in SW - more like a managed ISPM.}

\subsection{Method Cache Options}
\label{sec:method_cache_options}

We have several options to implement the method cache and related instructions
(call, return). Note, all replacement policies operate on the method/function
level.
\begin{itemize}
  \item Fetch on call, with FIFO replacement \\
        On a call it is checked whether the target method is in the cache or
        not. If yes, execution continues. Otherwise, the call instruction
        starts fetching the method into the cache, the pipeline is stalled.
  \item Fetch on call, with FILO replacement \\
        Same as above, only that the replacement strategy if FILO, i.e., like a
        stack.
  \item prefetch before call, with FILO replacement \\
        We define two instructions to perform a function call. A compiler-placed
        prefetch instructions ensure that the target method is either in the
        cache, or triggers the loading of the function. Calls merely check
        whether the method is completely loaded and transfer control.
        (FIFO replacement is not possible due to potential eviction of the
        current method executing the prefetch).
  \item prefetch before call, explicit unload, with FILO replacement \\
        Same as before, with an additional \emph{unload} instruction to evict
        methods explicitly from the cache -- i.e., to free space after as shown
        by the following example:
\begin{verbatim}
  A()
  // unload here to avoid mutual eviction
  // in loop of B and C, assuming A+B, A+C,
  // B+C fit in the cache, but not A+B+C
  while(true) {
    B() //
    C();
  }
\end{verbatim}
  \item fetch on call, LRU replacement
  \item prefetch before call, LRU replacement \\
        This is everybody's darling: compiler-placed prefetching with stalling
        call instructions. Safe, clean, and expensive in hardware ;-)
\end{itemize}

%\stefan{There are quite a few knobs that we can turn with the method cache (btw, should we call it function cache since we are compiling C
%code?)
%
%\begin{itemize}
%\item Functions or subfunctions:
%
%\item Split allocation and preloading or always load complete (sub)function:
%
%\item Allocation in hardware (FIFO) or software (FIFO, LRU,..):
%
%\item Organize in blocks or as SPM, continous function entries or allow allocation of random blocks:
%
%\item Hit detection in hardware or software:
%
%\item Separate i-cache to persist 'macros', interrupt handlers, or commonly used functions:
%
%\end{itemize}
%
%I will elaborate on this, just commiting it as it is for now.
%}


\stefan{How do we initialize the core, i.e., how do we get the initial start-up code into the i-cache, especially when
we use software-controlled i-cache allocation? How do we handle interrupts (they need to get into the cache before they can be executed)?}

\jack{This is a good use for I-SPM. Use it as an operating system ``ROM''
or at least a first-stage bootloader.}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Instruction Cache}

This section will cover a configuration of Patmos with a standard instruction cache design, i.e., without a method cache.

We would like to see a Patmos implementation with a 2-way set-associative instruction cache governed under the least-recently used (LRU) replacement policy.
This allows for a "real" comparison between a FIFO method cache and a standard instruction cache on the same architecture.

\cullmann{During the last meeting, we proposed to have a normal LRU instruction cache, not a method cache with LRU replacement.
I thought the idea was to compare the performance of a method cache with a normal cache.
Therefore we don't really see a benefit in LRU method cache but would like to see an normal instruction cache instead to allow this comparison.
}

\fb{For a reasonable comparison we still need a reasonable baseline for the
method cache. Please add a section on the standard instruction cache design,
issues you would like to see addressed there, and potential amendments to the
ISA with respect to the current proposal with a method cache. Thanks!}

\martin{Agreed that comparison between FIFO M\$ and plain I\$ is one of
the project goals. However, before doing the HW support for both I would
like to see the comparison within the WCET analysis. I think there is an
empty paper start here in this SVN under 2012/mceval/mceval.tex ;-)}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\section{Data Cache}

This section will cover a configuration of Patmos with a standard data cache design, i.e., without a stack cache (and shadow stack).

We would like to see a Patmos implementation with a 2-way or 4-way set-associative data cache governed under the least-recently used (LRU) replacement policy.
The data cache should be write-through.

Again this allows for a "real" comparison between the stack cache and a standard data cache on the same architecture.

It might also be interesting to explore the object cache idea \cite{jop:ocache, jop:ocwcet:ccpe} where objects (heap allocated structures) are tracked via
a fully associative cache. Furthermore, for arrays with low temporal locality
a small set of prefetch buffers may benefit from spacial locality.

\section{Hardware Interface}

For the connection of Patmos to a memory controller, I/O devices, the
core-to-core network on chip, and/or the memory arbiter an interface
standard needs to be specified. Several standards are available.  We
decided to base the interface on
OCP\footnote{\url{http://www.ocpip.org/}}~\cite{ocp:spec} and subset
the standard as we need it.

\subsection{Description}

\begin{figure}
  \centering
  \includegraphics[scale=.8]{fig/ocppipe}
  \caption{Localization of OCP signals in the pipeline}
  \label{fig:ocppipe}
\end{figure}

Figure~\ref{fig:ocppipe} shows the OCP signals in the Patmos
pipeline. The master signals are generated in the execute stage, and
the slave signals are captured in the memory stage.\footnote{For
  clarity, the handling of both parts is implemented in the file
  \code{Memory.scala}.}

\begin{figure}
  \centering
  \includegraphics[scale=.8]{fig/ocplevels}
  \caption{OCP levels in Patmos}
  \label{fig:ocplevels}
\end{figure}

The different variants of the OCP protocol in the scope of the Patmos
processor are shown in Figure~\ref{fig:ocplevels}.

\subsubsection{\code{OCPcore}}

The variant of OCP generated by the pipeline for the local address
space. The respective signals are shown in
Table~\ref{tab:coresignals}. \code{OCPcore} is tailored to accesses to
on-chip memories, which on FPGAs necessarily include an internal
register. To enable sub-word transfers of data, the signal
\code{MByteEn} is used. The following assumptions apply:
\begin{itemize}
\item Only reads (\code{RD}) and writes (\code{WR}) are
  supported. Writes require a response (\code{writeresp\_enable=1}),
  such that every command must be followed by a response.
\item No \code{SCmdAccept} or \code{MRespAccept}, flow control is done
  solely via \code{SResp}.
\item Slaves may generate responses earliest in the cycle after a
  command.
\item The master may issue commands in the same cycle as the slave
  sends its response, i.e., basic support for pipelining is required.
\item \code{MByteEn} is assumed to be properly aligned
  (\code{force\_aligned=1}). The signal can be ignored for read
  accesses without side-effects.
\end{itemize}

\begin{table}
  \centering
  \caption{\code{OCPcore} signals}
  \label{tab:coresignals}
  \begin{tabular}{ll}
    \toprule
    Signal & Description \\
    \midrule
    \code{MCmd} & Command \\
    \code{MAddr} & Address, byte-based, lowest two bits always 0 \\
    \code{MData} & Data for writes, 32 bits \\
    \code{MByteEn} & Byte enables for sub-word accesses, 4 bits \\
    \cmidrule{1-2}
    \code{SResp} & Response \\
    \code{SData} & Data for reads, 32 bits \\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection{\code{OCPio}}

The \code{OCPio} level is derived from the \code{OCPcore} level by
inserting a register in the master signals. It is slightly more
flexible than \code{OCPcore} and appropriate for I/O devices that do
not (or cannot) follow the semantics of \code{OCPcore}. Registering
the master signals changes the protocol as follows:
\begin{itemize}
\item Slaves may generate responses in the same cycle as they
  receive a command.
\item Commands are issued earliest in the cycle after a response (no
  pipelining).
\item \code{SCmdAccept} is supported. It is sufficient to register the
  master signals only if the currently registered command is
  \code{IDLE} or \code{SCmdAccept} is high.
\item In order to have symmetric handshaking for commands and
  responses and to facilitate clock-domain crossing, \code{OCPio} also
  includes a signal \code{MRespAccept}. An \code{OCPio} port that is
  derived directly from the pipeline's \code{OCPcore} port always
  accepts responses.
\end{itemize}

\subsubsection{\code{OCPcache}}

This OCP variant is generated for the global address space and is used
for communication between the pipeline and the caches. It is the same
as \code{OCPcore}, but includes an additional signal \code{MAddrSpace}
to specify the cache that should serve the access.

\subsubsection{\code{OCPburst}}

The caches access the external memory through bursts only;
Table~\ref{tab:burstsignals} shows the signals of the \code{OCPburst}
interface. The tie-off value for \code{MBurstLength} is 4, and
\code{MBurstSingleReq} is tied off to 1. This means that the master
supplies four data words for each write command, and the slave returns
four words for each read command. All other burst-related signals are
tied off to their default values. This entails that the only sequence
for burst addresses is \code{INCR}. Bursts always stat at an address
that is aligned to the burst size
(\code{burst\_aligned=1}). Furthermore, \code{reqdata\_together} is
set to 1, i.e., write commands and the respective first word are
issued together. Instead of the signal \code{MByteEn}, \code{OCPburst}
uses the signal \code{MDataByteEn}. This implies that partial write
transfers are fully supported, but partial read commands are
unsupported.

We assume that the master provides data for burst accesses in
consecutive cycles and that slaves can accept all burst data words
once they accept the first word. To enable handshaking for the
acceptance of the first data word, the \code{OCPburst} variant
includes the signals \code{MDataValid} and \code{SDataAccept}. As
\code{reqdata\_together} is set to 1, delaying the acceptance of data
also delays the acceptance of write commands. In order to do the same
for read commands, \code{OCPburst} also includes the signal
\code{SCmdAccept}. The signals \code{SCmdAccept} and
\code{SDataAccept} can be generated by the same logic. For the
acceptance of write commands they must be identical, otherwise at
least one of the signals can have an undefined value. We assume that
slaves return burst read data in consecutive cycles

The first response to a read command may be given in the cycle after
the command. The response to a write command may be given earliest in
the cycle after the last data word was sent. Commands may be issued
earliest in the cycle after the last response from an earlier command
is received.

\begin{table}
  \centering
  \caption{\code{OCPburst} signals}
  \label{tab:burstsignals}
  \begin{tabular}{ll}
    \toprule
    Signal & Description \\
    \midrule
    \code{MCmd} & Command \\
    \code{MAddr} & Address, byte-based, lowest two bits always 0 \\
    \code{MData} & Data for writes, 32 bits \\
    \code{MDataByteEn} & Byte enables for sub-word writes, 4 bits \\
    \code{MDataValid} & Signal that data is valid, 1 bit \\
    \cmidrule{1-2}
    \code{SResp} & Response \\
    \code{SData} & Data for reads, 32 bits \\
    \code{SCmdAccept} & Signal that command is accepted, 1 bit \\
    \code{SDataAccept} & Signal that data is accepted, 1 bit \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Remarks}

\code{SCmdAccept} is valid only while a command is unequal to
\code{IDLE}. Consequently, \code{SCmdAccept} must be properly
multiplexed to support multiple slaves. For handshaking via
\code{SResp}, it is sufficient to combine the responses of different
slaves with OR.

\wolf{With \code{writeresp\_enable} we could also require a response
  for ``normal'' writes.}

\wolf{\code{OCPburst} currently allows neither pipelining nor
  same-cycle responses. If we want to, we can make it more like
  \code{OCPcore} or \code{OCPio}.}

\wolf{I am pretty sure that we need to include \code{MDataValid} when
  using single request bursts. Please check if you understand the spec
  the same way.}

Note 4:
Bursts are restricted to exactly four words (D1, D2, D3 and D4), or some other constant which is a power of 2.  The address must be aligned. Burst data must be provided in four consecutive cycles. SResp is active during these cycles. For a burst write the master may have to provide D1 for two or more cycles if SResp is not active in the first cycle of the transaction.

\wolf{\code{SResp} is \emph{not} active during write bursts according
  to the OCP spec. If necessary, handshaking for write bursts is done
  via \code{SCmdAccept} and/or \code{SDataAccept}.}

\martin{Bust: Is it legal (in our OCP subset) that SCmdAccept comes earlier than SDataAccept?}

\martin{Burst: we said that we do not support pipelined requests in bust mode.
So when the slave sets SResp we are not allowing the master to initiate the next
request in the same cycle. Right?}

\martin{Are OCP addresses always in bytes? Even on the burst interface? I assume yes.
We need to look this up. -- According to Wolfgang they are.}


\clearpage
\subsection{Timing Diagrams}

\subsubsection{\code{OCPcore}}

Figure~\ref{fig:timing_core} shows a sequence read/write/read in
\code{OCPcore}, where the slave delays the response to the write by
one cycle.

\begin{figure}
\centering
\includegraphics{fig/timing_core}
\caption{Timing diagram for \code{OCPcore}}
\label{fig:timing_core}
\end{figure}

\begin{enumerate}[A:]
\item The master issues a read by setting \code{MCmd} to \code{RD},
  \code{MAddr} to \code{A$_1$} and \code{MByteEn} to \code{E$_1$}.
\item The slave responds to the read issued in cycle A by setting
  \code{SResp} to \code{DVA} and returning the appropriate data. The
  master can issue the next command in the same cycle as it receives
  the response and issues a write command \code{WR}. The
  master provides the byte enable value \code{E$_2$} along with the
  data \code{D$_2$} to specify which bytes should be actually written.
\item The slave does not respond immediately and the master is
  stalled. \code{MCmd} must be \code{IDLE} while the master is
  stalled.
\item The slave responds to the write issued in cycle B. The master
  issues a read in the same cycle.
\item The slave responds to the read issued in cycle D.
\end{enumerate}

\clearpage
\subsubsection{\code{OCPio}}

Figure~\ref{fig:timing_io} shows a sequence read/write/read in
\code{OCPio}, where the slave does not accept the write immediately
and delays the response to the write by one cycle.

\begin{figure}
\centering
\includegraphics{fig/timing_io}
\caption{Timing diagram for \code{OCPio}}
\label{fig:timing_io}
\end{figure}

\begin{enumerate}[A:]
\item The master issues a read by setting \code{MCmd} to
  \code{RD}. The slave accepts the command by setting
  \code{SCmdAccept} to high and responds immediately by setting
  \code{SResp} to \code{DVA} and returning the appropriate data.
\item The master issues a write command \code{WR} with
  data \code{D$_2$} and byte enables \code{E$_2$}. The slave
  signals that it does not accept the command by setting
  \code{SCmdAccept} to low.
\item As the slave did not accept the command in cycle B, the master
  still issues the command. The slave accepts the command by setting
  \code{SCmdAccept} to high.
\item The slave responds to the write it accepted in cycle C. Note
  that a) the master is not allowed to issue a new command immediately
  and b) \code{SCmdAccept} may take any value, because \code{MCmd} is
  \code{IDLE}.
\item The master issues a read to which the slave responds immediately.
\end{enumerate}

\clearpage
\subsubsection{\code{OCPburst}}

Figure~\ref{fig:timing_burst} shows a read followed by a write in
\code{OCPburst}, were the slave does not accept the first data word
immediately.

\begin{figure}
\centering
\includegraphics{fig/timing_burst}
\caption{Timing diagram for \code{OCPburst}}
\label{fig:timing_burst}
\end{figure}

\begin{enumerate}[A:]
\item The master issues a read command by setting \code{MCmd} to
  \code{RD}. The slave accepts by asserting \code{SCmdAccept}.
\item The slave provides the first response \code{DVA$_{1.0}$}, with
  data from address \code{A$_1$}.
\item The slave provides the second response \code{DVA$_{1.1}$}, with
  data from address \code{A$_1$+4}.
\item The slave provides the third response \code{DVA$_{1.2}$}, with
  data from address \code{A$_1$+8}.
\item The slave provides the fourth and last response
  \code{DVA$_{1.3}$}, with data from address \code{A$_1$+12}.
\item The master issues a write command by setting \code{MCmd} to
  \code{WR} and provides the first data word \code{D$_{2.0}$} width
  byte enables \code{E$_{2.0}$} It signals that the data is valid by
  asserting \code{MDataValid}. The slave signals that it cannot accept
  the data by setting \code{SDataAccept} to low.
\item As the slave did not accept the data in cycle F, the master
  keeps issuing the command and providing the data word
  \code{D$_{2.0}$}. The slave now accepts the data by setting
  \code{SDataAccept} to high.
\item The master provides the second data word, \code{D$_{2.1}$} with
  byte enables \code{E$_{2.1}$}.
\item The master provides the third data word, \code{D$_{2.2}$} with
  byte enables \code{E$_{2.2}$}.
\item The master provides the fourth and last data word,
  \code{D$_{2.3}$} with byte enables \code{E$_{2.3}$}.
\item The slave responds to the write burst by setting \code{SResp} to
  \code{DVA}.
\end{enumerate}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\chapter{Implementation}

\martin{This sections shall describe implementation details,
decisions, and options.}

After a first implementation of Patmos in VHDL we did a cleanup and
rewrite in a the hardware description language Chisel~\cite{chisel:dac2012}.
The following notes on the implementation of Patmos and implementation
decisions is based on first design discussions within the VHDL version
and concrete implementation experiments with Chisel. All size and frequency
numbers are from the Chisel implementation. A comparison between VHDL
and Chisel would be of great interest.

LoC, excluding the copyright header at 6.4.2013:
Chisel: 996
VHDL: 3020

The current version of Patmos uses Chisel version 1.0.8, but we will soon
migrate ti Chisel 2.x.

\section{Component Organization and Pipeline Structure}

The architecture of Patmos is structured around five components, each
representing one pipeline stage. Each component contains the \emph{left}
pipeline register. E.g., the output of the \code{DEC} stage (decode signals,
the two register values, and the immediate field) is combinatorial from
the decode stage and registered in the \code{EX} stage. The motivation of
this organization is that input registers of on-chip memory elements (e.g., instruction
memory, register file, and data memory) are part of the pipeline register.
They need to be fed unconditionally from the unregistered output of
the former stage.

Each stage has exactly one pipeline register, which is placed at the begin
of the component. The pipeline registers use an enable for stalling.
Register that have no enable (input registers of on-chip memories) need
a \emph{shadow} register and a multiplexer for stalls.

The interface from the EX stage to the MEM stage might use one
field for ALU results and the store data or individual fields. Individual
fields might reduce the pressure on the ALU multiplexers.
\martin{Update to the current implementation -- check for difference.}


\section{Register File}

There are two options to implement the register file (RF) in an FPGA: (1) use
two on-chip memories to provide two read ports and one write port, or (2)
use dedicated registers and larger multiplexer structures for the read
ports. Usually one aims to use on-chip memory for the RF. However,
in a design constraint largely by the available amount of on-chip memory,
a RF built out of registers might be preferable.

For a dual issue pipeline we need 4 read ports and 2 write ports into the RF.
We explored double clocking of a on-chip based RF in~\cite{patmos:ppes2011}.
It is feasible, the resulting maximum frequency fits for the ALU path, but feels
a little bit brittle. A RF from registers might give a more robust design for the
two write ports. Furthermore, Chisel does not provide the possibility to use
more than one clock.

The ideal solution would be to make it configurable if on-chip memory or
LCs are used. The issue width should also be configurable.




\section{Resource and Fmax Numbers}

State 13.3.2013 with Chisel and DE2-70: A shared field (for EX to MEM?) results 3435 LCs
and 81.7 MHz, two fields in 3499 LCs and 81.8 MHz. Looks like not a big deal,
but just 64 more LCs. Where does this cost come from? A very inefficient
enable on the pipeline register (MUX instead of an enable signal?).

\martin{Update with reduced ALU muxes and also with additional second
ALU pipeline (forwarding). Is dual issue configurable?}

\section{ALU Discussion}

The large multiplexers and the forwarding limit the maximum frequency.
We have already removed the expensive rotate instructions. Maybe more
can go (e.g. \code{abs}).

Current version (4.4.2013) with all ALU operations and test case ALU.s
for the DE2-70 is: 3415 LCs, 85.44 MHz. Dropping \code{rsub} and all unary
ALU operations: 3173 LCs, 91.91 MHz.

\chapter{Build Instructions}
\label{ch:build_instructions}

In the following we present the Patmos build instructions on a Linux/Ubuntu
system (13.10).\footnote{I used the 32-bit version of Ubuntu to simplify the Quartus installation.}
Patmos and the compiler have also been successfully installed on a Mac OSX
system. The support of Windows is marginal, or basically not existent.


\section{Setup On Ubuntu 13.10}

After a plain Ubuntu installation several packages need to be installed.
The following apt-get lists the packages that need to be
installed:\footnote{Some packages might be available in newer version
when reading this document.}

\begin{verbatim}
sudo apt-get install git openjdk-7-jdk gitk cmake make g++ texinfo flex bison \
  subversion libelf-dev graphviz libboost-dev libboost-program-options-dev ruby1.9.1 \
  ruby1.9.1-dev python zlib1g-dev gtkwave gtkterm
\end{verbatim}

For the Quartus setup it is best to change the default shall to \code{/bin/bash}:

\begin{verbatim}
sudo rm /bin/sh
sudo ln -s /bin/bash /bin/sh
\end{verbatim}

\paragraph{Ubuntu 12.04 LTS}

We also tested the setup on long-term support release 12.04 LTS. The compiler
(gcc) is recent enough for all T-CREST code. Only some additional packages need to
be installed:

\begin{verbatim}
sudo apt-get install python3-lxml
\end{verbatim}

\subsection{Compiler and Patmos}
\label{sec:build:compiler}

We assume that the T-CREST project will live in \code{\$HOME/t-crest}.
Patmos and the compiler can be checked out from GitHub and built as follows:

\begin{verbatim}
mkdir ~/t-crest
cd ~/t-crest
git clone https://github.com/t-crest/patmos-misc.git misc
./misc/build.sh
\end{verbatim}

For developers with push permission generate an ssh key and upload
it at GitHub (see \url{https://help.github.com/articles/generating-ssh-keys}
for detailed instructions).
The ssh based clone string for write access is then:

\begin{verbatim}
git clone git@github.com:t-crest/patmos-misc.git misc
./misc/build.sh
\end{verbatim}

This script (\code{build.sh}) will checkout several other repositories (the compiler, library,
the Patmos source, and benchmarks) and
builds the compiler, the Patmos simulator, and the test benches.
Therefore, take a cup of coffee and find some nice reading.
After building the compiler add the path
to the compiler executables (e.g., into your \code{.bashrc} or \code{.profile}):
\footnote{The path needs to be absolute. LLVM cannot handle
a path relative to the home folder \textasciitilde{}, e.g., \code{\textasciitilde{}/t-crest/local/bin}.}
%\footnote{The initialization file name depends on your system and setup. \code{.profile} files are only read by login
%terminals (remote terminals or at login), but your \code{.bashrc} or \code{.zshrc} (for interactive terminals within a
%session) might be set up to source \code{.profile} as well.}

\martin{In Mac OS X I have only .profile and I don't understand what the issue by 'only read by login terminals'.}

\stefan{Relative paths should actually work, but the \textasciitilde{} shortcut is shell-specific and may not work, but
not sure about this.}

\begin{verbatim}
export PATH=$PATH:$HOME/t-crest/local/bin
\end{verbatim}

Optionally, you may additionally add the \code{misc} checkout to your path, so that \code{build.sh} and the helper tools in 
\code{misc} can be executed from everywhere.

\begin{verbatim}
export PATH=$PATH:$HOME/t-crest/misc
\end{verbatim}

A complete logout from Ubuntu might be needed to take effect (just closing
a terminal window is not enough, depending on how you set up your profile files).

You can test your installation by checking if the compiler is available:

\begin{verbatim}
patmos-clang --version
\end{verbatim}

For correct correct signing of your changes set the username and
email in git with:

\begin{verbatim}
git config --global user.name "Joe Someone"
git config --global user.email "joe.someone@domain.com"
\end{verbatim}

\subsection{Quartus}

Download the free web edition of Quartus from Altera. The Linux version is
installed as follows:\footnote{\url{http://www.altera.com/literature/manual/quartus_install.pdf}}

\begin{verbatim}
tar xvf Quartus-web-xxx.tar
\end{verbatim}

The software installation is started with a plain \code{setup.sh}.\footnote{Or \code{bash setup.sh}}
Then add the bin directory of Quartus to your \$PATH.
%
For access to the serial port and access rights for the USB Blaster following additional steps are needed:

\begin{verbatim}
# Add user to dialout group for the serial port access
sudo usermod -a -G dialout user

# Add permissions to access the Altera USB Blaster
sudo su -
echo "SUBSYSTEM==\"usb\", DRIVER==\"usb\", ATTR{idVendor}==\"09fb\", \
ATTR{idProduct}==\"6001\", MODE=\"0666\"" > /etc/udev/rules.d/51-usbblaster.rules

udevadm control --reload-rules
\end{verbatim}

After fixing the permissions for the USB Blaster open Quartus and test if the
cable is found with the programmer. Select USB-Blaster in \emph{Hardware Setup}.
When connected to an FPGA test the USB-Blaster with \emph{Auto Detect}
(With the DE2-115, a question about shared JTAG ID pops up -- select EP4CE115).

Quartus~13.1 drops the support of Cyclone~II devices. Therefore, the
(phased out) Altera DE2-70 board is not supported anymore. Version 13.0 supports Cyclone~II\
till Cyclone~V devices and might be the best option at the moment.

\section{Setup On Mac OS X}

Several tools are needed, best installed with MacPorts. For Patmos simulator and assembler:
\code{boost}, \code{libelf}.
For simulation with ModelSim: \code{wine}
For Aegean: python33, py33-lxml. Make a link from python3 to python3.3 as this is the way it is invoked.
\todo{Alex suggested a way to avoid this by querying how to invoke python.}

\section{Hello World}

We can start with the standard, harmless looking Hello
World:\footnote{This example code is not part of the distribution, but
can be put at any directory.}
\begin{lstlisting}
int main() {
    printf("Hello Patmos!\n");
}
\end{lstlisting}

With the compiler installed it can be compiled to a Patmos executable
and run with the simulator as follows:

\begin{verbatim}
patmos-clang hello.c
pasim a.out
\end{verbatim}

However, this innocent examples is quiet challenging for an embedded system:
It needs a C compiler, an implementation of the standard C library, printf
itself is a challenging function, the generated ELF file needs to be understood
by a tool and the individual sections downloaded, and finally a terminal (often
a serial line) needs to be available on the target, and your test PC needs to
have a serial line as well and a terminal program needs to run.

Therefore, we might start from a minimal assembler program and execute
that in the simulator and emulator. From that base we can build up too
a multi-core version of Patmos that executes in an FPGA and bootstraps
with programs loaded via a serial port.


\section{Building Patmos}

The whole build process of Patmos,%
\footnote{Get the source from GitHub with: \code{git clone git@github.com:t-crest/patmos}}
applications in assembler
and in C, configuration of the FPGA, and downloading an application
is \code{Makefile} based. The build of Patmos is within the \code{patmos} folder,
therefore, the following descriptions assumes you have changed to:

\begin{verbatim}
    t-crest/patmos
\end{verbatim}


The complete design flow (including the LLVM
based C compiler) can execute in a Linux machine. The flow without
the C compiler should be able to execute in a Windows/Cygwin environment.
Under Mac OS X all tools, except Quartus, are working (ModelSim under
wine). For FPGA synthesis and configuration Windows XP within a VMWare
virtual machine is a possible solution.

On a Linux box with the installed LLVM compiler and Quartus in your PATH,
the complete build processes for a Hello World is as follows:

\begin{verbatim}
make BOOTAPP=bootable-bootloader APP=hello_puts \
   tools comp gen synth config download
\end{verbatim}

However, this involves quite many steps. Therefore, we suggest doing some \emph{manual}
buildup to explore the full build process and the possibilities.

As a start we build some tools (e.g., the assembler, simulator, file conversion
utility, and the boot loader). This has to be done once only.

\begin{verbatim}
make tools
\end{verbatim}

\subsection{A Few Assembler Instructions}

We start with a very small assembler program that moves a few values into registers
(see \code{asm/basic.s}). With following make command the program is assembled
and executed in the software simulator of Patmos.

\begin{verbatim}
make swsim BOOTAPP=basic
\end{verbatim}

The simulator options are set to write out the register contents after each instruction.
The emulator (the Chisel based simulator) can execute the same program
with following command:

\begin{verbatim}
make hwsim BOOTAPP=basic
\end{verbatim}

This command assembles the application, executes the Chisel based hardware
construction during which the program is used to initialize the on-chip ROM,
generates a C++ based emulator, compiles that emulator, and executes it.
The emulator shows the register content after each instruction.

Those two Patmos simulations, the software simulator and the Chisel based emulator,
are used for a co-simulation based test. In this co-simulation all available assembler
programs are executed in both simulations and the register out put is compared.
The test can be started with:

\begin{verbatim}
make test
\end{verbatim}


\todo{ModelSim simulation}

\subsection{We Can Blink in Assembler}

\todo{write}

\subsection{A C Based Blinking LED}

As a first real example we build the embedded version of Hello World, the
blinking LED, from a C program. You can find the C source in \code{c/blinking.c}.

\begin{verbatim}
make BOOTAPP=bootable-blinking comp gen synth config
\end{verbatim}

Additionally to blinking an LED this program also writes alternating `0' and `1'
to the serial port. Connect the FPGA board to your serial port,
open a terminal of your choice (e.g., \code{gtkterm}), connect to the serial port,
set the baud rate to 115200, no parity, and no handshaking.
You should see alternating `0' and `1' sent out synchronous to the blinking.

Note that the program name (\code{blink}) is prefixed by \code{bootable-}.
The marker selects the right compiler settings for a program that ends up in
the Patmos on-chip ROM. As the on-chip memory is limited, only tiny programs
are supported in this execution mode.

Figure~\ref{fig:blink} shows the code for the embedded Hello World
C program. Two constants (0xF0000900 and 0xF0000804) are the addresses
of the IO devices LED and serial port. IO devices connected to Patmos are
connected to the local, uncached memory area. This is the same memory
area where data SPM and NoC SPM are connected. Therefore, to access them
one needs to use the local load/store instructions. With the attribute \code{\_SPM}
the compiler is instructed to emit the correct load and store instructions.

\begin{lstlisting}[float,caption={A blinking LED\label{fig:blink}}]
/*
    This is a minimal C program executed on the FPGA version of Patmos.
    An embedded Hello World program: a blinking LED.

    Additional to the blinking LED we write to the UART '0' and '1' (if available).

    Author: Martin Schoeberl
    Copyright: DTU, BSD License
*/

#include <machine/spm.h>

int main() {

    volatile _SPM int *led_ptr = (volatile _SPM int *) 0xF0000900;
    volatile _SPM int *uart_ptr = (volatile _SPM int *) 0xF0000804;
    int i, j;

    for (;;) {
        *uart_ptr = '1';
        for (i=2000; i!=0; --i)
            for (j=2000; j!=0; --j)
                *led_ptr = 1;


        *uart_ptr = '0';
        for (i=2000; i!=0; --i)
            for (j=2000; j!=0; --j)
                *led_ptr = 0;

    }
}
\end{lstlisting}

\martin{I'm still a little bit confused when to use the APP and the BOOTAPP
thing.}

\martin{TODO: we need to streamline the make process a little bit.
The targets might be a little bit confusing. Patmos is compiled two times.}


\subsection{Make Targets}

A list of the most important make targets:

\begin{description}
\item[tools] build of all tools, including the Patmos software simulator
\item[asm] assemble source (from folder \code{asm})
\item[swsim] execute the Patmos simulator
\item[hwsim] execute the Patmos emulator
\item[emulator] build the Chisel based C++ emulator
\item[comp] compile a C program as loadable ELF binary
\item[bootcomp] compile a C program as a bootable image
\item[gen] generate the Verilog code
\item[synth] synthesize for an FPGA
\item[config] configure the FPGA
\item[download] download an elf file into the main memory via the Patmos bootloader
\item[test] run all assembler tests 
\end{description}

The name of an application that can execute from the on-chip ROM is set
with the BOOTAPP variable.

\subsection{Download of ELF Files}
\label{sec:elf:files}

On a Linux box with the installed LLVM compiler and Quartus in your PATH,
the complete build processes for the Hello World is as follows:

% make BOOTAPP=bootable-bootloader APP=hello_puts tools synth comp config download
\begin{verbatim}
make BOOTAPP=bootable-bootloader APP=hello_puts \
   tools comp gen synth config download
\end{verbatim}

You should see the download information and then the greeting from Patmos:

\begin{verbatim}
/home/martin/t-crest/patmos/install/bin/patserdow -v /dev/ttyUSB0 /home/martin/t-crest/patmos/tmp/hello_puts.elf
Port opened: true
Params set: true
Elf version is '1':true
CPU type is:48875
Instruction width is 32 bits:true
Is Big Endian:true
File is of type exe:true
Entry point:131076

[++++++++++] 49778/49778 bytes
Hello, World!

EXIT 0
\end{verbatim}

The \code{Makefile} use following variables to configure the build process:
\code{BOOTAPP} is an application that ends in the on-chip ROM. This may
be an assembler program or a simple C program;
most prominent the boot loader for ELF binaries.
A C program that shall be compiled as ROM target needs to be prefixed
with \code{bootable-}.
\code{APP} is a C program resulting in an ELF binary that can be either
loaded by the emulator or the boot loader when executing in an FPGA.
% TODO: test and talk about patsim. Having a complete ELF in the FPGA
% without the boot loader would be nice as well.

\todo{Describe ELF download without building the FPGA}


Here an example of the individual steps to build the blinking LED C
hello world (on a different FPGA board):
\begin{verbatim}
make tools
make BOOTAPP=bootable-echo bootcomp gen
make BOOTAPP=bootable-echo BOARD=bemicro synth
make BOARD=bemicro BLASTER_TYPE=Arrow-USB-Blaster config
\end{verbatim}

This split of the make commands is for demonstration. It is
possible to merge all steps into a single make (on Linux
systems) or two steps when using two operating
systems (e.g., Mac OSX for compilation and Windows for synthesis).

\paragraph{Emulator and elf File}

The emulator can read a standard ELF file. An example how to compile
a small C program that uses part of the standard library and executing
it on the emulator is as follows:

\begin{verbatim}
make emulator
make comp APP=hello_puts
install/bin/emulator tmp/hello_puts.elf
\end{verbatim}

\martin{Where is the make target to run the emulator with an ELF file?}

\subsection{Supported FPGA Boards}

At the time of this writing we have mainly focused on Altera FPGA based boards. Following boards
are directly supported in the default build process:

\begin{itemize}
\item Altera DE2-70 (\code{altde2-70})
\item Altera DE2-115 (\code{altde2-115})
\item Altera/Farnell BeMicro (\code{bemicro})
\end{itemize}

Setting the \code{BOARD} variable configures the board that shall be used.
Without changing the \code{Makefile} the default of the board (and any other build variables)
can be overridden by providing a local \code{config.mk} that is included in the \code{Makefile}.

\subsection{Multicore Patmos}

A multicore Patmos with shared external memory and the network-on-chip Argo is configured via
the Aegean framework. Aegean is a collection of Python scripts that read in XML based configuration
description (e.g., topology, network connections, processor types,...). Aegean generates the Chisel based
components (Partmos, memory arbitration tree, and memory controller), generates the VHDL top-level
to connect them with the VHDL based NoC, synthesizes the hardware, compiles the application for the
individual cores, configures the FPGA, and downloads the application.

In directory \code{aegean} run:

\begin{verbatim}
make platform AEGEAN_PLATFORM=mandelbrot_demo
make synth config AEGEAN_PLATFORM=mandelbrot_demo
\end{verbatim}

to generate (and synthesize) the mandelbrot application on a 4 core
version of Patmos for an Altera DE2-115 FPGA board. This application is compiled
into the on-chip memories and therefore executing right after configuration of the
FPGA. Have a terminal open and connected to the serial port (115200 baud, 1 stop bit,
no handshake) during and after the FPGA configuration and you shall see the output of
the mandelbrot calculation.

However, the approach to have the application in on-chip memory works for tiny programs
only. Furthermore, each software change needs a new synthesize run. A better approach is
to build a platform that contains a bootloader (similar to the single core version) and some
startup code to synchronize the program start with the other cores. This platform is the default
configuration in Aegean and needs to be generated only once with:

\begin{verbatim}
make platform
make synth
\end{verbatim}

The FPGA is configured from within the \code{aegean} directory with:

\begin{verbatim}
make config
\end{verbatim}

The compilation and download of the application is then best done within
the \code{patmos} directory with:

\begin{verbatim}
make APP=hello_puts comp download
\end{verbatim}

This application is the same single core \emph{Hello World} application that
we used in Section~\ref{sec:elf:files}. However, here we just compiled the
application and downloaded it via the serial port. We synthesized and configured
the FPGA from within the Aegean project for the multi-core version.

\todo{We shall have here three simple hello world applications: (1) just plain shared memory 4 cores saying hello - DONE -,
(2) a CMP program that uses shared memory, and (3) a simple NoC setup.}


%[2014-02-28 17:24:08 ] Stefan Hepp: #include <machine/patmos.h>
%[2014-02-28 17:24:22 ] Stefan Hepp: zu finden in patmos-newlib/newlib/libc/machine/patmos/machine/patmos.h

\section{The Xilinx ML605 Platform}

For the evaluation within the T-CREST project the Xilinx ML605 FPGA board was chosen as the `standard'
evaluation platform. The T-CREST platform contains, besides several Patmos' connected with the Argo NoC,
a memory tree, called BlueTree, from UoY and the time-predictable memory controller from TU/e. As the
building this platform needs some non-free tools and including closed source code, we provide prebuilt
configurations of the T-CREST platform as bit files available for download:

\begin{itemize}
\item A 4 core version: \url{http://patmos.compute.dtu.dk/t-crest-4core.bit}
\item A 16 core version: 
\end{itemize}

To configure the FPGA use the IMPACT software from Xilinx (command \code{impact}), which is part of the Xilinx ISE package
and needs no license (It is also available in a smaller Lab package). To compile an application and
download it to the ML605 follow the exact same steps as for the Altera CMP version, e.g., from
within the \code{patmos} directory:

\begin{verbatim}
make APP=hello_puts comp download
\end{verbatim}

\subsection{Getting the Xilinx Configuration Cable to Work}

Xilinx does not support Ubuntu (at least the last few versions) directly. The following description
is a summary of the help from Jamie.

Copy some Xilinx cable specific files to \code{/usr/share} with:

\begin{verbatim}
sudo cp /opt/Xilinx/14.7/ISE_DS/ISE/bin/lin/install_script/install_drivers/\
linux_drivers/pcusb/*.hex /usr/share
\end{verbatim}
 
Ensure that \code{fxload} is installed with:
\begin{verbatim} 
sudo apt-get install fxload
\end{verbatim}

Create \code{/etc/udev/rules.d/80-xusbdfw.rules} with following content:

\begin{tiny}
\begin{verbatim}
# version 0003
ATTR{idVendor}=="03fd", ATTR{idProduct}=="0008", MODE="666"
SUBSYSTEM=="usb", ACTION=="add", ATTR{idVendor}=="03fd", ATTR{idProduct}=="0007", RUN+="/sbin/fxload -v -t fx2 -I /usr/share/xusbdfwu.hex -D $tempnode"
SUBSYSTEM=="usb", ACTION=="add", ATTR{idVendor}=="03fd", ATTR{idProduct}=="0009", RUN+="/sbin/fxload -v -t fx2 -I /usr/share/xusb_xup.hex -D $tempnode"
SUBSYSTEM=="usb", ACTION=="add", ATTR{idVendor}=="03fd", ATTR{idProduct}=="000d", RUN+="/sbin/fxload -v -t fx2 -I /usr/share/xusb_emb.hex -D $tempnode"
SUBSYSTEM=="usb", ACTION=="add", ATTR{idVendor}=="03fd", ATTR{idProduct}=="000f", RUN+="/sbin/fxload -v -t fx2 -I /usr/share/xusb_xlp.hex -D $tempnode"
SUBSYSTEM=="usb", ACTION=="add", ATTR{idVendor}=="03fd", ATTR{idProduct}=="0013", RUN+="/sbin/fxload -v -t fx2 -I /usr/share/xusb_xp2.hex -D $tempnode"
SUBSYSTEM=="usb", ACTION=="add", ATTR{idVendor}=="03fd", ATTR{idProduct}=="0015", RUN+="/sbin/fxload -v -t fx2 -I /usr/share/xusb_xse.hex -D $tempnode"
\end{verbatim}
\end{tiny}

Restart the udev service with:

\begin{verbatim} 
sudo udevadm control --reload-rules
\end{verbatim}

Make sure that libusb is installed (e.g., in \code{/lib/i386-linux-gnu} on a 32-bit Ubuntu) and make a symbolic link with

\begin{verbatim} 
sudo ln -s libusb-1.0.so.0 libusb.so
\end{verbatim}

Best restart your machine and connect the USB cable again.

\paragraph{Using iMPACT}

Start impact from a terminal with 

\begin{verbatim} 
impact
\end{verbatim}

Impact pops up some dialog boxes:

Do you want iMPACT to automatically load the last saved project for you? -- answer with \emph{No}.

Do you want the system to automatically create and save a project file for you? -- answer \emph{Yes}.

Next window click Ok. iMPACT should detect the JTAG chain with two devices. Answer the
following dialog boxes wit \emph{No} and \emph{Cancel}.

Right-click on the xc6vlx240t device and select \emph{Assign New Configuration File} and select
the provided .bit file (e.g., \code{t-crest-4core.bit}). Answer the following question about attached
Flash PROMs with \emph{No}.

Right-click again on the FPGA symbol and select \emph{Program}. Select \emph{Ok} on the next
dialog box and the FPGA shall be configured.

Then compile and download an application as described above.

\paragraph{Installing the Xilinx Tools}

After extracting the tools with \code{tar} the install procedure is started within the
\code{Xilinx\_*} directory with:

\begin{verbatim}
sudo ./xsetup
\end{verbatim}

\paragraph{Starting Xilinx ISE}

In some setups it is needed to source a setup script, e.g.:

\begin{verbatim}
source /opt/Xilinx/14.7/ISE_DS/settings64.sh
\end{verbatim}

Then ISE can be started with \code{ise}.

\subsection{Updating the Patmos Cores with Aegean}

The correct Verilog file for the two version of Patmos (core 0 that downloads an application and the other cores)
is built with the \emph{Aegean} tool. The 4 cores version is built with:

\begin{verbatim}
make platform AEGEAN_PLATFORM=ml605_4core
\end{verbatim}

% make compile AEGEAN_PLATFORM=ml605_4core

A 16 core version is available as well. The generated Verilog files are the same,
but the schedule for the Argo NoC is different (which is copied to
\code{t-crest/patmos/c/nocinit.c}).

All generated file can be cleaned with \code{make cleanall}.

The configuration of T-CREST with Bluetree and the TU/e memory controller
is available via a .tgz file, exchanged via email to protect IP rights.
Therefore, they are not integrated in Aegean and some manual code copying is needed.
The \code{ml605.tgz} shall be extracted within the \code{t-crest} directory.
Copy the Patmos Verilog files (\code{ml605mPatmosCore.v} and \code{ml605mPatmosCore.v})
from the build directory (\code{t-crest/aegean/build/ml605\_4core}) into \code{t-crest/ml605}.



\section{ModelSim License}
In the case that you have a DTU Compute login you can access the license servers from outside the DTU network by setting up an SSH tunnel.
An example of how such a tunnel can be set up follows, you need to insert you own username.

\begin{verbatim}
ssh -L 1717:angel2:1717 -L 1718:angel2:1718 ${USERNAME}@sshlogin.compute.dtu.dk
\end{verbatim}
%% $

When the SSH tunnel is setup the LM\_LICENSE\_FILE needs to be set to:
\begin{verbatim}
LM_LICENSE_FILE=1717@localhost
\end{verbatim}

This way of setting up an SSH tunnel might also work for other institutions.

\paragraph{License settings for ModelSim and Xilinx}

\begin{verbatim}
export LM_LICENSE_FILE=1717@angel1: 1717@angel2: 1717@angel3
export XILINXD_LICENSE_FILE="2100@eda1.imm.dtu.dk"
\end{verbatim}



\chapter{The Patmos Compiler}
\label{sec:compiler}

\input{toolchain}


\chapter{Application Binary Interface}
\label{sec:abi}

\section{Data Representation}

Data words in memories are stored using the big-endian data representation, this
also includes the instruction representation.

\section{Register Usage Conventions}

The register usage conventions for the general purpose registers are as follows:
\begin{itemize}
  \item \texttt{r0} is defined to be zero at all times.
  \item \texttt{r1} and \texttt{r2} are used to pass the return value on
        function calls. \\
        For 64 bit results, the high part is stored in \texttt{r1},
        the low part in \texttt{r2}.
	32 bit results are returned using \texttt{r1} only.
  \item \texttt{r3} through \texttt{r8} are used to pass arguments on function
        calls. \\
        For 64 bit arguments, the high part is stored first,
        followed by the low part.\\ E.g., for a 64 bit argument passed in
        \texttt{[r3,r4]}, the high part is in \texttt{r3}, the low part
        in \texttt{r4}.
  \item \texttt{r27} is used as temp register.
  \item \texttt{r28} is defined as the frame pointer and \texttt{r29} is defined as the stack
        pointer for the \emph{shadow} stack in global memory. The use of a frame
        pointer is optional, the register can freely be used otherwise.
	\texttt{r29} is guaranteed to always hold the current stack pointer and is not used
	otherwise by the compiler.
  \item \texttt{r30} and \texttt{r31} are defined as the return function base
        and the return function offset.
        Usually, they are passed as operands to the \texttt{ret} instruction.
  \item \texttt{r1} through \texttt{r19} are caller-saved \emph{scratch}
        registers.
  \item \texttt{r20} through \texttt{r31} are callee-saved \emph{saved}
        registers.
\end{itemize}

The usage conventions of the predicate registers are as follows:
\begin{itemize}
  \item all predicate registers are callee-saved \emph{saved} registers.
\end{itemize}

\daniel{I have no educated opinion whether caller- or callee-saved.}
\fb{This is rather easy to change in the compiler, and should be evaluated
accordingly.}

\daniel{
What I'd like to have for single-path code is a guard predicate parameter for the called function, e.g., always passed in p1.
}
\fb{What would the guard predicate be good for? If need be we could define a
separate calling convention for single-path-programming programs; in order to
keep the conventions for the other programs sane.}

\stefan{We would gain in most cases by making predicates caller-saved, since predicates are usually not
used across function calls, this saves up to 6 instructions per call.
Predicate live ranges over calls only happen with single-path and if-conversion. However, having only caller-saved predicates
makes if-conversion of calls too costly (and too complex). A nicer solution would be to have p1-p4 caller-saved and
p5-p7 callee saved. I got this to work (do not alias s0 with p1-p7 in RegInfo.td, mark s0 and p5-p7 as callee saved
in RegInfo.cpp, and in processBeforeCalleeSavedScan set s0 as used when any of p5-p7 is used), but this would require the
if-converter to use callee-saved predicates when converting calls, i.e., to change the predicate of the condition.
Sounds easy, but is actually far from trivial to hack into the if-converter, and only works if the if-converter runs before the
prologue-inserter, so we stay with callee saved registers for now and live with the costs of spilling s0 at nearly every call.}


The usage conventions of the special registers are as follows:
\begin{itemize}
  \item The stack cache control registers \texttt{ss} and \texttt{st} are callee-saved
        \emph{saved} register.
  \item \texttt{s0}, representing the predicate registers, is a callee-saved \emph{saved} register.
  \item All other special registers are caller-saved scratch registers and should not be used
        across function calls.
\end{itemize}

\section{Function Calls}
\label{sec:function_calls}

Function calls have to be executed using the \texttt{call}
instruction that automatically prefetches the target function to the method
cache and stores the return information to the general-purpose register \texttt{r31}.
At a function call, the callers base address has to be in \texttt{r30}.
The callee is responsible to store/restore the callers function base and pass it as first
operand to the return instruction.
The \texttt{call} and \texttt{brcf} instructions neither use nor modify \texttt{r30}, the
return function base is only used by \texttt{ret}.

The register usage conventions of the previous section indicate which registers
are preserved across function calls.

The first $6$ arguments of integral data type are passed in registers, where
64-bit integer and floating point types occupy two registers. All other
arguments are passed on the \emph{shadow} stack via the global memory.

\fb{We could pass arguments via the stack cache, however, this would work only
for functions with a fixed number of arguments. The calling convention for
variadic function would then differ from the standard conventions. We should
probably also introduce a size limit on how many arguments should be passed via
the stack cache. Thus, for a first shot I decided to keep the conventions
simple.}
\martin{I would like to see arguments passed via the stack cache. No
reason to go via main memory (when the number is fixed.}

\martin{BTW: an on-chip stack, with single cycle access, is not so different from
a large register file. Maybe using a sliding window to keep the number of
addressing bits down. Shall we merge the stack cache with the register file?
Are we than redoing a stack (Java) processor?}

When the return function base \texttt{r30} and the return offset \texttt{r31}
needs to be saved to the stack, they have to be saved as the first elements
of the function's stack frame, i.e., right after the stack frame of the
calling function. Note that in contrast to \texttt{br} and \texttt{brcf} the
return offset refers to the next instruction after the \emph{delay slot} of the
corresponding \texttt{call} and can be implementation dependent (cf.\ the description
of the \texttt{call} and \texttt{ret} instructions).

\section{Sub-Functions}
A function can be split into several sub-functions. The program is only allowed to use
\texttt{br} to jump within the same sub-function. To enter a different sub-function,
\texttt{brcf} must be used. It can only be used to jump to the first instruction of a
sub-function.

In contrast to \texttt{call}, \texttt{brcf} does not provide link information.
Executing \texttt{ret} in a sub-function will therefore return to the last \texttt{call}, not to the last \texttt{brcf}.
Function offsets however are relative to the \emph{sub-function} base, not to the surrounding function.
The function base register \texttt{r30} must therefore be set to the base address
of the current \emph{sub-function} for calls inside sub-functions.

A sub-function must be aligned and must be prefixed with a word containing the size of the sub-function,
like for a regular function. If a function is split into sub-functions, the first sub-function
must also be prefixed with the size of the first sub-function, not with the size of the whole function.

There are no calling conventions for jumps between sub-functions, for the compiler
this behaves just like a regular jump, except that the base register \texttt{r30} must
be updated if the sub-function contains calls.


% TODO: examples

\stefan{We need to define how the call and local branch with cache fill get the size of the code to load. A word
containing the size at the target address, or prior to the target address? (including or excluding the size word?)}

\jack{I'd store the code size at the target address and have the
method itself start at an offset +4 or +8 from that place. I also
suggest storing the stack size here too. Could fit into the same
32-bit word (since I'd guess the local memory size limits us to
$<64$k code and $<64$k stack in a single method anyway).}

\martin{Agree for plain C code where there is no data structure
further describing a function. In the JOP JVM I have the luxury
to have those sizes as part of the virtual method dispatch table.
One indirection is there needed anyway, so why not having those
data there.}

\section{Stack Layout}

All stack data in the global memory, either managed by the stack cache or using
a frame/stack pointer, grows from top-to-bottom. The use of a frame pointer is
optional.

Unwinding of the call stack is done on the stack-cache managed stack frame,
following the conventions declared in the previous subsection on function calls.

\section{Interrupts and Context Switching}

Interrupt handlers may use the shadow stack pointer \texttt{r29} to spill registers
to the shadow stack. Interrupts must ensure that all special registers that might be in use
when the interrupt occurs are saved and restored.

Here is a simple example of storing and restoring the context for context switching.
\begin{verbatim}
sub $r29 = $r29, 40
sws [$r29 + 0] = $r31
sws [$r29 + 1] = $r30
sws [$r29 + 2] = $r22  // free some registers
sws [$r29 + 3] = $r23
mfs $r22 = $s2  // by now any mul should be finished
mfs $r23 = $s3
sws [$r29 + 4] = $r22
sws [$r29 + 5] = $r23
mfs $r22 = $r5  // read out cache pointers, spill
mfs $r23 = $s6
sub $r22 = $r23, $r22
sspill $r22   // spill the memory, s5 == s6 now
sws [$r29 + 6] = $r22  // store the stack pointer
sws [$r29 + 7] = $r23  // store stack size
...
// TODO store return base and offset

// restore
lws $r23 = [$r29 + 7]
lws $r22 = [$r29 + 6]
mts $s5 = $r22   // restore the stack
mts $s6 = $r22
sens $r23
lws $r23 = [$r29 + 5]
lws $r22 = [$r29 + 4]
mts $s2 = $r22
mts $s3 = $r23
....
\end{verbatim}

\stefan{We need to define how to register interrupts, how to return from interrupts (where do we get
the return-base and return-offset from,..). Interrupt handlers in C are currently not supported,
the compiler does not restore all the special registers for normal functions.}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\chapter{Potential Extensions}

\section{Decoupled Loads / Multiply / Wait / Move from Special}

\begin{itemize}
  \item Attaching a ready flag with all special registers.
  \item Specify destination special register with all decoupled operations; the
        operation sets/resets the ready flag accordingly.
  \item Wait operates on ready flags of special registers.
  \item Merged variant of Wait + Move from Special
  \item Wait with 16-bit mask to wait for multiple outstanding results.
\end{itemize}

This would be nice since it would allow to reload all special registers from
memory without going through the general purpose registers. It would be a
unified interface for decoupled operations and give more freedom to handle
parallel decoupled operations (pipelined multiplies, loads). We could apply this
also to the general purpose registers instead of the special registers.

\stefan{I think it makes more sense to use the ready flags with the general purpose registers. Then we can simply replace
all blocking and decoupled loads with special move by one unified load to a general purpose register. This however means
that all operations can potentially stall.}

\stefan{Regarding having additional write ports for the register file: We could still disallow multiple parallel loads by stalling a load
instruction until the last memory op is completed (else we would also need to take some special care of stores if loads and stores can be in progress at the same
time), this would not be any worse than the current ISA (currently we cannot have multiple loads to main memory in parallel). If a load is
ready, I guess we could just stall the pipeline for one cycle to write to the register. Again, this may still be better than the
current ISA, since we do not need the code space for the additional move from special instruction, but it may cost one stall whereas the
move-from-special could be scheduled in the second slot parallel to some other operation (ok, but then its not so RISC anymore..).}

\section{Bypass load checks data cache}

Let the bypass load use the data cache if the data is cached. If the data is not in the cache, load it from main memory, but do not update
the data cache (in contrast to the normal load). Therefore the compiler could use bypass to load data that will not be used a second time or
that might have a negative impact on the cache analysis, but we still take advantage of the cache if the data is already in the cache.

\stefan{For now the bypass is defined as load always from memory, even if it is in the cache, but there is no real disadvantage to checking the
cache first (except that bypass cannot be used to achieve a perfectly stable timing). But should a bypass update the LRU if it is a cache
hit? (probably not, it should not modify the cache state).

On the other hand, for synchronization we need a bypass that \emph{always} checks main memory. For consistency reasons, we may want to
update the data cache if it contains the accessed address.}

\section{Merged Stack Cache Operations and Function Return}

This might require an additional special-purpose register(?) to track the size
of the last reserve instruction (this register might also be set explicitly).
However, it would might reduce the number of ensure instructions needed.

Another option would be to merge the return and stack free operations. Both
instructions belong to the same function and, due to the simpler semantics of
the free, the combination would be easier to implement.

\stefan{I doubt that merging those functions would actually gain something. It reduces the flexibility for the compiler to perform ensure
only where needed, may limit the possibilities for passing data over the stack cache, and requires additional
code to restore the special registers that are required to track the size of the stack. Only gain I see is when the word in front of the
method code that stores the size of the method is used to store the stack size as 16bit value, and limit the size of the method to a 16bit
value. Then no additional data needs to be transferred. The stack control instructions would still be required to allow the compiler to
allocate stack e.g. only in certain contexts.}

\martin{If stack operations only happens at function call
and return this merge would make sense. It makes also
very clear that M$ memory access and S$ memory access are coordinated.}

\section{Non-Blocking Stack Control Instructions}

Currently, all stack control operations, except \texttt{sfree}, are blocking. It
might be useful to define non-blocking variants or define them to be
non-blocking in all cases.

It is questionable whether this would actually buy us anything. Most
\texttt{sres} instructions will be followed by a store to the stack cache
(spill of saved registers). It might be more profitable for \texttt{sens}
instructions.

\section{Purge Cache Content}
\begin{itemize}
  \item dynamic code generation, self-modifying code, etc.
  \item help cache analysis?
\end{itemize}

\stefan{Might not gain much: for LRU caches there is no need to purge, we might need to purge the stack cache on context switches anyway,
and for the method cache, purging the cache might also not gain much (depending on the used analysis). I do not think that Self-modifying
should be used in hard real-time code anyway, as it might introduce some slight problems to the WCET analysis ;) }

\section{Freeze Cache Content}

\begin{itemize}
  \item Bypass load can be used to avoid cache updates, but not on per-context basis (we cannot lock the cache and then call any function
  and assume the function does not update the cache. Instead we would need to generate function variants that only use bypass loads).
  \item Method cache freeze? Or should we just use a I-SPM for this if we want instruction cache locking?
\end{itemize}

\section{Unified Memory Access}

\stefan{All our memories (except the stack cache) use a unified address space, but due to the typed loads, references must include both address and
type of the cache. Since the type is encoded in the code, any generated code can only use one type of cache or SPM.
If we have a function with $m$ arguments and $n$ different caches or ways of accessing
memory, in the worst case we would need $m^n$ copies of that function to handle all cases (increases instruction cache costs!). We also need to expose this to the programmer,
either through having different names for the functions (very ugly and annoying to use, remember that this is transitive, you need to copy
your whole libraries!), or by having a type system on top of the C types, which means implementing some sort of overloading in C and
auto-generating variants of functions.

Note: we need typed loads to tell the processor which cache (not) to use. We also need a shadow stack not only due to typed loads,
but also due to the write-back policy of the stack cache (would make consistency a night-mare if we would allow to access stack-cache
allocated data over the data cache!).
}
\martin{we discussed that issue in The Vienna mini workshop and should continue to discuss it.}

Instead of having typed loads per cache or SPM, maybe have types per ``use-case scenario'', use local memory based on address
\begin{itemize}
\item Type for stack access (guaranteed hit, can be used in both slots)
\item Type for guaranteed hit (any local SPM access, access to data cache must be always hit, else undefined result)
\item Type for unknown data access (access SPM or data cache, or main memory and update data-cache)
\item Type for bypass (access SPM or main memory, do not allocate in data cache)
\item Maybe a type for no-allocate (access SPM, data cache or main memory, but do not allocate data in cache; could be useful to prevent
single loads from thrashing the cache), or use some sort of cache-lock instruction instead (could be useful to prevent a code sequence or function call from
thrashing the cache)
\end{itemize}

\stefan{
The idea behind the typed load is 1.~it makes analysis simpler by having guaranteed hit instructions, 2.~it makes the hardware
simpler and faster (I assume), no need to read the address to determine the local memory to use.
However, typed loads shift the analysis effort to the compiler, but while the analysis may make over-approximations about the accessed
memory, the compiler must always know the correct memory type, which requires either a precise analysis or help from the programmer by
additional annotations. An analysis can also be context-sensitive, but it is not possible for the compiler to generate ``context-sensitive
typed loads''. A better way for 1.~could be to generate annotations for memory accesses by the compiler instead of putting the
information into the code, which then can be context-sensitive and imprecise and allows for better code reuse, although this (mostly) eliminates the advantages of 2.
Might be interesting to compare those two approaches.
}

\stefan{Note: if we use a DMA controller, we do not need separate memcpy functions depending on the source and destination memory, since the
DMA controller can determine source and target based on the addresses (hopefully).
}

\section{Memory Management Unit}
\begin{itemize}
  \item Simple software managed TLB.
  \item Main idea is to provide protection and separation.
  \item Could be used for memory testing.
  \item No traps. Instead, reset or kill thread?
\end{itemize}

\section{Supervisor Mode}
\label{sec:supervisor}
\begin{itemize}
  \item To restrict reconfiguration of critical components (e.g., TLB, Interrupt
        Tables).
  \item Transfer to supervisor mode, e.g., via \texttt{syscall}.
  \item Might be handy when running multiple threads or an OS.
\end{itemize}

\section{DMA Interface}
\begin{itemize}
  \item Transfers between local and global memory
  \item Through special registers
  \item Alternatively, dedicated instructions \texttt{dmastart} and \texttt{dmalen}
\end{itemize}

\begin{bytefield}{32}
\bitheader{0-31}\\
\bitbox{1}{x} & \bitbox{5}{01010} &
\bitbox{4}{Pred} & \bitbox{5}{Type} & \bitbox{5}{Ra} & \bitbox{5}{Rs} &
\bitbox{7}{} \\
\end{bytefield}

\begin{tabular}{lll}
Type  & Mnemonic & Operation \\ \hline
00001 & dma.sp   & Start memory copy \\
      &          & which contain the word \\
xx001 & st.sc    & Stack cache, no write through to \\
      &          & main memory, never wait \\
xx010 & st.sp    & Scratchpad, no write through to \\
\end{tabular}

\stefan{If the I-SPM and D-SPM has a separate connection to the NoC, we may
also want a separate wait that stalls until the transfer is complete, independent of other memory transfers (load and stores using the data cache).}

\jack{Very useful to have both a blocking and a non-blocking
version of this instruction. The blocking version is probably
more important, though, so if it has to be one or the other,
then blocking should be it.

I-SPM should not really be a concern right now. I'd see this as
a store for permanently-resident code, therefore not requiring DMA.

What does worry me here is the number of DMA routes that might be
possible. I think we need to carefully consider what DMA operations
are really needed. There are three sources/destinations of interest:
stack cache (sc), local memory (lm) and global shared memory (gm),
and they're all effectively in different memory spaces.

I think we want to be able to transfer data from either
lm or sc, to gm. And vice-versa. This gives four DMA routes:
gm~$\rightarrow~$lm, gm~$\rightarrow~$sc, lm~$\rightarrow~$gm,
sc~$\rightarrow~$gm.

I would suggest that there should be a 2-bit field to say which DMA route
should be used and a 1-bit field to say if the operation should block
or not.}

\stefan{The stack cache spill and fill ops are performed by dedicated instructions, so we do not need to support this in the ISA (but it
will probably be handled by the DMA in the background). Do we also handle method cache fill by the DMA (probably yes)? Prefetching into the data cache
(probably not)?

Accessing main memory over the data cache or with bypass will basically access words in more or less random locations. Stack cache fill and
spill, method cache fill, and DMA transfers from main memory to scratchpad and back perform burst writes. Communication between scratchpads
may be somewhere in between? Maybe we could have two connections to the NoC, or two NoCs, with different package sizes,.. to support the
different transfer scenarios more efficiently. However, the bottleneck will
then be the SDRAM controller.}

\martin{I see also (at least ;-) two NoCs: one for main memory related transfers and one for
message passing communication between SPMs. And those have kind of different DMAs.
The SPM message passing has a real DMA -- I mean one that is programmed by the
CPU like: start from this address and load so many words. The other (caches) memory
transfer is also done by a small state machine - be it cache line fill, method cache several
block fill, or stack cache range fill. However, this bursty transfer is more implicit and
I would not call it DMA.

However, this might just be a naming convention issue. Maybe we are playing
with architectures and naming conventions in a spectrum. E.g. instruction
memory: plain I\$ -- I\$ with prefetch instructions -- method cache -- SPM with
pointer redirect (address translation) -- plain SPM in its own address space.}

\section{Data scratchpad}

\begin{itemize}
  \item Every core has its D-SPM with its own address range (all in the same address space), a core can write to the D-SPM of another core
  by writing to an address of the SPM of that core.
  \item Maybe have some sort of protection mechanism, to prevent cores from writing to any address in any remote SPM
  \item How do we handle writes to the same address by (remote) dma transfers and local writes (this might prevent local load and stores to
  the SPM from completing in a single cycle)?  \jack{This sounds
    like undefined behavior. The program should avoid doing this.
    But local loads/stores must take priority over remote loads/stores
    to SPM (including locally-initiated DMA transfers) because otherwise
    we will not have single-cycle access to SPM. There must be fixed-priority
    arbitration giving the CPU top priority.}
\end{itemize}

\stefan{The compiler needs to generate typed loads for SPM access. How do we tell the compiler when to use SPM access, without causing too
much pain to the programmer (we would need to annotate all variables, including function arguments and pointers to SPM allocated data).
Same problem here as with stack cache and second data cache due to typed access: Passing data to a function requires the function to know in which memory the
data is stored. We need specialized functions for
memory types of parameters, in all occurring combinations, this increases code size and method cache misses.}

\martin{I see the D-SPM mainly as buffer for core-to-core message passing.
DMAs transfer data between the NoC and the D-SPM on one memory port
and the CPU accesses it on the other (we are assuming two ports that can
change the direction, which was (is?) not available on all FPGA families).

Part of this D-SPM can also be used just to hold core local data. We don't
need to have an extra one.

Write access to the same address from two sides is simply undefined.
Why shall we provide HW for that? It is a programming error anyway.}

\section{Halt}
\fb{Might not be needed for now. In the simulator we could simply jump to a
label \texttt{exit} or \texttt{halt} or do an explicit syscall to quit the
simulation.}

\section{Floating-Point Instructions}

\section{Prefetching}

\begin{itemize}
  \item For method cache
  \item For local memory
\end{itemize}

\stefan{Prefetching with Method cache with FIFO is tricky, maybe we can use an I-SPM to store code that is executed while we prefetch the
method cache.}

\section{Data Caches}

\begin{itemize}
  \item Add a second (possibly larger), simple (direct mapped,..) data cache, to be used when the pointer address is known at compile time
    (i.e., a load does not destroy the whole cache state in the analysis), for array operations, ..
  \item We would need additional types in the load operation for that cache, but there are only two unused types left. Either use only
    blocking (?) loads and only word and byte (?) access, or replace some lesser used types, or even introduce a new opcode somehow..
\end{itemize}

\stefan{There were some plans about introducing two separate data caches, one direct mapped and a small fully associative (B1.1.6
in the final DoW), with the intention to avoid having loads from unknown addresses destroy the direct mapped cache state in the analysis.
Therefore we might want to reserve some types for loads that use a second data cache (if we do not use write-allocation for store, we do not need to make
this distinction for the store operation).}

\stefan{We also need to define how the store operation performs the data cache updates. I would suggest/prefer to merge the \texttt{gm} and
\texttt{dc} store types, use write-through (to avoid hard-to-analyze write-back costs at some random loads) without write-allocation (to
avoid problems with byte and half-word stores and unnecessary cache conflicts). A store should update all data caches that contain the data.
Then a load immediately after a store to the same address does not need to wait for the store write-through to complete if the load is a cache hit.}

\stefan{Actually, a second data cache with typed loads would be tricky for the compiler. We cannot simply pass a pointer to a constant to a
function that takes any kind of data. if the constant is in a different cache the function does not know this and loads the data also into
the first cache, making the second cache very inefficient.}

\martin{We can overdue it by adding some more data caches: one for
static data (as mentioned before where addresses are known) and one
for constants. The difference is cache coherency, which is not needed
for constants. But are we considering cache coherence?}

\section{Instruction scratchpad}

\begin{itemize}
\item For instruction handlers or other code that should not destroy the method cache
\item Could be used to store code that is executed at a call site (even if the method cache entry of the caller gets replaced)
\item Replacement of code at runtime, statically scheduled or with some sort of software-controlled replacement strategy (maybe this could
be used to prevent threads from destroying the I-cache of real-time tasks)
\item Keep frequently used code on the I-SPM, can be used to do some sort of cache locking (instead of somehow locking the method cache).
\end{itemize}

\jack{I suggest that an I-SPM should be used only to store
permanently resident code. Interrupt handlers, scheduler,
that sort of thing.  Otherwise we need a more complex DMA
controller (more ports) and we need to explain why the I-SPM is
sometimes better than using the method cache. This is a bit tricky
because while dynamic (run-time replacement) I-SPM allocation strategies
do have the advantage of being able to load parts of methods, or pack
multiple methods into a single load, and thus get better performance
in some cases, you can get the same behavior by manipulating the
control flow graph (inlining/exlining/combining methods) and using
a method cache. So it is hard to make that argument in a report.}


\section{Wired-AND/OR for predicates}

Let \texttt{cmp} be some operation that sets a predicate \texttt{Pd} and is predicated by \texttt{Pred}.
Then we could define the following variants:
\begin{verbatim}
cond:   if (Pred)           Pd = <cmp>
and:    if (Pred && !<cmp>) Pd = False
or:     if (Pred &&  <cmp>) Pd = True
uncond: Pd = Pred && <cmp>
\end{verbatim}
Note that we do not need to read \texttt{Pd}, but the last variant uses \texttt{Pred} as input, not as write-enable signal. First variant is
the normal (conditional) execution. The last variant forces \texttt{Pd} to \texttt{false} if \texttt{Pred} is false, thus saving the
initialization of \texttt{Pd} or explicit \texttt{and} with \texttt{Pred} for code like
\begin{verbatim}
if (p1)  p2 = R1 < R2
if (p2)  ...
\end{verbatim}
The other variants can be used to implement stuff like
\begin{verbatim}
if (a != 0 && b < 5) { .. }

p1  = cmpnez r1,    addi r3 = r0 + 5;
p1 &= cmplt r2, r3
\end{verbatim}
To implement \texttt{a != 0 \&\& b > 1} we would need an additional bit that negates either the result of \texttt{<cmp>} or the value that is
assigned to \texttt{Pd} (including the \texttt{true} and \texttt{false} assignments).

Note that if we do not need \texttt{Pred}, we can do \texttt{and} and \texttt{or} simply as
\begin{verbatim}
Pd &= <cmp>  ... if ( Pd)  Pd = <cmp>
Pd |= <cmp>  ... if (!Pd)  Pd = <cmp>
\end{verbatim}
i.e., we use \texttt{Pd} as \texttt{Pred}.


\section{Deadline instruction}

\begin{bytefield}{32}
\bitheader{0-31}\\
\bitbox{1}{x} & \bitbox{5}{01011} &
\bitbox{4}{Pred} &
\bitbox{22}{Counter} \\
\end{bytefield}

Wait until the deadline counter reaches zero, then restart the counter with the given initial value.

\stefan{Do we want this? This is similar to the PRET deadline instruction. We could use this to fix the execution time
of some CFG region, e.g., if we know that in a sequence of load instructions at most one load will be a cache miss, but we do
not know which one, then we can set a deadline to that sequence that allows for one cache miss, and get stable timings for the
program without forcing every memory access that we cannot guarantee to be always-hit to be a cache-miss. We could also use
that instruction to verify the WCET results, by raising an exception if the counter already reached zero when the deadline instruction
is issued. However, we have no exception-handling in our ISA so far..}

\cullmann{For timing analysis, I see no benefit in having the deadline instruction.
Our aiT tool-chain copes well with block with variable durations and can use that to get actually a better WCET bound. (e.g. if for some calling contexts a block is fast, this will be exploited in the analysis)
On the other side, if a deadline instruction is around, we would need to keep track of the current active deadline counter which will increase the complexity of the analysis.
(if I understand it right, that you can at one time start such a deadline and later at an other point wait for it)
}
\stefan{The deadline instruction is basically a tool to make the \emph{measured} execution time more stable. The idea here is to set the
deadline counter based on the WCET analysis, not the other way round (we can set the counter to zero for analysis, so no deadline
instruction will ever wait and the analysis can ignore the instructions, and then use the analysis results to set the counters in the final
code).}
\stefan{Actually, there are only two real use-cases for a deadline instruction: test WCET analysis results (by throwing an exception if
deadline already expired when the instruction is executed, but this requires exception handling in our ISA first!), and making the timing of
\emph{output} produced by the program stable (by waiting on the deadline directly before producing some output; This allows to generate some
periodic signals without having some timer hardware or timer interrupts, i.e., without interrupting the control-flow, thus really bringing
the notion of time into the ISA!).

There is a third use-case: we can fix the execution time of basic-blocks or some code regions. This could be used to make the WCET
of the program more composable by reducing problems with timing anomalies, and to avoid cases where the processor state depends on
the timing of some previous instructions. However, I \emph{strongly} suggest to formally check our ISA and our code generator to be
free of timing anomalies,.., so that we avoid this use-case by design. We want to be able to calculate the WCET for some sub-CFG from the
WCET of the basic-blocks as easily as possible, so that we can keep track of the WCET/WCEP in the compiler when doing code
transformations.}

\chapter{Conclusion}
\label{sec:conclusion}

Patmos is the next cool thing in the dry world of real-time systems.

\bibliographystyle{abbrv}
\bibliography{patbib.bib}

\end{document}

\appendix


\chapter{Archived Discussions}

\begin{itemize}

\item ALU immediate (add, or, and, xor), the function is indicated with \emph{ff}: \\

\begin{bytefield}{32}
\bitheader{0-31}\\
\bitbox{1}{x} & \bitbox{3}{000} & \bitbox{2}{ff} &
\bitbox{4}{Pred} & \bitbox{5}{Rd} & \bitbox{5}{Rs1} &
\bitbox{12}{Constant}\end{bytefield}\\

\begin{tabular}{lll}
ff & Name & Semantics \\ \hline
00 & add & Rd = Rs1 + $\sconst$ \\
01 & or  & Rd = Rs1 \OR $\sconst$ \\
10 & and & Rd = Rs1 \AND $\sconst$ \\
11 & xor & Rd = Rs1 \XOR $\sconst$
\end{tabular}

\stefan{Is Constant sign extended? We have the long immediate ALU instruction anyway, so we do not
need sign extension here, but then we cannot use this short form for subtraction.}
\martin{Yes, it is sign extension.}
\cullmann{Question: for the add, sign extension makes sense, but for the logic stuff, I see no real reason to sign extend, beside that you then only can use 1 bit less of the constant.}
\fb{Might be a good idea to spend some more bits on \texttt{ff} here -- to merge
    the \emph{ALU very short immediate} format introduced by Stefan (except for
    \texttt{rl}). Merge with \texttt{ldi}. \texttt{xor} might not be that
    relevant here, we could replace it with \texttt{sub} and then consider the
    constant to be unsigned.}


\item ALU function: \\

\begin{bytefield}{32}
\bitheader{0-31}\\
\bitbox{1}{x} & \bitbox{5}{00101} &
\bitbox{4}{Pred} & \bitbox{5}{Rd} & \bitbox{5}{Rs1} & \bitbox{5}{Rs2} &
\bitbox{7}{0~\textbar~Func}\end{bytefield}\\

\begin{tabular}{lll}
Function & Name & Semantics \\ \hline
0000000 & add  & Rd = Rs1 + Rs2 \\
0000001 & or   & Rd = Rs1 \OR Rs2 \\
0000010 & and  & Rd = Rs1 \AND Rs2 \\
0000011 & xor  & Rd = Rs1 \XOR Rs2 \\
0000100 & sub  & Rd = Rs1 - Rs2 \\
0000110 & nor  & Rd = \NOT (Rs1 \OR Rs2) \\

0001000 & sl   & Rd = Rs1 \shl Rs2 \\
0001001 & sr   & Rd = Rs1 \shr Rs2 \\
0001010 & sra  & Rd = Rs1 \ashr Rs2 \\
0001011 & rl   & Rd = (Rs1 \shl Rs2) \OR \\
        &      & \phantom{Rd = }(Rs1 \shr (32-Rs2)) \\

0010000 & carr & Rd = ((uint64\_t)Rs1 + \\
        &      & \phantom{Rd = }(uint64\_t)Rs2) \shr 32 \\
0010001 & borr & Rd = ((uint64\_t)Rs1 - \\
        &      & \phantom{Rd = }(uint64\_t)Rs2) \shr 32 \\

??????? & shadd  & Rd = Rs1 + Rs2 \shl 1 \\
??????? & shadd2 & Rd = Rs1 + Rs2 \shl 2 \\
\end{tabular}

\fb{What is the semantic of shifts when \texttt{R2} \textgreater~32 or
    \texttt{R2} \textless~0? I would suggest to consider the lower 5 bits only.}

\stefan{Wolfgang will probably recognize a lot of this.. I shamelessly copied most of the arithmetic
and condition stuff from the Lemberg ISA, so that Martin can copy the implementation and I can copy the compiler ;) }
\martin{Is an ISA under copyright ;-) I'm not sure if I really want all of that, but lets see...}

\stefan{Do we want to have compare instructions that store the result in registers? We could use the same Function code as
for the compare-to-predicate instruction, including compare with immediate value..}
\martin{Compare instructions set a predicate.}

\cullmann{Can't we just allow all ALU functions that take 2 arguments in both ways? e.g. one with R and imm, one with R1 and R2. Would make the ISA for ALU instructions more regular.}


\item Compare (constant could be Rs2 field). It is predicated and has a
destination predicate field: \\

\begin{bytefield}{32}
\bitheader{0-31}\\
\bitbox{1}{x} & \bitbox{5}{01000} &
\bitbox{4}{Pred} & \bitbox{2}{\bitsunused} & \bitbox{3}{Pd} & \bitbox{5}{Rs1} & \bitbox{5}{Rs2} &
\bitbox{7}{Function}\end{bytefield}\\

\begin{tabular}{lll}
Function & Name & Semantics \\ \hline
??????? & cmpz  & Pd = Rs1 ==  0 \\
??????? & cmpnz  & Pd = Rs1 !=  0 \\
0100000 & cmpeq  & Pd = Rs1 ==  Rs2 \\
0100001 & cmpne  & Pd = Rs1 != Rs2 \\
0100010 & cmplt  & Pd = Rs1 \textless Rs2 \\
0100011 & cmple  & Pd = Rs1 \textless= Rs2 \\
0100100 & cmpult & Pd = Rs1 \textless Rs2, unsigned \\
0100101 & cmpule & Pd = Rs1 \textless= Rs2, unsigned \\
0100110 & btest  & Pd = (Rs1 \AND (1 \shl Rs2)) != 0 \\
\end{tabular}

If the MSB in the Function field is set, use Rs2 as immediate value.

\stefan{We could use bits 3-4 to define wired-AND/OR combination operation, like

\begin{tabular}{lll}
Function & Name & Semantics \\ \hline
0100xxx & uncond & Pd = (Rs1 op Rs2) \\
0101xxx & and    & Pd \AND= (Rs1 op Rs2) \\
0110xxx & or     & Pd \OR= (Rs1 op Rs2) \\
\end{tabular}
}

\daniel{The idea of this was that one can compute a predicate for an expression like
the following by implementing the definition type with wired semantics,
i.e. with wired-AND, 0 is only written to Pd if (Rs1 op Rs2) evaluates to false,
whereas if it evaluates to true, Pd remains untouched by the operation.
}
\begin{verbatim}
  if (a<b && b<c) { ..

the boolean expr becomes ($p1 needs to be
initialized somewhere previously)

  cmplt_AND $p1, a, b
  cmplt_AND $p1, b, c
  ;;
\end{verbatim}

\daniel{Likewise with OR that does not write 0 but only 1 if the comparison evals to true.\\
To make it complete, we'd need also complement variants, i.e. a NAND that
only writes 0 to the Pd if the comparison evaluates to true (likewise NOR).\\
Maybe we could abuse bit 20 or 21 for this? Could make cmpne superfluous.
}
\fb{The \texttt{\&=} and \texttt{\textbar=} variants would require an additional
    register port for the predicate register file -- not sure it the overhead is
    negligible.}

\daniel{For single-path code generation (and if-conversion in general), an additional variant for predicate definition
(\texttt{cmpXX}) is useful. The standard semantics is \texttt{if Pred then Pd := Rs1 op Rs2}. In a block guarded by Pred,
if we compute predicates for an if-statement, the predicate \texttt{Pd} for the then-block would be \texttt{Pred $\wedge$ (Rs1 op Rs2)} and for the
else-block \texttt{Pred $\wedge$ $\neg$(Rs1 op Rs2)}. Note that with the semantics above, \texttt{Pd} needs to be initialised to false.
Directly $\wedge$-ing the Pred and comparison result to define Pd could avoid explicit initialisation.
Ideally, the compare instruction would accept two \texttt{Pd}s, one for $\wedge$-ing Pred with the comparison result and one for $\wedge$-ing Pred
with the negation of the comparison result, but I guess this is infeasible to encode and implement in hardware,
and we'll need to use predicate combination for that purpose.
}

\stefan{
In case we run out of opcodes, we could consider to use only 3 bits for \texttt{Pred}, allow only \texttt{if (Pd)} predicate, use the
bit for opcode or \texttt{Func}/\texttt{Offset} instead, and use either bit 20 or bit 3 to negate the result of the comparison, i.e.,
do \texttt{Pd = !(Rs1 op Rs2)} if that bit is set, instead of \texttt{cmpnez} and \texttt{cmpne}.}


\item Multicycle NOP: \\

\begin{bytefield}{32}
\bitheader{0-31}\\
\bitbox{1}{x} & \bitbox{5}{01010} &
\bitbox{4}{Pred} & \bitbox{5}{\bitsunused} & \bitbox{5}{\bitsunused} & \bitbox{5}{count} &
\bitbox{7}{\bitsunused} \end{bytefield}\\

\stefan{Predicated multicycle nop? Creepy.. However, it might actually make sense to
issue the nop together with another operation (like a branch), so we could do stuff like

  \begin{tabular}{ll}
    Pipeline A & Pipeline B \\
    \hline
    \texttt{if (c1) br addrA} & \texttt{if (c1) nop 5} \\
    \ldots & \\
    \texttt{if (c2) mul r1, r2} & \texttt{if (c2) nop 3} \\
    \texttt{if (c2) ldx r3, \$mul}
  \end{tabular}

Therefore it would be nice to be able to issue the nop in both pipelines.
}

\cullmann{For timing analysis, I see no benefit in specifying a time for the nop.}
\stefan{It should save a lot of code size though, since most instructions are not intended to stall or to detect hazards (branch, mul, call,
..).}


\item Wait instruction: \\

\begin{bytefield}{32}
\bitheader{0-31}\\
\bitbox{1}{x} & \bitbox{5}{01110} &
\bitbox{4}{Pred} &
\bitbox{22}{\bitsunused} \end{bytefield}\\

Wait for the current memory transaction to finish.

\cullmann{For our analysis and in practice, blocking loads would be in my eyes more interesting than explicit waits.
It is true that if you know that e.g. it will be a cache hit you might save to emit the wait, but with blocking loads, you will never need them.
}
\stefan{I agree that we do want to have blocking loads in our ISA. However, the advantage of split loads is that in case of a cache miss, we
can hide some other instructions, therefore I would argue that we would like to have both in the ISA (if starting a load should wait for the
previous memory transaction to finish is a different question though, since in case of a cache hit we would not need to wait for a store to
complete, but having to insert a wait between each store and load explicitly maybe hurts even more due to instruction cache miss costs..
should be evaluated). In any case, we still need a generic 'wait for memory' instruction, since we need it to
\begin{itemize}
\item wait for scratchpad load / store, instruction cache loads, ..
\item wait for all memory transactions to finish at the end of basic blocks /.. to make the timing of the next blocks independent from the
previous block (as much as possible), so that we can estimate the WCET of some code in the compiler during optimization more easily.. On the
other hand, we should just try to get our ISA free of timing anomalies.
\end{itemize}
}

\item Multiply: \\

\begin{bytefield}{32}
\bitheader{0-31}\\
\bitbox{1}{x} & \bitbox{5}{01101} &
\bitbox{4}{Pred} & \bitbox{5}{\bitsunused} & \bitbox{5}{Rs1} & \bitbox{5}{Rs2} &
\bitbox{7}{\bitsunused} \end{bytefield}\\

Start multiplication of Rs1 * Rs2. Result is stored to a special register after a fixed number of cycles.

\cullmann{For the analysis, it makes no difference, but for later use, would a blocking multiply not be more easy?}


\item Load with a memory type: \\

\begin{bytefield}{32}
\bitheader{0-31}\\
\bitbox{1}{x} & \bitbox{5}{01001} &
\bitbox{4}{Pred} & \bitbox{5}{Rd} & \bitbox{5}{Ra} &
\bitbox{5}{Type} & \bitbox{7}{Offset}\end{bytefield}\\

\medskip

\begin{tabular}{lll}
Type  & Mnemonic   & Operation \\ \hline
xx100 & ldm.bypass & Bypass caches, load from  \\
      &            & main memory to Rd, impl.\ wait \\
xx101 & ldm.dm     & Direct-mapped cache, load from \\
      &            & main memory, impl.\ wait on miss \\
xx110 & ldm.fa     & Fully-assoc.\ cache, load from \\
      &            & main memory, impl.\ wait on miss \\
xx010 & ldm.sc     & Stack cache, always hit (no wait) \\
xx001 & ldm.sp     & Scratchpad, always hit (no wait) \\
00xxx &            & Load word \\
01xxx &            & Load half-word \\
10xxx &            & Load byte \\
11100 & ldm.split  & Start Split-load from main-memory, \\
      &            & aligned, implicit wait \\
\end{tabular}

Address register Ra counts in bytes, Offset counts in natural word length (4 bytes for a word).

ldm.sp can be issued in both pipelines, the others only in the first pipeline.

\daniel{If offset is in word length, what do load word/half-word/byte mean?
Only from starting at byte 0?  MIPS offers st/ld for word, half-word, byte as
well, with byte offset addressing.
I see two solutions: offset in bytes (range -64 to 63 as +/-16 words, ouch), or,
we could, to be consistent with the \texttt{ldx} instruction, offer an extract
instruction that extracts half-word/byte from a register with an offset. Not as
much code bloat as shift/and.  }
\daniel{Okay, one can use byte offsets from Ra, but this somehow destroys the concept of base address\ldots}




\item Store with a memory type: \\

\begin{bytefield}{32}
\bitheader{0-31}\\
\bitbox{1}{x} & \bitbox{5}{01010} &
\bitbox{4}{Pred} & \bitbox{5}{Type} & \bitbox{5}{Ra} & \bitbox{5}{Rs} &
\bitbox{7}{Offset} \end{bytefield}\\

Address register Ra counts in bytes, Offset counts in natural word length (4 bytes for a word).

\begin{tabular}{lll}
Type  & Mnemonic & Operation \\ \hline
xx111 & st.mem   & Store to memory, update caches \\
      &          & which contain the word \\
xx001 & st.sc    & Stack cache, no write through to \\
      &          & main memory, never wait \\
xx010 & st.sp    & Scratchpad, no write through to \\
      &          & main memory, never wait \\
00xxx &          & Store word \\
01xxx &          & Store half-word \\
10xxx &          & Store byte
\end{tabular}

\texttt{st.mem} always writes through to memory and stalls at the start of the operation if the memory bus is not ready.
Storing is performed in two stages:
\begin{enumerate}
\item cache update; within one cycle (or a fixed number of cycles)
\item write to memory
\end{enumerate}
For \texttt{st.sp, st.sc} stage 2 is omitted -- hence no interaction with memory bus, never has to wait.  \texttt{st.sc}
can be issued in both pipelines.

\cullmann{
Would loads that implicitly wait not make the life easier for the compiler? For the analysis, in any way, we will need to keep track of the load to implement wait.
For the "good" cases which hit the cache or SPM, an implicit waiting load would have no penalty, but for the "normal" cached main memory access, you would only need one instruction and not up to three (load, wait, move result).
Beside, it will avoid possible semantic errors which are hard to detect. e.g. you can miss to insert a wait and it will work ok as long as you hit the cache but if by accident, you miss it, you get wrong behavior.
To create such timing sensitive code would be impossible with stalling loads.
Beside, I see no benefit in performance and predictability with the split load approach. With blocking loads, you will save instructions, which should increase performance, and for the analysis, we need to implement the waiting anyway.
}

\daniel{We have reworked the load instructions to offer both a stalling and a split load. We can use a split load whenever we know we will miss the cache.}

%\item xxx: \\
%
%\begin{bytefield}{32}
%\bitheader{0-31}\\
%\bitbox{1}{x} & \bitbox{5}{00---} &
%\bitbox{4}{Pred} & \bitbox{5}{Rd} & \bitbox{5}{Rs1} & \bitbox{5}{Rs2} &
%\bitbox{7}{Function}%\end{bytefield}\\

\end{itemize}

\bigskip

\todo{The above should be generated from a common definition of the ISA (which is used by all parts of the project).}

Some more instructions to be defined: branch and link, load method cache, interrupt handling -- we will support multithreading, therefore the full processor context needs to be save- and restorable,...

\martin{Do we need exceptions? The only two I can think of are null pointer and divide by zero. Is it worth implementing (exact) exceptions for this or do we just leave it to the compiler?}

\wolf{I do not think that we need exceptions besides null pointer and division by zero (and even those could be raised explicitly). But we have to think about the handling of interrupts: What do we do if we get an interrupt between issuing a memory load or the start of a multiplication and retrieving the result? Disable interrupts in between? Make result registers writable so we can restore the original state?}

\wolf{We also need to think about the handling of exceptions on the higher-language level. Can we support Java-style exceptions efficiently?}


\cullmann{
For aiT, we definitely want to have call/return instructions. (they should perhaps even do stuff like save/restore the registers which need to be restored/saved)
For avionics software, which uses a lot of floating point computations, is any FPU in the todo?
General: If you lower the bits reserved for predicates, you could use more bits for immediate constants.
}


\section{Memory Accesses}

\wolf{There are (at least) two ways to split memory loads. Explicit wait and
load: \texttt{ldm addr; wait; dest = ldx \$mem}, or implicit wait:
\texttt{ldm addr; dest = ldx \$mem}. Depending on the available units,
the former may allow pipelined memory accesses:

\medskip
\begin{tabular}{ll}
  Pipeline A & Pipeline B \\
  \hline
  \texttt{ldm addrA} & \\
  \texttt{wait for rdycnt<=1} & \texttt{ldm addrB} \\
  \texttt{destA = ldx \$mem} & \\
  \texttt{ldm addrC} & \texttt{wait for rdycnt<=1} \\
                     & \texttt{destB = ldx \$mem} \\
  \texttt{wait for rdycnt<=1} & \\
  \texttt{destC = ldx \$mem} & \\
\end{tabular}

\medskip
Also, the \texttt{ldx} part of the loads can go through the ALU, which
\emph{might} reduce multiplexing in the forwarding logic (only the
ALUs ever produce results). The \texttt{wait} could maybe be omitted
for stack and scratchpad memory accesses.

On the other hand, implicit waiting clearly wins in terms of code size
and instructions to be executed (more free slots for other stuff).  I
think that implicit waiting is more profitable for Patmos, but
educated guesses (or facts\ldots) would be very welcome.}

\martin{I would argue that split loads are only for the main memory and
caches that \emph{might} miss. Access to stack cache and SPM should
be a guaranteed hit. BTW: normal memory loads can only be scheduled
in the first instruction slot -- we have only one l/s unit. However, Florian
convinced me that double ld/st to the stack can be beneficial for register
spill and fill. So the plan is to allow stack ld/st (we just need word here....)
in both slots.}

\wolf{Fast access to the stack is a good idea, the following only
  applies to ``normal'' memory accesses. Loading the results of all
  loads via the ALUs would be a cheap way to free the first slot for
  issuing loads/stores and jumps. There is no real need to put that
  into the memory unit (is there?). Do you assume that waiting is
  handled in the l/s unit? In my opinion, a separate stall unit that
  is accessible from both pipelines would be nice (forwarding does not
  get more complicated). With implicit waiting, three consecutive
  accesses could look like this:

  \medskip
  \begin{tabular}{ll}
    Pipeline A & Pipeline B \\
    \hline
    \texttt{ldm addrA} & \\
    \texttt{ldm addrB} & \texttt{destA = ldx \$mem} \\
    \texttt{ldm addrC} & \texttt{destB = ldx \$mem} \\
                       & \texttt{destC = ldx \$mem} \\
  \end{tabular}
}

\section{General Discussion}

\tommy{Martin (jokingly?) told me that Patmos aims to
beat YARI. I'm all for it as I've been trying to do the same for a long
time. However I've found it very difficult to beat a simple single-
issue RISC. Although I think I have the answer, it's a very
different path from Patmos and I'll not push it here.}

\martin{Partially a joke, partially I really would like to beat it ;-)
Serious, Patmos will be optimized for WCET and not average
case performance. On the other side it should no be too slow.}

\tommy{In YARI, fMAX (AKA 1/cycle-time) is strongly correlated to the
 width of the widest mux in any stage. Another way to say this,
 the more things a register depends on, the slower it gets.}

\martin{Yes, I'm a little bit afraid of the forwarding muxes from the
two pipelines. We have not yet implemented that part.

I know that Sven wants to pipeline the forwarding. This
costs one cycle latency and we don't know if this can be
compensated by clever instruction scheduling in the
compiler.}

\wolf{In my hobby design I use registers that do nothing but feed the
forwarding. This lets the synthesis place the registers freely and
makes forwarding less painful by reducing the interconnect delay. Of
course, if the synthesis is clever, it can duplicate registers to
achieve the same thing, but doing it explicitly helped at least in my
design.}

\tommy{Ways to improve this in order of effectiveness:

 1) Change the ISA

 2) Whenever possible, share sources for a register, fx. to copy a
    value consider adding it to zero rather than having a separate
    mux input, and, failing everything else,

 3) move logic up to an earlier stage (unfortunately this usually
    implies increasing latencies).}

 \tommy{A consequence of this fact was to not use interlocking in the
 pipeline at all, but to restart the pipeline whenever a hazard
 was violated. This way there's only one source for the
 instruction for every stage in the pipeline (along with a valid
 bit which is cleared by restart and checked only by logic that
 would commit state).}

\martin{We also plan to not use interlocking, but no pipeline restarting.
All instructions (except one) do not stall at all. (This is similar
to the JOP pipeline).}

\tommy{Replace my use of interlocking with stalling. The ability to
  stall the pipeline is expensive. Restarting the pipeline (and
  replaying the original instruction) achieves the same effect with
  less cycle-time impact at the cost of potentially higher resume
  latency compared to true stalling. (Powerwise, stalling is better).}

\martin{The intention of Patmos is to have no stalls at all -- with one
exception. So no stalling, no restart. The single exception is main
memory access. In that case we use a split load and an explicit wait
for the result. That wait is somehow \emph{pipeline stalling}. Perhaps
we do it the JOP way and fill the pipeline with wait instructions that
do nothing and only one stage needs to check the memory ready.
BTW: this is also the reason for the ready counter in SimpCon we
discussed some long time ago. It is used to restart the pipeline a
few cycles earlier so the load instruction is in EX stage when the actual
data arrives.}

 \tommy{One very real example I struggled with load latency. MIPS
 defines a one-cycle load latency. Violating this in YARI would
 cause a pipeline restart, originally a five cycle penalty.
 Unfortunately, the JIT we were using (CACAO) has a pretty
 naive code generator and even making it emit a nop for the
 load was non-trivial.}

 \martin{We have more freedom with the VLIW approach to
 expose latencies and the compiler should be smart ;-)}

\tommy{I tried quite a lot of alternatives:
 0. simply completing the load in one cycle (really hurt fMAX)
 1. moving the EX stage down to the 2nd half of the load (now
    the hazard is on loads whose address register isn't ready
    two cycles ahead of the load),
 2. A hybrid, where only 8- and 16-bit loads suffer the additional
   latency.}

\tommy{ Without measurements I couldn't predict which would win, but
 real-life results were counter-intuitive and in the end I got the
 best result with

 3. keeping it was, but reducing the hazard penalty to two
  cycles (IIRC).}

\tommy{After much mux optimization
  (cf.~http://portal.acm.org/citation.cfm?id=968291 and
  http://portal.acm.org/citation.cfm?id=1065692)
 the bypass network and register file ended up being the critical
 path and the only way to I know to improve that would be at
 the cost of more hazards (eg.~restart on unsupported forwards).}


\tommy{How does this apply to Patmos?
* Unlike YARI, Patmos doesn't implement an existing ISA, so to
 me an architectural simulator is even more critical. Developing
 the compiler against an evolving SystemC implementation
 seems somewhat wanting.}

\tommy{* Many design decision in the paper strikes me as somewhat
 premature in the absence of measurements, but I guess I
 shouldn't worry about that.}

\martin{You're right. However, we even would need WCET analysis instead
of measurements to judge the decisions. Even harder to do at this
stage. So we can only perform educated guesses.}

\tommy{* Assuming (optimistically) that the compiler can extract an avg. IPC
 of 1.6, Patmos would need an fMAX of 56 MHz just to be at parity
 with YARI and that's not accounting for the overhead of the two-
 instruction loads. I'm concerned that the bypass network may not
 be able to go that fast.}

\martin{Something to be evaluated....}


\section{More on load/store}

\tommy{Sign extended loads hurts FPGA implementations much more than real
silicon because muxes are very expensive here. I strongly recommend
making all loads zero extending and provide explicit sext8 and sext16
instructions. (This matches the RISC philosophy of exposing the real
cost of operations).

(Caution: The Alpha route of providing only word-level accesses proved to be a mistake (I/O
problems and more) and they had to provide byte and short level accesses
in the second version of the architecture.)}

\tommy{
Load and store offset of 7 bits is too small. Assuming it's signed
(right?) that's just [-64; 63]. Even if scaled by four for word
accesses that's still just [-256; 248]. Luckily, there are bits that can
be recovered}

\martin{to be explored}

\tommy{ Merge "ALU function" and "Compare" -> save one bit

- Load/Store to use natural scaling of offset.

 It's very rare in practice for the offset not to be naturally
 aligned, so we can save some redundant bits at the cost of more
 muxing in the address computation.}

 \martin{agree on both, although compare has an additional field for
 the predicate destination.}

\stefan{Instructions for atomic access like load-and-link or read-modify-write?}

\stefan{As far as I can see, we will have the following?
\begin{itemize}
\item load bypass: load directly from memory, do not modify cache.
\item stack-cache: direct mapped, explicit write-back, write allocate. Should we make loading to the stack cache also explicit?
  Then the stack-cache would actually become something like a stack-SPM.
\item D-caches: direct-mapped, write-through, for memory ops with known addresses, and a 'small, highly associative', write-through cache for
everything else. What about write allocation?
\item D-SPM: to be defined..
\end{itemize}

Stack-cache, SPM and instruction cache will probably always load and write-back larger blocks of data. Is there an advantage in having
instructions that load more than a single word (i.e., an instruction to start a memory transfer of n bytes from/to address A using cache X)
in terms of performance (e.g., by transferring larger packages than one word over the NoC..)? We will need something like this for the
method cache anyway. However, such an instruction will block the memory bus for quite some time in the worst-case, which may be problematic with
preemption..

Where do we want to wait?
\begin{itemize}
\item Access to a cache with guaranteed 'cache hit' by design (D-SPM, stack-cache): no need to wait, no split-load.
\item Explicit data load or write back (D-SPM, stack-cache, method cache, bypass, store): guaranteed memory transfer, needs to wait for previous
memory transfer to complete.
\item Load from D-cache that is always-hit: no need to wait for other memory transfers (method-cache preload, stack write-back, SPM),
no need for a split load. However, if the D-caches do not use write-allocation and if there is a store or write-back in progress, we still need to wait
for the store to complete if the compiler cannot prove that they go to different memory locations.
\item Load from D-cache that may be a cache-miss: since we are interested in WCET, we will probably just wait for any previous memory op to
complete.
\item Fetching the result of a split-load: If we do not know that the load is a guaranteed hit, we always need to wait before accessing
\$mem. If we know that the load is always-miss, there is no advantage in using a split-load.
\end{itemize}
}


\jack{Also we want to be able to wait for a DMA operation to complete~- I
see you already mentioned waiting for the {\it previous} DMA to finish,
but we probably also want to wait for the current DMA to finish,
e.g.~if the next instruction depends on something being loaded.}


\section{Stack Cache Discussion}

In order to allow two parallel load/store operations the memory of the stack
cache might be operated with a doubled clock frequency.

\fb{This might also allow to define a precise definition of the result of
    concurrent, overlapping stores on the stack cache.}
\martin{Double clocking of the S\$ is conditional and depending on
the critical path with the rather large multiplexer for the memory stage.}

\gebhard{Where is the benefit of the stack cache, if we additionally need to maintain a shadow stack in main memory?
This complicates a static stack/value analysis.
For C/C++ programs using pointers the compiler would often have to allocate local variables on the shadow stack.
Consider the following program:

\begin{tabular}{l}
\texttt{int main (void) \{} \\
\texttt{  int c;} \\
\texttt{  void compute (\&c);} \\
\texttt{\}} \\
\end{tabular}

Obviously, the variable \texttt{c} cannot be put onto the stack cache.
The function \texttt{compute} cannot know that its parameter references an object in the stack cache.
A similar problem would occur with 3rd party library routines that have pointer parameters.
In general, we thus cannot benefit from discerning between references on the stack and references on the heap on the instruction level.
This does actually not correspond to the C/C++ memory model.

Why don't we use a dedicated memory for the stack?
This could be a fast scratchpad memory that is allocated to a dedicated address range.
Similarly, we could implement the stack cache as a separate data cache that only caches words from a dedicated memory region.
As we cannot discern between types of references on the instruction level, we can do so by means of the target address of a memory access.}

\fb{We will find out ... Some things might be interesting though: stack accesses
through the cache do not pollute the data cache and do not interfere with the
analysis of the other caches in the system. All stack accesses can be predicted
nicely. The compiler can place the stack control instructions depending on the
WCET. Most C functions should be easily handled with the stack cache. It becomes
easier to judge how spill code generated during register allocation impacts the
WCET. .... }

\gebhard{This benefit of not polluting the data cache is also given in my design.
The stack scratchpad memory would be dedicated to a certain address range that is \emph{not} cached.
Hence, we would have the same benefit with less effort (i.e., in terms of instructions and processor logic).}

\cullmann{An additional problem is: the timing of a task relies then on the exact fill level of the stack cache at task start.
This causes additional interference of multiple tasks, if the stack cache is already filled to some amount, the new started task will need to evict the stuff to memory.
Or is the stack cache cleared before a new task is started to decouple the tasks again?}

\cullmann{What happens on preemptions or interrupts btw.? Is the stack then saved and restored somehow?}

\fb{This is actually a nice property of the stack cache. You can reestablish the
state of the stack cache -- as far as WCET analysis is concerned -- after a task
switch. You clear the cache with a \texttt{reserve} of the stack cache size, you
switch to another task, when you activate the original task all you need to do
is a \texttt{free} of the stack cache size and an \texttt{ensure} of either
the stack cache size or the actual size of the data that was in the cache before
the last task switch. Other schemes might also be possible.}

%\stefan{Currently all the stack control operations use immediates, so \texttt{ensure} using the actual size will be .. tricky. This may
%actually cause problems, since after a context switch there might be more data in the stack cache than before, which can cause the following
%\texttt{reserve} ops to spill, i.e., they take longer than without the context switch (assuming the hardware does not keep track of dirty
%cache lines). If might also be a good idea to avoid spilling/filling above the stack origin, else we would need to have an unused memory block of
%the size of the stack cache above every stack to support context switches (this requires telling the hardware about the stack origin address).
%However, we do not even know as of yet how/when/if we want to do
%context switches and interrupts anyway in this project (only at preemption points?, handling of dma transfers/loads/method cache fills,..).}
%\martin{I think stack spill and fill on a context switch is a reasonable option.
%One has to consider this in preemption cache cost analysis anyway.
%Why not simply make it explicit?}

\end{document}
